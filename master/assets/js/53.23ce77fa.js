(window.webpackJsonp=window.webpackJsonp||[]).push([[53],{458:function(t,a,e){"use strict";e.r(a);var s=e(26),n=Object(s.a)({},(function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("h1",{attrs:{id:"the-basics"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#the-basics"}},[t._v("#")]),t._v(" The basics")]),t._v(" "),e("p",[t._v("The library is built around a few simple concepts. This section explains everything you need to know to get started (feel free to jump into the sections you are more interested in):")]),t._v(" "),e("p"),e("div",{staticClass:"table-of-contents"},[e("ul",[e("li",[e("a",{attrs:{href:"#pipeline"}},[t._v("Pipeline")]),e("ul",[e("li",[e("a",{attrs:{href:"#tokenizer"}},[t._v("Tokenizer")])]),e("li",[e("a",{attrs:{href:"#features"}},[t._v("Features")])]),e("li",[e("a",{attrs:{href:"#encoder"}},[t._v("Encoder")])]),e("li",[e("a",{attrs:{href:"#head"}},[t._v("Head")])])])]),e("li",[e("a",{attrs:{href:"#dataset"}},[t._v("Dataset")])]),e("li",[e("a",{attrs:{href:"#vocabulary"}},[t._v("Vocabulary")])]),e("li",[e("a",{attrs:{href:"#train"}},[t._v("Train")])]),e("li",[e("a",{attrs:{href:"#using-pre-trained-pipelines"}},[t._v("Using pre-trained pipelines")]),e("ul",[e("li",[e("a",{attrs:{href:"#predict"}},[t._v("Predict")])]),e("li",[e("a",{attrs:{href:"#training-and-transfer-learning"}},[t._v("Training and transfer learning")])]),e("li",[e("a",{attrs:{href:"#serve"}},[t._v("Serve")])])])]),e("li",[e("a",{attrs:{href:"#next-steps"}},[t._v("Next steps")])])])]),e("p"),t._v(" "),e("p",[t._v("Before going into details, let's see a simple example:")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" biome"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Pipeline"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Dataset"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Trainer\n\npipeline "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Pipeline"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_config"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"name"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"my-first-classifier"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"head"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"TextClassification"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"labels"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"positive"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"negative"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\ntrain_ds "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Dataset"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_csv"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'training_data.csv'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\ntrainer "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Trainer"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    pipeline"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("pipeline"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    train_dataset"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("train_ds\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntrainer"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("p",[t._v("The above example trains a text classifier from scratch by configuring a "),e("code",[t._v("Pipeline")]),t._v(", making a "),e("code",[t._v("Dataset")]),t._v(" from a csv file and passing both to a "),e("code",[t._v("Trainer")]),t._v(".\nLet's dive into the details.")]),t._v(" "),e("h2",{attrs:{id:"pipeline"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#pipeline"}},[t._v("#")]),t._v(" Pipeline")]),t._v(" "),e("p",[t._v("Pipelines are the main entry point to the library. A "),e("code",[t._v("Pipeline")]),t._v(" bundles components needed to train, evaluate and use custom NLP models.")]),t._v(" "),e("p",[t._v("Pipelines encompass tokenization, feature processing, model configuration and actions such as serving or inference.")]),t._v(" "),e("p",[t._v("Let's continue with our example:")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("div",{staticClass:"highlight-lines"},[e("br"),e("br"),e("div",{staticClass:"highlighted"},[t._v(" ")]),e("div",{staticClass:"highlighted"},[t._v(" ")]),e("div",{staticClass:"highlighted"},[t._v(" ")]),e("div",{staticClass:"highlighted"},[t._v(" ")]),e("br"),e("br"),e("br"),e("br"),e("br"),e("br"),e("br"),e("br"),e("br")]),e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" biome"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Pipeline"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Dataset"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Trainer\n\npipeline "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Pipeline"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_config"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"name"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"my-first-classifier"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"head"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"TextClassification"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"labels"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"positive"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"negative"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\ntrain_ds "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Dataset"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_csv"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'training_data.csv'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\ntrainer "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Trainer"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    pipeline"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("pipeline"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    train_dataset"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("train_ds\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntrainer"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("p",[t._v("Here we configure a "),e("code",[t._v("Pipeline")]),t._v(" from scratch using a dictionary. A pipeline can be created in two ways:")]),t._v(" "),e("ol",[e("li",[e("p",[t._v("Using the "),e("code",[t._v("Pipeline.from_config()")]),t._v(" method, which accepts a dict or a "),e("RouterLink",{attrs:{to:"/api/biome/text/configuration.html#pipelineconfiguration"}},[t._v("PipelineConfiguration")])],1)]),t._v(" "),e("li",[e("p",[t._v("Using YAML configuration files with the "),e("code",[t._v("Pipeline.from_yaml()")]),t._v(". The YAML config file in our example would be following:")])])]),t._v(" "),e("div",{staticClass:"language-yaml extra-class"},[e("pre",{pre:!0,attrs:{class:"language-yaml"}},[e("code",[e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("name")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" my"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v("first"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("-")]),t._v("classifier\n"),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("head")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("type")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" TextClassification\n    "),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("labels")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"positive"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"negative"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),e("p",[t._v("In this example, we only define the "),e("code",[t._v("name")]),t._v(" and the task we want to train our model on, using the "),e("code",[t._v("head")]),t._v(" parameter, the rest is configured from defaults.\nIn "),e("em",[t._v("biome.text")]),t._v(" we try to provide sensible defaults so you don't have to configure everything just to start experimenting, but there are many things you can tune and configure.")]),t._v(" "),e("p",[t._v("In particular, a "),e("code",[t._v("Pipeline")]),t._v(" has the following configurable components:")]),t._v(" "),e("h3",{attrs:{id:"tokenizer"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#tokenizer"}},[t._v("#")]),t._v(" Tokenizer")]),t._v(" "),e("p",[t._v("The tokenizer defines how we want to process the text of our input features. Tokenizers are based on "),e("a",{attrs:{href:"https://spacy.io/api/tokenizer",target:"_blank",rel:"noopener noreferrer"}},[t._v("spaCy tokenizers"),e("OutboundLink")],1),t._v(" and have the following main configuration options:")]),t._v(" "),e("ol",[e("li",[e("code",[t._v("lang")]),t._v(": the main language of the text to be tokenized (default is English). Here you can use available "),e("a",{attrs:{href:"https://spacy.io/usage/models/",target:"_blank",rel:"noopener noreferrer"}},[t._v("spaCy model codes"),e("OutboundLink")],1),t._v(".")]),t._v(" "),e("li",[e("code",[t._v("segment_sentences")]),t._v(": enable sentence splitting for text within your input features, which is especially relevant for long text classification problems.")]),t._v(" "),e("li",[e("code",[t._v("text_cleaning")]),t._v(": simple python functions to pre-process text before tokenization. You can define your own but "),e("em",[t._v("biome.text")]),t._v(" provides pre-defined functions for things like cleaning up html tags or remove extra blank spaces.")])]),t._v(" "),e("h3",{attrs:{id:"features"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#features"}},[t._v("#")]),t._v(" Features")]),t._v(" "),e("p",[t._v("Features are a central concept of the library. Building on the flexibility of AllenNLP, "),e("em",[t._v("biome.text")]),t._v(" gives you the ability of combining "),e("RouterLink",{attrs:{to:"/api/biome/text/features.html#wordfeatures"}},[t._v("Word")]),t._v(", "),e("RouterLink",{attrs:{to:"/api/biome/text/features.html#charfeatures"}},[t._v("Character")]),t._v(" and other input features easily. There are many things which can be configured here: the size of the embeddings, encoder type (e.g., CNNs or RNNs) for character encoding, pre-trained word vectors, and other things.")],1),t._v(" "),e("p",[t._v("To learn more about how to configure and use Features, see the "),e("RouterLink",{attrs:{to:"/api/biome/text/configuration.html#featuresconfiguration"}},[t._v("FeaturesConfiguration API docs")]),t._v(".")],1),t._v(" "),e("h3",{attrs:{id:"encoder"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#encoder"}},[t._v("#")]),t._v(" Encoder")]),t._v(" "),e("p",[t._v('To support transfer learning, models are structured into a model "backbone" for processing and encoding features and a "task" head for a certain NLP task.')]),t._v(" "),e("p",[t._v("The "),e("code",[t._v("Encoder")]),t._v(' is a central piece of the backbone. It\'s basically a sequence to sequence or seq2seq encoder, which "contextualizes" textual features in the context of a task (supervised or unsupervised). In this way, the encoder can be pre-trained and fine-tuned for different downstream tasks by just changing the head, as we will see later. You can check the encoders provided by '),e("a",{attrs:{href:"https://github.com/allenai/allennlp/tree/master/allennlp/modules/seq2seq_encoders",target:"_blank",rel:"noopener noreferrer"}},[t._v("AllenNLP"),e("OutboundLink")],1),t._v(" or even write your own by implementing the "),e("a",{attrs:{href:"https://github.com/allenai/allennlp/blob/master/allennlp/modules/seq2seq_encoders/seq2seq_encoder.py",target:"_blank",rel:"noopener noreferrer"}},[t._v("Seq2SeqEncoder interface"),e("OutboundLink")],1),t._v(".")]),t._v(" "),e("p",[t._v("For defining encoders, "),e("em",[t._v("biome.text")]),t._v(" builds on top of the "),e("code",[t._v("Seq2SeqEncoder")]),t._v(" abstraction from AllenNLP, which brings many configuration possibilities, that go from RNNs to the official PyTorch Transformer implementation.")]),t._v(" "),e("h3",{attrs:{id:"head"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#head"}},[t._v("#")]),t._v(" Head")]),t._v(" "),e("p",[t._v("Task heads are the other key component to support flexible transfer learning. A head defines the NLP task (e.g., text classification, token-level classification, language modelling) and specific features related to the task, for example the labels of a text classifier ("),e("code",[t._v("positive")]),t._v(" and "),e("code",[t._v("negative")]),t._v(" in our example).")]),t._v(" "),e("p",[t._v("You can check available heads in the "),e("RouterLink",{attrs:{to:"/api/biome/text/modules/heads/"}},[t._v("API documentation")]),t._v(".")],1),t._v(" "),e("h2",{attrs:{id:"dataset"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#dataset"}},[t._v("#")]),t._v(" Dataset")]),t._v(" "),e("p",[t._v("The "),e("code",[t._v("Dataset")]),t._v(" class provides an easy way to load data for training, evaluation and inference coming from different sources: "),e("RouterLink",{attrs:{to:"/api/biome/text/dataset.html#dataset"}},[t._v("csv, json or from pandas DataFrames among others")]),t._v(".")],1),t._v(" "),e("p",[t._v("It is a very thin wrapper around HuggingFace's awesome "),e("a",{attrs:{href:"https://huggingface.co/docs/datasets/master/package_reference/main_classes.html#datasets.Dataset",target:"_blank",rel:"noopener noreferrer"}},[t._v("datasets.Dataset"),e("OutboundLink")],1),t._v(".\nMost of HuggingFace's "),e("code",[t._v("Dataset")]),t._v(" API is exposed, and you can check out their nice "),e("a",{attrs:{href:"https://huggingface.co/docs/datasets/master/processing.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("documentation"),e("OutboundLink")],1),t._v(" on how to work with data in a "),e("code",[t._v("Dataset")]),t._v(".")]),t._v(" "),e("p",[t._v("Coming back to our example:")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("div",{staticClass:"highlight-lines"},[e("br"),e("br"),e("br"),e("br"),e("br"),e("br"),e("br"),e("div",{staticClass:"highlighted"},[t._v(" ")]),e("br"),e("br"),e("br"),e("br"),e("br"),e("br"),e("br")]),e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" biome"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Pipeline"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Dataset"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Trainer\n\npipeline "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Pipeline"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_config"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"name"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"my-first-classifier"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"head"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"TextClassification"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"labels"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"positive"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"negative"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\ntrain_ds "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Dataset"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_csv"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'training_data.csv'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\ntrainer "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Trainer"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    pipeline"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("pipeline"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    train_dataset"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("train_ds\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntrainer"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("p",[t._v("Here we instantiate a "),e("code",[t._v("Dataset")]),t._v(" from a csv file that looks like this:")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("text")]),t._v(" "),e("th",{staticStyle:{"text-align":"center"}},[t._v("label")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[t._v("I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air con...")]),t._v(" "),e("td",{staticStyle:{"text-align":"center"}},[t._v("positive")])]),t._v(" "),e("tr",[e("td",[t._v("Phil the Alien is one of those quirky films where the humour is based around the oddness of ...")]),t._v(" "),e("td",{staticStyle:{"text-align":"center"}},[t._v("negative")])]),t._v(" "),e("tr",[e("td",[t._v("...")]),t._v(" "),e("td",{staticStyle:{"text-align":"center"}},[t._v("positive")])])])]),t._v(" "),e("p",[t._v("Columns in data sets are intimately related to what the pipeline expects as input and output features. In our example, we are defining a text classification model which expects a "),e("code",[t._v("text")]),t._v(" and a "),e("code",[t._v("label")]),t._v(" column.\nIn cases where users don't have the option to align the columns of the data with the features of the model, the "),e("code",[t._v("Dataset")]),t._v(" class provides a "),e("code",[t._v("rename_column_()")]),t._v(" and a "),e("code",[t._v("map()")]),t._v(" method. Imagine our data set looked like this:")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("title")]),t._v(" "),e("th",[t._v("review")]),t._v(" "),e("th",{staticStyle:{"text-align":"center"}},[t._v("label")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[t._v("Cool summer movie!")]),t._v(" "),e("td",[t._v("I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air con...")]),t._v(" "),e("td",{staticStyle:{"text-align":"center"}},[t._v("positive")])]),t._v(" "),e("tr",[e("td",[t._v("Horrible horror movie")]),t._v(" "),e("td",[t._v("Phil the Alien is one of those quirky films where the humour is based around the oddness of ...")]),t._v(" "),e("td",{staticStyle:{"text-align":"center"}},[t._v("negative")])]),t._v(" "),e("tr",[e("td"),t._v(" "),e("td",[t._v("...")]),t._v(" "),e("td",{staticStyle:{"text-align":"center"}},[t._v("positive")])])])]),t._v(" "),e("p",[t._v("Using the "),e("code",[t._v("rename_column_()")]),t._v(" method we could not only work with this data set schema by renaming one column:")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("train_ds"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("rename_column_"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'review'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'text'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("p",[t._v("but we could also combine both "),e("em",[t._v("title")]),t._v(" and "),e("em",[t._v("review")]),t._v(" to feed them as input features using the "),e("code",[t._v("map()")]),t._v(" method:")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("train_ds "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" train_ds"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" row"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"text"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" row"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"titile"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" row"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"review"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("p",[e("em",[t._v("biome.text")]),t._v(" was created with semi-structured data problems in mind, so it provides specialized models for learning from structured records such as the "),e("RouterLink",{attrs:{to:"/api/biome/text/modules/heads/classification/record_classification.html#recordclassification"}},[t._v("RecordClassification")]),t._v(" head, which lets you define mappings to arbitrary input fields and combine their vector representations in a hierarchical way (e.g., combining encoders at field and record level)")],1),t._v(" "),e("h2",{attrs:{id:"vocabulary"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#vocabulary"}},[t._v("#")]),t._v(" Vocabulary")]),t._v(" "),e("p",[t._v('For doing NLP with neural networks, your NLP pipeline needs to turn words, subwords and/or characters into numbers. A typical process consists of tokenizing the text, and mapping word (or sub-word) tokens and maybe characters into integers or indexes. This process is often referred to as "indexing".')]),t._v(" "),e("p",[t._v("In order to support this you need a "),e("code",[t._v("Vocabulary")]),t._v(", which holds a mapping from tokens and characters to numerical ids (if you are interested, this post provides a good "),e("a",{attrs:{href:"https://blog.floydhub.com/tokenization-nlp/",target:"_blank",rel:"noopener noreferrer"}},[t._v("overview"),e("OutboundLink")],1),t._v("). This mapping can be learned and built during pre-training phases as is the case for word piece models and generally sub-word models like those provided by "),e("a",{attrs:{href:"https://github.com/huggingface/tokenizers",target:"_blank",rel:"noopener noreferrer"}},[t._v("Huggingface tokenizers"),e("OutboundLink")],1),t._v(").")]),t._v(" "),e("p",[t._v("A more classical approach is to build or extend an existing vocabulary from training and validation data sets. For certain use cases, in highly specialized domains, this is sometimes the best way to proceed.")]),t._v(" "),e("p",[e("em",[t._v("biome.text")]),t._v(" takes care of building your vocabulary automatically if necessary.\nBy default, it will build the vocab based on the training data set, but if you want more control over this step you can pass a "),e("code",[t._v("VocabularyConfiguration")]),t._v(" instance to the "),e("code",[t._v("Trainer")]),t._v(" method:")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" biome"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" VocabularyConfiguration\n\nvocab_config "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" VocabularyConfiguration"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("include_valid_data"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" max_vocab_size"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("1000")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntrainer "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Trainer"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    pipeline"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("pipeline"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    train_dataset"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("train_ds"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    vocab_config"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("vocab_config"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("p",[t._v("Here we create our vocabulary using the training and validation data set and limit it to 1000 entries.\nAllenNLP provides neat abstractions for dealing with multi-feature vocabularies (e.g., chars, words, etc.) and "),e("em",[t._v("biome.text")]),t._v(" builds on top of those abstractions to make it easy to create, reuse and extend vocabularies.")]),t._v(" "),e("p",[t._v("To learn more about how to configure and use the "),e("code",[t._v("Vocabulary")]),t._v(", see the "),e("RouterLink",{attrs:{to:"/api/biome/text/configuration.html#vocabularyconfiguration"}},[t._v("VocabularyConfiguration API docs")])],1),t._v(" "),e("h2",{attrs:{id:"train"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#train"}},[t._v("#")]),t._v(" Train")]),t._v(" "),e("p",[t._v("Once we have everything ready, we can use the "),e("RouterLink",{attrs:{to:"/api/biome/text/trainer.html"}},[e("code",[t._v("Trainer")])]),t._v(" to train our pipeline.\nOur "),e("code",[t._v("Trainer")]),t._v(" is a light wrapper around the amazing "),e("a",{attrs:{href:"https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html",target:"_blank",rel:"noopener noreferrer"}},[e("code",[t._v("Pytorch Lightning")]),t._v(" trainer"),e("OutboundLink")],1),t._v(".\nGoing back to our example:")],1),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("div",{staticClass:"highlight-lines"},[e("br"),e("br"),e("br"),e("br"),e("br"),e("br"),e("br"),e("br"),e("br"),e("div",{staticClass:"highlighted"},[t._v(" ")]),e("div",{staticClass:"highlighted"},[t._v(" ")]),e("div",{staticClass:"highlighted"},[t._v(" ")]),e("div",{staticClass:"highlighted"},[t._v(" ")]),e("div",{staticClass:"highlighted"},[t._v(" ")]),e("br")]),e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" biome"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Pipeline"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Dataset"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Trainer\n\npipeline "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Pipeline"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_config"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"name"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"my-first-classifier"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"head"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"TextClassification"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"labels"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"positive"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"negative"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\ntrain_ds "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Dataset"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_csv"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'training_data.csv'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\ntrainer "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Trainer"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    pipeline"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("pipeline"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    train_dataset"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("train_ds\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntrainer"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("p",[t._v("By default, the "),e("code",[t._v("trainer.fit()")]),t._v(" method will create an "),e("code",[t._v("output")]),t._v(" folder with a "),e("code",[t._v("model.tar.gz")]),t._v(" and a "),e("code",[t._v("metrics.json")]),t._v(" file:")]),t._v(" "),e("ul",[e("li",[e("code",[t._v("model.tar.gz")]),t._v(": This is the more relevant file since it contains the trained model weights, the vocabulary and the pipeline configuration, which is everything we need to explore, serve or fine-tune the trained model.")]),t._v(" "),e("li",[e("code",[t._v("metrics.json")]),t._v(": It merely contains a summary of the logged metrics during the training and can be useful if you want to quickly compare different output folders.")])]),t._v(" "),e("p",[t._v("The "),e("code",[t._v("Pytorch Lightning")]),t._v(" trainer under the hood will also create a "),e("code",[t._v("training_logs")]),t._v(" folder in your working directory that contains all your checkpoints and logged metrics, which you can visualize with "),e("a",{attrs:{href:"https://www.tensorflow.org/tensorboard/",target:"_blank",rel:"noopener noreferrer"}},[t._v("tensorboard"),e("OutboundLink")],1),t._v(" for example.")]),t._v(" "),e("p",[t._v("In the example, we only provide a "),e("code",[t._v("train_dataset")]),t._v(" which is of course not recommended for most use cases, where you need at least a validation set and desirably a test set.\nWe also do not set anything related to the training process, such as optimizer, learning rate, epochs and so on.\nThe library provides basic defaults for this just to get started.\nWhen further experimenting, you will probably need to use a "),e("RouterLink",{attrs:{to:"/api/biome/text/configuration.html#trainerconfiguration"}},[t._v("TrainerConfiguration")]),t._v(" object:")],1),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" biome"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" TrainerConfiguration\n\ntrainer_config "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TrainerConfiguration"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("optimizer"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"adamw"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"lr"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.01")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntrainer "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Trainer"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    pipeline"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("pipeline"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    train_dataset"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("train_ds"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    trainer_config"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("trainer_config"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("h2",{attrs:{id:"using-pre-trained-pipelines"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#using-pre-trained-pipelines"}},[t._v("#")]),t._v(" Using pre-trained pipelines")]),t._v(" "),e("p",[t._v("As mentioned above, "),e("code",[t._v("model.tar.gz")]),t._v(" files resulting from a training run contain everything you need to start using your trained pipeline.")]),t._v(" "),e("p",[t._v("Let's see how:")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("pipeline "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Pipeline"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_pretrained"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'output/model.tar.gz'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("p",[t._v("After loading the pipeline, you can use it for predictions:")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("pipeline"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("text"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Good movie indeed!'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("h3",{attrs:{id:"predict"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#predict"}},[t._v("#")]),t._v(" Predict")]),t._v(" "),e("p",[t._v("The arguments for the "),e("RouterLink",{attrs:{to:"/api/biome/text/pipeline.html#predict"}},[t._v("Pipeline.predict()")]),t._v(" method will be aligned to match our model input features ("),e("code",[t._v("text")]),t._v(" in this case).\nModels such as the "),e("code",[t._v("RecordClassifier")]),t._v(" can be used to define more fine-grained input features, such as, for example, an email classifier with two fields "),e("code",[t._v("subject")]),t._v(" and "),e("code",[t._v("body")]),t._v(":")],1),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("pipeline"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("subject"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Hi!'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" body"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Hi, hope you are well..'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("p",[t._v("Some of our heads support attributing the input to the prediction by means of integrated gradients.\nFollowing will add an "),e("code",[t._v("attributions")]),t._v(" key in the prediction dictionary:")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("pipeline"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("subject"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Hi!'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" body"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Hi, hope you are well..'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" add_attributions"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("h3",{attrs:{id:"training-and-transfer-learning"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#training-and-transfer-learning"}},[t._v("#")]),t._v(" Training and transfer learning")]),t._v(" "),e("p",[t._v("After loading a pipeline, you can continue the training for the same task with new data:")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("trainer "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Trainer"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    pipeline"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("loaded_pipeline"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    train_dataset"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("new_training_ds\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntrainer"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("output_dir"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"finetuned_output"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("p",[t._v("Another thing you can do is to use this pre-trained pipeline for related task, for example another classifier with different labels but a similar domain:")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("div",{staticClass:"highlight-lines"},[e("br"),e("br"),e("div",{staticClass:"highlighted"},[t._v(" ")]),e("div",{staticClass:"highlighted"},[t._v(" ")]),e("div",{staticClass:"highlighted"},[t._v(" ")]),e("div",{staticClass:"highlighted"},[t._v(" ")]),e("br"),e("br"),e("br"),e("br"),e("br"),e("br"),e("br")]),e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" biome"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("modules"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("heads "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" TextClassification\n\nloaded_pipeline"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("set_head"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    TextClassification"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    labels "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"horror_movie"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"comedy"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"drama"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"social"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\ntrainer "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Trainer"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    pipeline"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("loaded_pipeline"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    train_dataset"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("new_training_ds\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntrainer"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("output_dir"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"finetuned_output"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("p",[t._v("Here we just use the "),e("RouterLink",{attrs:{to:"/api/biome/text/pipeline.html#set-head"}},[t._v("Pipeline.set_head()")]),t._v(" method to set a new task head which classifies film categories instead of review sentiment.")],1),t._v(" "),e("p",[t._v('The more common "pre-training a language model + fine-tuning on downstream tasks" is also supported by using a '),e("RouterLink",{attrs:{to:"/api/biome/text/modules/heads/language_modelling.html#languagemodelling"}},[t._v("LanguageModelling")]),t._v(" head for pre-training.")],1),t._v(" "),e("h3",{attrs:{id:"serve"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#serve"}},[t._v("#")]),t._v(" Serve")]),t._v(" "),e("p",[e("em",[t._v("biome.text")]),t._v(" uses the awesome "),e("a",{attrs:{href:"https://fastapi.tiangolo.com/",target:"_blank",rel:"noopener noreferrer"}},[t._v("FastAPI"),e("OutboundLink")],1),t._v(" library to give you a simple REST endpoint with the "),e("RouterLink",{attrs:{to:"/api/biome/text/cli/serve.html"}},[t._v("biome serve")]),t._v(" CLI command, which provides methods aligned with your input features (e.g., a method accepting "),e("code",[t._v("subject")]),t._v(" and "),e("code",[t._v("body")]),t._v(" parameters).\nIn a terminal just type in:")],1),t._v(" "),e("div",{staticClass:"language-terminal extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("biome serve path/to/your/model.tar.gz\n")])])]),e("h2",{attrs:{id:"next-steps"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#next-steps"}},[t._v("#")]),t._v(" Next steps")]),t._v(" "),e("p",[t._v("The best place to start is the tutorials section.")]),t._v(" "),e("p",[t._v("You can also find detailed "),e("RouterLink",{attrs:{to:"/api/"}},[t._v("API docs")]),t._v(" and use the search bar on top to find out more about specific concepts and features.")],1)])}),[],!1,null,null,null);a.default=n.exports}}]);