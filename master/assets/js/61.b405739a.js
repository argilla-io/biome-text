(window.webpackJsonp=window.webpackJsonp||[]).push([[61],{466:function(t,a,s){"use strict";s.r(a);var e=s(26),n=Object(e.a)({},(function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h1",{attrs:{id:"training-a-short-text-classifier-of-german-business-names"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#training-a-short-text-classifier-of-german-business-names"}},[t._v("#")]),t._v(" Training a short text classifier of German business names")]),t._v(" "),s("p",[s("a",{attrs:{target:"_blank",href:"https://www.recogn.ai/biome-text/master/documentation/tutorials/1-Training_a_text_classifier.html"}},[s("img",{staticClass:"icon",attrs:{src:"https://www.recogn.ai/biome-text/master/assets/img/biome-isotype.svg",width:"24"}})]),t._v(" "),s("a",{attrs:{href:"https://www.recogn.ai/biome-text/master/documentation/tutorials/1-Training_a_text_classifier.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("View on recogn.ai"),s("OutboundLink")],1)]),t._v(" "),s("p",[s("a",{attrs:{target:"_blank",href:"https://colab.research.google.com/github/recognai/biome-text/blob/master/docs/docs/documentation/tutorials/1-Training_a_text_classifier.ipynb"}},[s("img",{staticClass:"icon",attrs:{src:"https://www.tensorflow.org/images/colab_logo_32px.png",width:"24"}})]),t._v(" "),s("a",{attrs:{href:"https://colab.research.google.com/github/recognai/biome-text/blob/master/docs/docs/documentation/tutorials/1-Training_a_text_classifier.ipynb",target:"_blank",rel:"noopener noreferrer"}},[t._v("Run in Google Colab"),s("OutboundLink")],1)]),t._v(" "),s("p",[s("a",{attrs:{target:"_blank",href:"https://github.com/recognai/biome-text/blob/master/docs/docs/documentation/tutorials/1-Training_a_text_classifier.ipynb"}},[s("img",{staticClass:"icon",attrs:{src:"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png",width:"24"}})]),t._v(" "),s("a",{attrs:{href:"https://github.com/recognai/biome-text/blob/master/docs/docs/documentation/tutorials/1-Training_a_text_classifier.ipynb",target:"_blank",rel:"noopener noreferrer"}},[t._v("View source on GitHub"),s("OutboundLink")],1)]),t._v(" "),s("p",[t._v("When running this tutorial in Google Colab, make sure to install "),s("em",[t._v("biome.text")]),t._v(" first:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("!pip install "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("U pip\n!pip install "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("U git"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v("https"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("//")]),t._v("github"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("com"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("recognai"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("biome"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("git\nexit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Force restart of the runtime")]),t._v("\n")])])]),s("p",[s("em",[t._v("If")]),t._v(" you want to log your runs with "),s("a",{attrs:{href:"https://wandb.ai/home",target:"_blank",rel:"noopener noreferrer"}},[t._v("WandB"),s("OutboundLink")],1),t._v(", don't forget to install its client and log in.")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("!pip install wandb\n!wandb login\n")])])]),s("h2",{attrs:{id:"introduction"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#introduction"}},[t._v("#")]),t._v(" Introduction")]),t._v(" "),s("p",[t._v("In this tutorial we will train a basic short-text classifier for predicting the sector of a business based only on its business name.\nFor this we will use a training data set with business names and business categories in German.")]),t._v(" "),s("h3",{attrs:{id:"imports"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#imports"}},[t._v("#")]),t._v(" Imports")]),t._v(" "),s("p",[t._v("Let us first import all the stuff we need for this tutorial:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" biome"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Pipeline"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Dataset"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Trainer\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" biome"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("configuration "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" VocabularyConfiguration"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" WordFeatures"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" TrainerConfiguration\n")])])]),s("h2",{attrs:{id:"explore-the-training-data"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#explore-the-training-data"}},[t._v("#")]),t._v(" Explore the training data")]),t._v(" "),s("p",[t._v("Let's take a look at the data we will use for training. For this we will use the "),s("a",{attrs:{href:"https://www.recogn.ai/biome-text/master/api/biome/text/dataset.html#dataset",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("Dataset")]),s("OutboundLink")],1),t._v(" class that is a very thin wrapper around HuggingFace's awesome "),s("a",{attrs:{href:"https://huggingface.co/docs/datasets/master/package_reference/main_classes.html#datasets.Dataset",target:"_blank",rel:"noopener noreferrer"}},[t._v("datasets.Dataset"),s("OutboundLink")],1),t._v(".\nWe will download the data first to create "),s("code",[t._v("Dataset")]),t._v(" instances.")]),t._v(" "),s("p",[t._v("Apart from the training data we will also download an optional validation data set to estimate the generalization error.")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Downloading the dataset first")]),t._v("\n!curl "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("O https"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("//")]),t._v("biome"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("tutorials"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("s3"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("eu"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("west"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.")]),t._v("amazonaws"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("com"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("text_classifier"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("business"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cat"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("csv\n!curl "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("O https"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("//")]),t._v("biome"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("tutorials"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("s3"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("eu"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("west"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.")]),t._v("amazonaws"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("com"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("text_classifier"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("business"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cat"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("valid"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("csv\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Loading from local")]),t._v("\ntrain_ds "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Dataset"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_csv"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"business.cat.train.csv"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nvalid_ds "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Dataset"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_csv"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"business.cat.valid.csv"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("Most of HuggingFace's "),s("code",[t._v("Dataset")]),t._v(" API is exposed and you can checkout their nice "),s("a",{attrs:{href:"https://huggingface.co/docs/datasets/master/processing.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("documentation"),s("OutboundLink")],1),t._v(" on how to work with data in a "),s("code",[t._v("Dataset")]),t._v(". For example, let's quickly check the size of our training data and print the first 10 examples as a pandas DataFrame:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("train_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("head"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("As we can see we have two relevant columns "),s("em",[t._v("label")]),t._v(" and "),s("em",[t._v("text")]),t._v(". Our classifier will be trained to predict the "),s("em",[t._v("label")]),t._v(" given the "),s("em",[t._v("text")]),t._v(".")]),t._v(" "),s("div",{staticClass:"custom-block tip"},[s("p",{staticClass:"custom-block-title"},[t._v("Tip")]),t._v(" "),s("p",[t._v("The "),s("a",{attrs:{href:"https://www.recogn.ai/biome-text/master/api/biome/text/modules/heads/task_head.html#taskhead",target:"_blank",rel:"noopener noreferrer"}},[t._v("TaskHead"),s("OutboundLink")],1),t._v(" of our model below will expect a "),s("em",[t._v("text")]),t._v(" and a "),s("em",[t._v("label")]),t._v(" column to be present in the "),s("code",[t._v("Dataset")]),t._v(". In our data set this is already the case, otherwise we would need to change or map the corresponding column names via "),s("code",[t._v("Dataset.rename_column_()")]),t._v(" or "),s("code",[t._v("Dataset.map()")]),t._v(".")])]),t._v(" "),s("p",[t._v("We can also quickly check the distribution of our labels. Use "),s("code",[t._v("Dataset.head(None)")]),t._v(" to return the complete data set as a pandas DataFrame:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("train_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("head"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"label"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("value_counts"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("The "),s("code",[t._v("Dataset")]),t._v(" class also provides access to Hugging Face's extensive NLP datasets collection via the "),s("code",[t._v("Dataset.load_dataset()")]),t._v(" method. Have a look at their "),s("a",{attrs:{href:"https://huggingface.co/docs/datasets/master/quicktour.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("quicktour"),s("OutboundLink")],1),t._v(" for more details about their awesome library.")]),t._v(" "),s("h2",{attrs:{id:"configure-your-biome-text-pipeline"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#configure-your-biome-text-pipeline"}},[t._v("#")]),t._v(" Configure your "),s("em",[t._v("biome.text")]),t._v(" Pipeline")]),t._v(" "),s("p",[t._v("A typical "),s("a",{attrs:{href:"https://www.recogn.ai/biome-text/master/api/biome/text/pipeline.html#pipeline",target:"_blank",rel:"noopener noreferrer"}},[t._v("Pipeline"),s("OutboundLink")],1),t._v(" consists of tokenizing the input, extracting features, applying a language encoding (optionally) and executing a task-specific head in the end.")]),t._v(" "),s("p",[t._v("After training a pipeline, you can use it to make predictions.")]),t._v(" "),s("p",[t._v("As a first step we must define a configuration for our pipeline.\nIn this tutorial we will create a configuration dictionary and use the "),s("code",[t._v("Pipeline.from_config()")]),t._v(" method to create our pipeline, but there are "),s("a",{attrs:{href:"https://www.recogn.ai/biome-text/master/api/biome/text/pipeline.html#pipeline",target:"_blank",rel:"noopener noreferrer"}},[t._v("other ways"),s("OutboundLink")],1),t._v(".")]),t._v(" "),s("p",[t._v("A "),s("em",[t._v("biome.text")]),t._v(" pipeline has the following main components:")]),t._v(" "),s("div",{staticClass:"language-yaml extra-class"},[s("pre",{pre:!0,attrs:{class:"language-yaml"}},[s("code",[s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("name")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# a descriptive name of your pipeline")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("tokenizer")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# how to tokenize the input")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("features")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# input features of the model")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("encoder")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# the language encoder")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("head")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# your task configuration")]),t._v("\n\n")])])]),s("p",[t._v("See the "),s("a",{attrs:{href:"https://www.recogn.ai/biome-text/master/documentation/user-guides/2-configuration.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("Configuration section"),s("OutboundLink")],1),t._v(" for a detailed description of how these main components can be configured.")]),t._v(" "),s("p",[t._v("Our complete configuration for this tutorial will be following:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("pipeline_dict "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"name"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"german_business_names"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"tokenizer"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"text_cleaning"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"rules"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"strip_spaces"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"features"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"word"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"embedding_dim"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("64")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"lowercase_tokens"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"char"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"embedding_dim"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("32")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"lowercase_characters"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"encoder"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"gru"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"num_layers"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"hidden_size"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("32")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"bidirectional"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"dropout"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"head"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"TextClassification"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"labels"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" train_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("unique"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"label"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"pooler"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"gru"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"num_layers"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"hidden_size"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("32")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"bidirectional"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"feedforward"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"num_layers"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"hidden_dims"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("32")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"activations"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"relu"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"dropout"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("       \n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),s("p",[t._v("With this dictionary we can now create a "),s("code",[t._v("Pipeline")]),t._v(":")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("pl "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Pipeline"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_config"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pipeline_dict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h2",{attrs:{id:"configure-the-vocabulary"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#configure-the-vocabulary"}},[t._v("#")]),t._v(" Configure the vocabulary")]),t._v(" "),s("p",[t._v("The default behavior of "),s("em",[t._v("biome.text")]),t._v(" is to add all tokens from the training data set to the pipeline's vocabulary.\nThis is done automatically when training the pipeline for the first time.")]),t._v(" "),s("p",[t._v("If you want to have more control over this step, you can define a "),s("code",[t._v("VocabularyConfiguration")]),t._v(" and pass it to the "),s("a",{attrs:{href:"https://www.recogn.ai/biome-text/master/api/biome/text/trainer.html",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("Trainer")]),s("OutboundLink")],1),t._v(' later on.\nIn our business name classifier we only want to include words with a general meaning to our word feature vocabulary (like "Computer" or "Autohaus", for example), and want to exclude specific names that will not help to generally classify the kind of business.\nThis can be achieved by including only the most frequent words in our training set via the '),s("code",[t._v("min_count")]),t._v(" argument. For a complete list of available arguments see the "),s("a",{attrs:{href:"https://www.recogn.ai/biome-text/master/api/biome/text/configuration.html#vocabularyconfiguration",target:"_blank",rel:"noopener noreferrer"}},[t._v("VocabularyConfiguration API"),s("OutboundLink")],1),t._v(".")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("vocab_config "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" VocabularyConfiguration"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("min_count"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("WordFeatures"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("namespace"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h2",{attrs:{id:"configure-the-trainer"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#configure-the-trainer"}},[t._v("#")]),t._v(" Configure the trainer")]),t._v(" "),s("p",[t._v("As a next step we have to configure the "),s("a",{attrs:{href:"https://www.recogn.ai/biome-text/master/api/biome/text/trainer.html",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("Trainer")]),s("OutboundLink")],1),t._v(", which in essentially is a light wrapper around the amazing "),s("a",{attrs:{href:"https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("Pytorch Lightning Trainer"),s("OutboundLink")],1),t._v(".")]),t._v(" "),s("p",[t._v("The default trainer has sensible defaults and should work alright for most of your cases.\nIn this tutorial, however, we want to tune a bit the learning rate and limit the training time to three epochs only.\nWe also want to modify the monitored validation metric (by default it is the "),s("code",[t._v("validation_loss")]),t._v(") that is used to rank the checkpoints, as well as for the early stopping mechanism and to load the best model weights at the end of the training.\nFor a complete list of available arguments see the "),s("a",{attrs:{href:"https://www.recogn.ai/biome-text/master/api/biome/text/configuration.html#trainerconfiguration",target:"_blank",rel:"noopener noreferrer"}},[t._v("TrainerConfiguration API"),s("OutboundLink")],1),t._v(".")]),t._v(" "),s("div",{staticClass:"custom-block tip"},[s("p",{staticClass:"custom-block-title"},[t._v("Tip")]),t._v(" "),s("p",[t._v("By default we will use a CUDA device if one is available. If you prefer not to use it, just set "),s("code",[t._v("gpus=0")]),t._v(" in the "),s("code",[t._v("TrainerConfiguration")]),t._v(".")])]),t._v(" "),s("div",{staticClass:"custom-block tip"},[s("p",{staticClass:"custom-block-title"},[t._v("Tip")]),t._v(" "),s("p",[t._v("The default "),s("a",{attrs:{href:"https://wandb.ai/site",target:"_blank",rel:"noopener noreferrer"}},[t._v("WandB"),s("OutboundLink")],1),t._v(' logger will log the runs to the "biome" project.\nYou can easily change this by setting the '),s("code",[t._v("WANDB_PROJECT")]),t._v(" env variable:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" os\nos"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("environ"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"WANDB_PROJECT"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"my_project"')]),t._v("\n")])])])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("trainer_config "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TrainerConfiguration"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    optimizer"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"adam"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"lr"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.01")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    max_epochs"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    monitor"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"validation_accuracy"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    monitor_mode"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"max"')]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h2",{attrs:{id:"train-your-model"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#train-your-model"}},[t._v("#")]),t._v(" Train your model")]),t._v(" "),s("p",[t._v("Now we have everything ready to start the training of our model:")]),t._v(" "),s("ul",[s("li",[t._v("training data set")]),t._v(" "),s("li",[t._v("pipeline")]),t._v(" "),s("li",[t._v("trainer configuration")])]),t._v(" "),s("p",[t._v("In a fist step we have to create a "),s("code",[t._v("Trainer")]),t._v(" instance and pass in the pipeline, the training/validation data, the trainer configuration and our vocabulary configuration.\nThis will load the data into memory (unless you specify "),s("code",[t._v("layz=True")]),t._v(") and build the vocabulary.")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("trainer "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Trainer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    pipeline"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("pl"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    train_dataset"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("train_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    valid_dataset"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("valid_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    trainer_config"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("trainer_config"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    vocab_config"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("vocab_config"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("In a second step we simply have to call the "),s("code",[t._v("Trainer.fit()")]),t._v(" method to start the training.\nBy default, at the end of the training the trained pipeline and the training metrics will be saved in a folder called "),s("code",[t._v("output")]),t._v(".\nThe trained pipeline is saved as a "),s("code",[t._v("model.tar.gz")]),t._v(" file that contains the pipeline configuration, the model weights and the vocabulary.\nThe metrics are saved to a "),s("code",[t._v("metrics.json")]),t._v(" file.")]),t._v(" "),s("p",[t._v("During the training the "),s("code",[t._v("Trainer")]),t._v(" will also create a logging folder called "),s("code",[t._v("training_logs")]),t._v(" by default.\nYou can modify this path via the "),s("code",[t._v("default_root_dir")]),t._v(" option in your "),s("code",[t._v("TrainerConfiguration")]),t._v(", that also supports remote addresses such as s3 or hdfs.\nThis logging folder contains all your checkpoints and logged metrics, like the ones logged for "),s("a",{attrs:{href:"https://www.tensorflow.org/tensorboard/",target:"_blank",rel:"noopener noreferrer"}},[t._v("TensorBoard"),s("OutboundLink")],1),t._v(" for example.")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("trainer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("After 3 epochs we achieve a validation accuracy of about 0.91.\nThe validation loss seems to be decreasing further, though, so we could probably train the model for a few more epochs without overfitting the training data.\nFor this we could simply reinitialize the "),s("code",[t._v("Trainer")]),t._v(" and call "),s("code",[t._v("Trainer.fit(exist_ok=True)")]),t._v(" again.")]),t._v(" "),s("div",{staticClass:"custom-block tip"},[s("p",{staticClass:"custom-block-title"},[t._v("Tip")]),t._v(" "),s("p",[t._v("If for some reason the training gets interrupted, you can continue from the last saved checkpoint by setting the "),s("code",[t._v("resume_from_checkpoint")]),t._v(" option in the "),s("code",[t._v("TrainerConfiguration")]),t._v(".")])]),t._v(" "),s("div",{staticClass:"custom-block tip"},[s("p",{staticClass:"custom-block-title"},[t._v("Tip")]),t._v(" "),s("p",[t._v("If you receive warnings about the data loader being a bottleneck, try to increase the "),s("code",[t._v("num_workers_for_dataloader")]),t._v(" parameter in the "),s("code",[t._v("TrainerConfiguration")]),t._v(" (up to the number of cpus on your machine).")])]),t._v(" "),s("h2",{attrs:{id:"make-your-first-predictions"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#make-your-first-predictions"}},[t._v("#")]),t._v(" Make your first predictions")]),t._v(" "),s("p",[t._v("Now that we trained our model we can go on to make our first predictions.\nWe provide the input expected by our "),s("code",[t._v("TaskHead")]),t._v(" of the model to the "),s("code",[t._v("Pipeline.predict()")]),t._v(" method.\nIn our case it is a "),s("code",[t._v("TextClassification")]),t._v(" head that classifies a "),s("code",[t._v("text")]),t._v(" input:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("pl"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("text"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Autohaus biome.text"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("The output of the "),s("code",[t._v("Pipeline.predict()")]),t._v(" method is a dictionary with a "),s("code",[t._v("labels")]),t._v(" and "),s("code",[t._v("probabilities")]),t._v(" key containing a list of labels and their corresponding probabilities, ordered from most to less likely.")]),t._v(" "),s("div",{staticClass:"custom-block tip"},[s("p",{staticClass:"custom-block-title"},[t._v("Tip")]),t._v(" "),s("p",[t._v("When configuring the pipeline in the first place, we recommend to check that it is correctly setup by using the "),s("code",[t._v("predict")]),t._v(" method.\nSince the pipeline is still not trained at that moment, the predictions will be arbitrary.")])]),t._v(" "),s("p",[t._v("We can also load the trained pipeline from the training output. This is useful in case you trained the pipeline in some earlier session, and want to continue your work with the inference steps:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("pl_trained "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Pipeline"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_pretrained"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"output/model.tar.gz"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])])])}),[],!1,null,null,null);a.default=n.exports}}]);