(window.webpackJsonp=window.webpackJsonp||[]).push([[56],{461:function(t,a,e){"use strict";e.r(a);var n=e(26),i=Object(n.a)({},(function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("h1",{attrs:{id:"biome-text-trainer"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#biome-text-trainer"}},[t._v("#")]),t._v(" biome.text.trainer "),e("Badge",{attrs:{text:"Module"}})],1),t._v(" "),e("div"),t._v(" "),e("pre",{staticClass:"title"},[e("h3",{attrs:{id:"create-dataloader"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#create-dataloader"}},[t._v("#")]),t._v(" create_dataloader "),e("Badge",{attrs:{text:"Function"}})],1),t._v("\n")]),t._v(" "),e("dt",[e("div",{staticClass:"language-python extra-class"},[e("pre",{staticClass:"language-python"},[e("code",[t._v("\n"),e("span",{staticClass:"token keyword"},[t._v("def")]),t._v(" "),e("span",{staticClass:"ident"},[t._v("create_dataloader")]),t._v(" ("),t._v("\n  instance_dataset: Union[allennlp.data.dataset_readers.dataset_reader.AllennlpDataset, allennlp.data.dataset_readers.dataset_reader.AllennlpLazyDataset],\n  batch_size: int = 16,\n  data_bucketing: bool = False,\n)  -> allennlp.data.dataloader.PyTorchDataLoader\n")]),t._v("\n")])])]),t._v(" "),e("dd",[e("p",[t._v("Returns a pytorch DataLoader for AllenNLP instances")]),t._v(" "),e("h2",{attrs:{id:"parameters"}},[t._v("Parameters")]),t._v(" "),e("dl",[e("dt",[e("strong",[e("code",[t._v("instance_dataset")])])]),t._v(" "),e("dd",[t._v("The dataset of instances for the DataLoader")]),t._v(" "),e("dt",[e("strong",[e("code",[t._v("batch_size")])])]),t._v(" "),e("dd",[t._v("Batch size")]),t._v(" "),e("dt",[e("strong",[e("code",[t._v("data_bucketing")])])]),t._v(" "),e("dd",[t._v("If True, tries to sort batches with respect to the maximum input lengths per batch.\nNot supported for lazily loaded data!")])]),t._v(" "),e("h2",{attrs:{id:"returns"}},[t._v("Returns")]),t._v(" "),e("dl",[e("dt",[e("code",[t._v("data_loader")])]),t._v(" "),e("dd",[t._v(" ")])])]),t._v(" "),e("div"),t._v(" "),e("pre",{staticClass:"title"},[e("h2",{attrs:{id:"trainer"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#trainer"}},[t._v("#")]),t._v(" Trainer "),e("Badge",{attrs:{text:"Class"}})],1),t._v("\n")]),t._v(" "),e("pre",{staticClass:"language-python"},[e("code",[t._v("\n"),e("span",{staticClass:"token keyword"},[t._v("class")]),t._v(" "),e("span",{staticClass:"ident"},[t._v("Trainer")]),t._v(" ("),t._v("\n    "),e("span",[t._v("pipeline: "),e("a",{attrs:{title:"biome.text.pipeline.Pipeline",href:"pipeline.html#biome.text.pipeline.Pipeline"}},[t._v("Pipeline")])]),e("span",[t._v(",")]),t._v("\n    "),e("span",[t._v("train_dataset: "),e("a",{attrs:{title:"biome.text.dataset.Dataset",href:"dataset.html#biome.text.dataset.Dataset"}},[t._v("Dataset")])]),e("span",[t._v(",")]),t._v("\n    "),e("span",[t._v("valid_dataset: Union["),e("a",{attrs:{title:"biome.text.dataset.Dataset",href:"dataset.html#biome.text.dataset.Dataset"}},[t._v("Dataset")]),t._v(", NoneType] = None")]),e("span",[t._v(",")]),t._v("\n    "),e("span",[t._v("trainer_config: Union["),e("a",{attrs:{title:"biome.text.configuration.LightningTrainerConfiguration",href:"configuration.html#biome.text.configuration.LightningTrainerConfiguration"}},[t._v("LightningTrainerConfiguration")]),t._v(", NoneType] = None")]),e("span",[t._v(",")]),t._v("\n    "),e("span",[t._v("vocab_config: Union["),e("a",{attrs:{title:"biome.text.configuration.VocabularyConfiguration",href:"configuration.html#biome.text.configuration.VocabularyConfiguration"}},[t._v("VocabularyConfiguration")]),t._v(", str, NoneType] = 'default'")]),e("span",[t._v(",")]),t._v("\n    "),e("span",[t._v("lazy: bool = False")]),e("span",[t._v(",")]),t._v("\n"),e("span",[t._v(")")]),t._v("\n")]),t._v("\n")]),t._v(" "),e("p",[t._v("A class for training a "),e("code",[t._v("biome.text.Pipeline")]),t._v(".")]),t._v(" "),e("p",[t._v("It is basically a light wrapper around the awesome Pytorch Lightning Trainer to facilitate the interaction\nwith our pipelines.")]),t._v(" "),e("h2",{attrs:{id:"parameters"}},[t._v("Parameters")]),t._v(" "),e("dl",[e("dt",[e("strong",[e("code",[t._v("pipeline")])])]),t._v(" "),e("dd",[t._v("Pipeline to train")]),t._v(" "),e("dt",[e("strong",[e("code",[t._v("train_dataset")])])]),t._v(" "),e("dd",[t._v("The training dataset")]),t._v(" "),e("dt",[e("strong",[e("code",[t._v("valid_dataset")])])]),t._v(" "),e("dd",[t._v("The validation dataset. Default: "),e("code",[t._v("None")]),t._v(".")]),t._v(" "),e("dt",[e("strong",[e("code",[t._v("trainer_config")])])]),t._v(" "),e("dd",[t._v("The configuration of the trainer. Default: "),e("code",[t._v("LightningTrainerConfiguration()")]),t._v(".")]),t._v(" "),e("dt",[e("strong",[e("code",[t._v("vocab_config")])])]),t._v(" "),e("dd",[t._v("A "),e("code",[t._v("VocabularyConfiguration")]),t._v(" to create/extend the pipeline's vocabulary.\nIf "),e("code",[t._v('"default"')]),t._v(" (str), we will use the default configuration "),e("code",[t._v("VocabularyConfiguration()")]),t._v(".\nIf None, we will leave the pipeline's vocabulary untouched. Default: "),e("code",[t._v('"default"')]),t._v(".")]),t._v(" "),e("dt",[e("strong",[e("code",[t._v("lazy")])])]),t._v(" "),e("dd",[t._v("If True, instances are lazily loaded from disk, otherwise they are loaded into memory. Default: False.")])]),t._v(" "),e("dl",[e("pre",{staticClass:"title"},[e("h3",{attrs:{id:"fit"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#fit"}},[t._v("#")]),t._v(" fit "),e("Badge",{attrs:{text:"Method"}})],1),t._v("\n")]),t._v(" "),e("dt",[e("div",{staticClass:"language-python extra-class"},[e("pre",{staticClass:"language-python"},[e("code",[t._v("\n"),e("span",{staticClass:"token keyword"},[t._v("def")]),t._v(" "),e("span",{staticClass:"ident"},[t._v("fit")]),t._v("("),e("span",[t._v("self)")]),t._v("\n")]),t._v("\n")])])]),t._v(" "),e("dd",[e("p",[t._v("Train the pipeline")])])])])}),[],!1,null,null,null);a.default=i.exports}}]);