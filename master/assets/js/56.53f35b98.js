(window.webpackJsonp=window.webpackJsonp||[]).push([[56],{461:function(e,t,n){"use strict";n.r(t);var a=n(26),o=Object(a.a)({},(function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[n("h1",{attrs:{id:"biome-text-trainer"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#biome-text-trainer"}},[e._v("#")]),e._v(" biome.text.trainer "),n("Badge",{attrs:{text:"Module"}})],1),e._v(" "),n("div"),e._v(" "),n("pre",{staticClass:"title"},[n("h3",{attrs:{id:"create-dataloader"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#create-dataloader"}},[e._v("#")]),e._v(" create_dataloader "),n("Badge",{attrs:{text:"Function"}})],1),e._v("\n")]),e._v(" "),n("dt",[n("div",{staticClass:"language-python extra-class"},[n("pre",{staticClass:"language-python"},[n("code",[e._v("\n"),n("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),n("span",{staticClass:"ident"},[e._v("create_dataloader")]),e._v(" ("),e._v("\n  instance_dataset: Union[allennlp.data.dataset_readers.dataset_reader.AllennlpDataset, allennlp.data.dataset_readers.dataset_reader.AllennlpLazyDataset],\n  batch_size: int = 16,\n  data_bucketing: bool = False,\n)  -> allennlp.data.dataloader.PyTorchDataLoader\n")]),e._v("\n")])])]),e._v(" "),n("dd",[n("p",[e._v("Returns a pytorch DataLoader for AllenNLP instances")]),e._v(" "),n("h2",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),n("dl",[n("dt",[n("strong",[n("code",[e._v("instance_dataset")])])]),e._v(" "),n("dd",[e._v("The dataset of instances for the DataLoader")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("batch_size")])])]),e._v(" "),n("dd",[e._v("Batch size")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("data_bucketing")])])]),e._v(" "),n("dd",[e._v("If True, tries to sort batches with respect to the maximum input lengths per batch.\nNot supported for lazily loaded data!")])]),e._v(" "),n("h2",{attrs:{id:"returns"}},[e._v("Returns")]),e._v(" "),n("dl",[n("dt",[n("code",[e._v("data_loader")])]),e._v(" "),n("dd",[e._v(" ")])])]),e._v(" "),n("div"),e._v(" "),n("pre",{staticClass:"title"},[n("h2",{attrs:{id:"trainer"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#trainer"}},[e._v("#")]),e._v(" Trainer "),n("Badge",{attrs:{text:"Class"}})],1),e._v("\n")]),e._v(" "),n("pre",{staticClass:"language-python"},[n("code",[e._v("\n"),n("span",{staticClass:"token keyword"},[e._v("class")]),e._v(" "),n("span",{staticClass:"ident"},[e._v("Trainer")]),e._v(" ("),e._v("\n    "),n("span",[e._v("logger: Union[pytorch_lightning.loggers.base.LightningLoggerBase, Iterable[pytorch_lightning.loggers.base.LightningLoggerBase], bool] = True")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("checkpoint_callback: bool = True")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("callbacks: Union[List[pytorch_lightning.callbacks.base.Callback], pytorch_lightning.callbacks.base.Callback, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("default_root_dir: Union[str, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("gradient_clip_val: float = 0")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("process_position: int = 0")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("num_nodes: int = 1")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("num_processes: int = 1")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("gpus: Union[int, str, List[int], NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("auto_select_gpus: bool = False")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("tpu_cores: Union[int, str, List[int], NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("log_gpu_memory: Union[str, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("progress_bar_refresh_rate: Union[int, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("overfit_batches: Union[int, float] = 0.0")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("track_grad_norm: Union[int, float, str] = -1")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("check_val_every_n_epoch: int = 1")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("fast_dev_run: Union[int, bool] = False")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("accumulate_grad_batches: Union[int, Dict[int, int], List[list]] = 1")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("max_epochs: Union[int, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("min_epochs: Union[int, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("max_steps: Union[int, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("min_steps: Union[int, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("limit_train_batches: Union[int, float] = 1.0")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("limit_val_batches: Union[int, float] = 1.0")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("limit_test_batches: Union[int, float] = 1.0")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("limit_predict_batches: Union[int, float] = 1.0")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("val_check_interval: Union[int, float] = 1.0")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("flush_logs_every_n_steps: int = 100")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("log_every_n_steps: int = 50")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("accelerator: Union[str, pytorch_lightning.accelerators.accelerator.Accelerator, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("sync_batchnorm: bool = False")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("precision: int = 32")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("weights_summary: Union[str, NoneType] = 'top'")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("weights_save_path: Union[str, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("num_sanity_val_steps: int = 2")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("truncated_bptt_steps: Union[int, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("resume_from_checkpoint: Union[str, pathlib.Path, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("profiler: Union[pytorch_lightning.profiler.profilers.BaseProfiler, bool, str, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("benchmark: bool = False")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("deterministic: bool = False")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("reload_dataloaders_every_epoch: bool = False")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("auto_lr_find: Union[bool, str] = False")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("replace_sampler_ddp: bool = True")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("terminate_on_nan: bool = False")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("auto_scale_batch_size: Union[str, bool] = False")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("prepare_data_per_node: bool = True")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("plugins: Union[pytorch_lightning.plugins.base_plugin.Plugin, str, list, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("amp_backend: str = 'native'")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("amp_level: str = 'O2'")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("distributed_backend: Union[str, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("automatic_optimization: Union[bool, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("move_metrics_to_cpu: bool = False")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("multiple_trainloader_mode: str = 'max_size_cycle'")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("stochastic_weight_avg: bool = False")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("add_csv_logger: bool = True")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("add_tensorboard_logger: bool = True")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("add_wandb_logger: bool = True")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("batch_size: int = 16")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("data_bucketing: bool = False")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("optimizer: Union[Dict, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n"),n("span",[e._v(")")]),e._v("\n")]),e._v("\n")]),e._v(" "),n("p",[e._v("A class for training a "),n("code",[e._v("biome.text.Pipeline")]),e._v(".")]),e._v(" "),n("p",[e._v("It is basically a light wrapper around the awesome Pytorch Lightning Trainer to facilitate the interaction\nwith our pipelines. The docs are mainly a copy from the\n"),n("a",{attrs:{href:"https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.html#pytorch_lightning.trainer.trainer.Trainer"}},[e._v("Lightning Trainer API")]),e._v("\nwith some additional parameters added.")]),e._v(" "),n("h2",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),n("dl",[n("dt",[n("strong",[n("code",[e._v("accelerator")])])]),e._v(" "),n("dd",[e._v("Previously known as distributed_backend (dp, ddp, ddp2, etc…).\nCan also take in an accelerator object for custom hardware.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("accumulate_grad_batches")])])]),e._v(" "),n("dd",[e._v("Accumulates grads every k batches or as set up in the dict.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("add_csv_logger")])])]),e._v(" "),n("dd",[e._v("Adds a default CSV logger if "),n("code",[e._v("logger")]),e._v(" is not False. Default: True")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("add_tensorboard_logger")])])]),e._v(" "),n("dd",[e._v("Adds a default Tensorboard logger if "),n("code",[e._v("logger")]),e._v(" is not False. Default: True")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("add_wandb_logger")])])]),e._v(" "),n("dd",[e._v("Adds a default WandB logger if "),n("code",[e._v("logger")]),e._v(" is not False and wandb is installed. Default: True")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("amp_backend")])])]),e._v(" "),n("dd",[e._v('The mixed precision backend to use ("native" or "apex")')]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("amp_level")])])]),e._v(" "),n("dd",[e._v("The optimization level to use (O1, O2, etc…).")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("auto_lr_find")])])]),e._v(" "),n("dd",[e._v("If set to True, will make trainer.tune() run a learning rate finder,\ntrying to optimize initial learning for faster convergence. trainer.tune() method will\nset the suggested learning rate in self.lr or self.learning_rate in the LightningModule.\nTo use a different key set a string instead of True with the key name.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("auto_scale_batch_size")])])]),e._v(" "),n("dd",[e._v("If set to True, will "),n("code",[e._v("initially")]),e._v(" run a batch size\nfinder trying to find the largest batch size that fits into memory.\nThe result will be stored in self.batch_size in the LightningModule.\nAdditionally, can be set to either "),n("code",[e._v("power")]),e._v(" that estimates the batch size through\na power search or "),n("code",[e._v("binsearch")]),e._v(" that estimates the batch size through a binary search.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("auto_select_gpus")])])]),e._v(" "),n("dd",[e._v("If enabled and "),n("code",[e._v("gpus")]),e._v(' is an integer, pick available\ngpus automatically. This is especially useful when\nGPUs are configured to be in "exclusive mode", such\nthat only one process at a time can access them.')]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("batch_size")])])]),e._v(" "),n("dd",[e._v("Size of the batch.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("benchmark")])])]),e._v(" "),n("dd",[e._v("If true enables cudnn.benchmark.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("callbacks")])])]),e._v(" "),n("dd",[e._v("Add a callback or list of callbacks.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("checkpoint_callback")])])]),e._v(" "),n("dd",[e._v("If "),n("code",[e._v("True")]),e._v(", enable checkpointing.\nIt will configure a default ModelCheckpoint callback if there is no user-defined ModelCheckpoint in\n:paramref:"),n("code",[e._v("~pytorch_lightning.trainer.trainer.Trainer.callbacks")]),e._v(". Default: "),n("code",[e._v("True")]),e._v(".")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("check_val_every_n_epoch")])])]),e._v(" "),n("dd",[e._v("Check val every n train epochs.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("data_bucketing")])])]),e._v(" "),n("dd",[e._v("If enabled, try to apply data bucketing over training batches.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("default_root_dir")])])]),e._v(" "),n("dd",[e._v("Default path for logs and weights when no logger/ckpt_callback passed.\nCan be remote file paths such as 's3://mybucket/path' or 'hdfs://path/'\nDefault: './training_logs'.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("deterministic")])])]),e._v(" "),n("dd",[e._v("If true enables cudnn.deterministic.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("fast_dev_run")])])]),e._v(" "),n("dd",[e._v("runs n if set to "),n("code",[e._v("n")]),e._v(" (int) else 1 if set to "),n("code",[e._v("True")]),e._v(" batch(es)\nof train, val and test to find any bugs (ie: a sort of unit test).")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("flush_logs_every_n_steps")])])]),e._v(" "),n("dd",[e._v("How often to flush logs to disk (defaults to every 100 steps).")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("gpus")])])]),e._v(" "),n("dd",[e._v("number of gpus to train on (int) or which GPUs to train on (list or str) applied per node")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("gradient_clip_val")])])]),e._v(" "),n("dd",[e._v("0 means don't clip.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("limit_train_batches")])])]),e._v(" "),n("dd",[e._v("How much of training dataset to check (floats = percent, int = num_batches)")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("limit_val_batches")])])]),e._v(" "),n("dd",[e._v("How much of validation dataset to check (floats = percent, int = num_batches)")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("limit_test_batches")])])]),e._v(" "),n("dd",[e._v("How much of test dataset to check (floats = percent, int = num_batches)")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("log_every_n_steps")])])]),e._v(" "),n("dd",[e._v("How often to log within steps (defaults to every 50 steps).")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("log_gpu_memory")])])]),e._v(" "),n("dd",[e._v("None, 'min_max', 'all'. Might slow performance")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("logger")])])]),e._v(" "),n("dd",[e._v("Logger (or iterable collection of loggers) for experiment tracking.\nIf not False, we will add some loggers by default, see "),n("code",[e._v("add_[csv, tensorboard, wandb]_logger")]),e._v(".\nDefault: True")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("prepare_data_per_node")])])]),e._v(" "),n("dd",[e._v("If True, each LOCAL_RANK=0 will call prepare data.\nOtherwise only NODE_RANK=0, LOCAL_RANK=0 will prepare data")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("process_position")])])]),e._v(" "),n("dd",[e._v("orders the progress bar when running multiple models on same machine.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("progress_bar_refresh_rate")])])]),e._v(" "),n("dd",[e._v("How often to refresh progress bar (in steps). Value "),n("code",[e._v("0")]),e._v(" disables progress bar.\nIgnored when a custom progress bar is passed to :paramref:"),n("code",[e._v("~Trainer.callbacks")]),e._v(". Default: None, means\na suitable value will be chosen based on the environment (terminal, Google COLAB, etc.).")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("profiler")])])]),e._v(" "),n("dd",[e._v("To profile individual steps during training and assist in identifying bottlenecks. Passing bool\nvalue is deprecated in v1.1 and will be removed in v1.3.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("overfit_batches")])])]),e._v(" "),n("dd",[e._v("Overfit a percent of training data (float) or a set number of batches (int). Default: 0.0")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("plugins")])])]),e._v(" "),n("dd",[e._v("Plugins allow modification of core behavior like ddp and amp, and enable custom lightning plugins.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("precision")])])]),e._v(" "),n("dd",[e._v("Full precision (32), half precision (16). Can be used on CPU, GPU or TPUs.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("max_epochs")])])]),e._v(" "),n("dd",[e._v("Stop training once this number of epochs is reached. Disabled by default (None).\nIf both max_epochs and max_steps are not specified, defaults to "),n("code",[e._v("max_epochs")]),e._v(" = 1000.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("min_epochs")])])]),e._v(" "),n("dd",[e._v("Force training for at least these many epochs. Disabled by default (None).\nIf both min_epochs and min_steps are not specified, defaults to "),n("code",[e._v("min_epochs")]),e._v(" = 1.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("max_steps")])])]),e._v(" "),n("dd",[e._v("Stop training after this number of steps. Disabled by default (None).")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("min_steps")])])]),e._v(" "),n("dd",[e._v("Force training for at least these number of steps. Disabled by default (None).")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("num_nodes")])])]),e._v(" "),n("dd",[e._v("number of GPU nodes for distributed training.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("num_processes")])])]),e._v(" "),n("dd",[e._v('number of processes for distributed training with distributed_backend="ddp_cpu"')]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("num_sanity_val_steps")])])]),e._v(" "),n("dd",[e._v("Sanity check runs n validation batches before starting the training routine.\nSet it to "),n("code",[e._v("-1")]),e._v(" to run all batches in all validation dataloaders. Default: 2")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("optimizer")])])]),e._v(" "),n("dd",[e._v("Configuration for an "),n("a",{attrs:{href:"https://docs.allennlp.org/main/api/training/optimizers/"}},[e._v("AllenNLP/PyTorch optimizer")]),e._v("\nthat is constructed via the AllenNLP configuration framework.\nDefault: "),n("code",[e._v('{"type": "adam", "lr": 0.001}')])]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("reload_dataloaders_every_epoch")])])]),e._v(" "),n("dd",[e._v("Set to True to reload dataloaders every epoch.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("replace_sampler_ddp")])])]),e._v(" "),n("dd",[e._v("Explicitly enables or disables sampler replacement. If not specified this\nwill toggled automatically when DDP is used. By default it will add "),n("code",[e._v("shuffle=True")]),e._v(" for\ntrain sampler and "),n("code",[e._v("shuffle=False")]),e._v(" for val/test sampler. If you want to customize it,\nyou can set "),n("code",[e._v("replace_sampler_ddp=False")]),e._v(" and add your own distributed sampler.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("resume_from_checkpoint")])])]),e._v(" "),n("dd",[e._v("Path/URL of the checkpoint from which training is resumed. If there is\nno checkpoint file at the path, start from scratch. If resuming from mid-epoch checkpoint,\ntraining will start from the beginning of the next epoch.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("sync_batchnorm")])])]),e._v(" "),n("dd",[e._v("Synchronize batch norm layers between process groups/whole world.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("terminate_on_nan")])])]),e._v(" "),n("dd",[e._v("If set to True, will terminate training (by raising a "),n("code",[e._v("ValueError")]),e._v(") at the\nend of each training batch, if any of the parameters or the loss are NaN or +/-inf.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("tpu_cores")])])]),e._v(" "),n("dd",[e._v("How many TPU cores to train on (1 or 8) / Single TPU to train on [1]")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("track_grad_norm")])])]),e._v(" "),n("dd",[e._v("-1 no tracking. Otherwise tracks that p-norm. May be set to 'inf' infinity-norm.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("truncated_bptt_steps")])])]),e._v(" "),n("dd",[e._v("Truncated back prop breaks performs backprop every k steps of much longer\nsequence.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("val_check_interval")])])]),e._v(" "),n("dd",[e._v("How often to check the validation set. Use float to check within a training epoch,\nuse int to check every n steps (batches).")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("weights_summary")])])]),e._v(" "),n("dd",[e._v("Prints a summary of the weights when training begins.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("weights_save_path")])])]),e._v(" "),n("dd",[e._v("Where to save weights if specified. Will override default_root_dir\nfor checkpoints only. Use this if for whatever reason you need the checkpoints\nstored in a different place than the logs written in "),n("code",[e._v("default_root_dir")]),e._v(".\nCan be remote file paths such as "),n("code",[e._v("s3://mybucket/path")]),e._v(" or 'hdfs://path/'\nDefaults to "),n("code",[e._v("default_root_dir")]),e._v(".")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("move_metrics_to_cpu")])])]),e._v(" "),n("dd",[e._v("Whether to force internal logged metrics to be moved to cpu.\nThis can save some gpu memory, but can make training slower. Use with attention.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("multiple_trainloader_mode")])])]),e._v(" "),n("dd",[e._v("How to loop over the datasets when there are multiple train loaders.\nIn 'max_size_cycle' mode, the trainer ends one epoch when the largest dataset is traversed,\nand smaller datasets reload when running out of their data. In 'min_size' mode, all the datasets\nreload when reaching the minimum length of datasets.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("stochastic_weight_avg")])])]),e._v(" "),n("dd",[e._v("Whether to use "),n("code",[e._v("Stochastic Weight Averaging (SWA)\n<https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/>_")])])]),e._v(" "),n("dl",[n("pre",{staticClass:"title"},[n("h3",{attrs:{id:"fit"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#fit"}},[e._v("#")]),e._v(" fit "),n("Badge",{attrs:{text:"Method"}})],1),e._v("\n")]),e._v(" "),n("dt",[n("div",{staticClass:"language-python extra-class"},[n("pre",{staticClass:"language-python"},[n("code",[e._v("\n"),n("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),n("span",{staticClass:"ident"},[e._v("fit")]),e._v(" ("),e._v("\n  self,\n  pipeline: "),n("a",{attrs:{title:"biome.text.pipeline.Pipeline",href:"pipeline.html#biome.text.pipeline.Pipeline"}},[e._v("Pipeline")]),e._v(",\n  train_dataset: "),n("a",{attrs:{title:"biome.text.dataset.Dataset",href:"dataset.html#biome.text.dataset.Dataset"}},[e._v("Dataset")]),e._v(",\n  valid_dataset: Union["),n("a",{attrs:{title:"biome.text.dataset.Dataset",href:"dataset.html#biome.text.dataset.Dataset"}},[e._v("Dataset")]),e._v(", NoneType] = None,\n  vocab_config: Union["),n("a",{attrs:{title:"biome.text.configuration.VocabularyConfiguration",href:"configuration.html#biome.text.configuration.VocabularyConfiguration"}},[e._v("VocabularyConfiguration")]),e._v(", str, NoneType] = 'default',\n  include_valid_data_in_vocab: bool = False,\n  lazy: bool = False,\n) \n")]),e._v("\n")])])]),e._v(" "),n("dd",[n("p",[e._v("Train the pipeline")]),e._v(" "),n("h2",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),n("dl",[n("dt",[n("strong",[n("code",[e._v("pipeline")])])]),e._v(" "),n("dd",[e._v("Pipeline to train")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("train_dataset")])])]),e._v(" "),n("dd",[e._v("The training dataset")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("valid_dataset")])])]),e._v(" "),n("dd",[e._v("The validation dataset")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("vocab_config")])])]),e._v(" "),n("dd",[e._v("A "),n("code",[e._v("VocabularyConfiguration")]),e._v(" to create/extend the pipeline's vocabulary.\nIf 'default' (str), we will use the default configuration "),n("code",[e._v("VocabularyConfiguration()")]),e._v(".\nIf None, we will leave the pipeline's vocabulary untouched. Default: 'default'.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("include_valid_data_in_vocab")])])]),e._v(" "),n("dd",[e._v("If True, take the validation data into account when creating the vocabulary (apart from the training data).\nHas no effect if "),n("code",[e._v("vocab_config")]),e._v(" is None. Default: False.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("lazy")])])]),e._v(" "),n("dd",[e._v("If True, instances are lazily loaded from disk, otherwise they are loaded into memory. Default: False.")])])])])])}),[],!1,null,null,null);t.default=o.exports}}]);