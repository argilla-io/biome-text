(window.webpackJsonp=window.webpackJsonp||[]).push([[21],{426:function(e,t,n){"use strict";n.r(t);var a=n(26),s=Object(a.a)({},(function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[n("h1",{attrs:{id:"biome-text-configuration"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#biome-text-configuration"}},[e._v("#")]),e._v(" biome.text.configuration "),n("Badge",{attrs:{text:"Module"}})],1),e._v(" "),n("div"),e._v(" "),n("div"),e._v(" "),n("pre",{staticClass:"title"},[n("h2",{attrs:{id:"featuresconfiguration"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#featuresconfiguration"}},[e._v("#")]),e._v(" FeaturesConfiguration "),n("Badge",{attrs:{text:"Class"}})],1),e._v("\n")]),e._v(" "),n("pre",{staticClass:"language-python"},[n("code",[e._v("\n"),n("span",{staticClass:"token keyword"},[e._v("class")]),e._v(" "),n("span",{staticClass:"ident"},[e._v("FeaturesConfiguration")]),e._v(" ("),e._v("\n    "),n("span",[e._v("word: Union["),n("a",{attrs:{title:"biome.text.features.WordFeatures",href:"features.html#biome.text.features.WordFeatures"}},[e._v("WordFeatures")]),e._v(", NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("char: Union["),n("a",{attrs:{title:"biome.text.features.CharFeatures",href:"features.html#biome.text.features.CharFeatures"}},[e._v("CharFeatures")]),e._v(", NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("transformers: Union["),n("a",{attrs:{title:"biome.text.features.TransformersFeatures",href:"features.html#biome.text.features.TransformersFeatures"}},[e._v("TransformersFeatures")]),e._v(", NoneType] = None")]),n("span",[e._v(",")]),e._v("\n"),n("span",[e._v(")")]),e._v("\n")]),e._v("\n")]),e._v(" "),n("p",[e._v("Configures the input features of the "),n("code",[e._v("Pipeline")])]),e._v(" "),n("p",[e._v("Use this for defining the features to be used by the model, namely word and character embeddings.")]),e._v(" "),n("p",[e._v(":::tip\nIf you do not pass in either of the parameters ("),n("code",[e._v("word")]),e._v(" or "),n("code",[e._v("char")]),e._v("),\nyour pipeline will be setup with a default word feature (embedding_dim=50).\n:::")]),e._v(" "),n("p",[e._v("Example:")]),e._v(" "),n("pre",[n("code",{staticClass:"language-python"},[e._v("word = WordFeatures(embedding_dim=100)\nchar = CharFeatures(embedding_dim=16, encoder={'type': 'gru'})\nconfig = FeaturesConfiguration(word, char)\n")])]),e._v(" "),n("h2",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),n("dl",[n("dt",[n("strong",[n("code",[e._v("word")])])]),e._v(" "),n("dd",[e._v("The word feature configurations, see "),n("code",[n("a",{attrs:{title:"biome.text.features.WordFeatures",href:"features.html#biome.text.features.WordFeatures"}},[e._v("WordFeatures")])])]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("char")])])]),e._v(" "),n("dd",[e._v("The character feature configurations, see "),n("code",[n("a",{attrs:{title:"biome.text.features.CharFeatures",href:"features.html#biome.text.features.CharFeatures"}},[e._v("CharFeatures")])])]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("transformers")])])]),e._v(" "),n("dd",[e._v("The transformers feature configuration, see "),n("code",[n("a",{attrs:{title:"biome.text.features.TransformersFeatures",href:"features.html#biome.text.features.TransformersFeatures"}},[e._v("TransformersFeatures")])]),e._v("\nA word-level representation of the "),n("a",{attrs:{href:"https://huggingface.co/models"}},[e._v("transformer")]),e._v(" models using AllenNLP's")])]),e._v(" "),n("pre",{staticClass:"title"},[n("h3",{attrs:{id:"ancestors"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#ancestors"}},[e._v("#")]),e._v(" Ancestors")]),e._v("\n")]),e._v(" "),n("ul",{staticClass:"hlist"},[n("li",[e._v("allennlp.common.from_params.FromParams")])]),e._v(" "),n("dl",[n("pre",{staticClass:"title"},[n("h3",{attrs:{id:"from-params"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#from-params"}},[e._v("#")]),e._v(" from_params "),n("Badge",{attrs:{text:"Static method"}})],1),e._v("\n")]),e._v(" "),n("dt",[n("div",{staticClass:"language-python extra-class"},[n("pre",{staticClass:"language-python"},[n("code",[e._v("\n"),n("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),n("span",{staticClass:"ident"},[e._v("from_params")]),e._v(" ("),e._v("\n  params: allennlp.common.params.Params,\n  **extras,\n)  -> "),n("a",{attrs:{title:"biome.text.configuration.FeaturesConfiguration",href:"#biome.text.configuration.FeaturesConfiguration"}},[e._v("FeaturesConfiguration")]),e._v("\n")]),e._v("\n")])])]),e._v(" "),n("dd",[n("p",[e._v("This is the automatic implementation of "),n("code",[e._v("from_params")]),e._v(". Any class that subclasses\n"),n("code",[e._v("FromParams")]),e._v(" (or "),n("code",[e._v("Registrable")]),e._v(", which itself subclasses "),n("code",[e._v("FromParams")]),e._v(') gets this\nimplementation for free.\nIf you want your class to be instantiated from params in the\n"obvious" way – pop off parameters and hand them to your constructor with the same names –\nthis provides that functionality.')]),e._v(" "),n("p",[e._v("If you need more complex logic in your from "),n("code",[e._v("from_params")]),e._v(" method, you'll have to implement\nyour own method that overrides this one.")]),e._v(" "),n("p",[e._v("The "),n("code",[e._v("constructor_to_call")]),e._v(" and "),n("code",[e._v("constructor_to_inspect")]),e._v(" arguments deal with a bit of\nredirection that we do.\nWe allow you to register particular "),n("code",[e._v("@classmethods")]),e._v(" on a class as\nthe constructor to use for a registered name.\nThis lets you, e.g., have a single\n"),n("code",[e._v("Vocabulary")]),e._v(" class that can be constructed in two different ways, with different names\nregistered to each constructor.\nIn order to handle this, we need to know not just the class\nwe're trying to construct ("),n("code",[e._v("cls")]),e._v("), but also what method we should inspect to find its\narguments ("),n("code",[e._v("constructor_to_inspect")]),e._v("), and what method to call when we're done constructing\narguments ("),n("code",[e._v("constructor_to_call")]),e._v(").\nThese two methods are the same when you've used a\n"),n("code",[e._v("@classmethod")]),e._v(" as your constructor, but they are "),n("code",[e._v("different")]),e._v(" when you use the default\nconstructor (because you inspect "),n("code",[e._v("__init__")]),e._v(", but call "),n("code",[e._v("cls()")]),e._v(").")])])]),e._v(" "),n("pre",{staticClass:"title"},[n("h3",{attrs:{id:"instance-variables"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#instance-variables"}},[e._v("#")]),e._v(" Instance variables")]),e._v("\n")]),e._v(" "),n("dl",[n("dt",{attrs:{id:"biome.text.configuration.FeaturesConfiguration.configured_namespaces"}},[n("code",{staticClass:"name"},[e._v("var "),n("span",{staticClass:"ident"},[e._v("configured_namespaces")]),e._v(" : List[str]")])]),e._v(" "),n("dd",[n("p",[e._v("Return the namespaces of the features that are configured")])])]),e._v(" "),n("dl",[n("pre",{staticClass:"title"},[n("h3",{attrs:{id:"compile-embedder"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#compile-embedder"}},[e._v("#")]),e._v(" compile_embedder "),n("Badge",{attrs:{text:"Method"}})],1),e._v("\n")]),e._v(" "),n("dt",[n("div",{staticClass:"language-python extra-class"},[n("pre",{staticClass:"language-python"},[n("code",[e._v("\n"),n("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),n("span",{staticClass:"ident"},[e._v("compile_embedder")]),e._v(" ("),e._v("\n  self,\n  vocab: allennlp.data.vocabulary.Vocabulary,\n)  -> allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder\n")]),e._v("\n")])])]),e._v(" "),n("dd",[n("p",[e._v("Creates the embedder based on the configured input features")]),e._v(" "),n("h2",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),n("dl",[n("dt",[n("strong",[n("code",[e._v("vocab")])])]),e._v(" "),n("dd",[e._v("The vocabulary for which to create the embedder")])]),e._v(" "),n("h2",{attrs:{id:"returns"}},[e._v("Returns")]),e._v(" "),n("dl",[n("dt",[n("code",[e._v("embedder")])]),e._v(" "),n("dd",[e._v(" ")])])]),e._v(" "),n("pre",{staticClass:"title"},[n("h3",{attrs:{id:"compile-featurizer"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#compile-featurizer"}},[e._v("#")]),e._v(" compile_featurizer "),n("Badge",{attrs:{text:"Method"}})],1),e._v("\n")]),e._v(" "),n("dt",[n("div",{staticClass:"language-python extra-class"},[n("pre",{staticClass:"language-python"},[n("code",[e._v("\n"),n("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),n("span",{staticClass:"ident"},[e._v("compile_featurizer")]),e._v(" ("),e._v("\n  self,\n  tokenizer: "),n("a",{attrs:{title:"biome.text.tokenizer.Tokenizer",href:"tokenizer.html#biome.text.tokenizer.Tokenizer"}},[e._v("Tokenizer")]),e._v(",\n)  -> "),n("a",{attrs:{title:"biome.text.featurizer.InputFeaturizer",href:"featurizer.html#biome.text.featurizer.InputFeaturizer"}},[e._v("InputFeaturizer")]),e._v("\n")]),e._v("\n")])])]),e._v(" "),n("dd",[n("p",[e._v("Creates the featurizer based on the configured input features")]),e._v(" "),n("p",[e._v(":::tip\nIf you are creating configurations programmatically\nuse this method to check that you provided a valid configuration.\n:::")]),e._v(" "),n("h2",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),n("dl",[n("dt",[n("strong",[n("code",[e._v("tokenizer")])])]),e._v(" "),n("dd",[e._v("Tokenizer used for this featurizer")])]),e._v(" "),n("h2",{attrs:{id:"returns"}},[e._v("Returns")]),e._v(" "),n("dl",[n("dt",[n("code",[e._v("featurizer")])]),e._v(" "),n("dd",[e._v("The configured "),n("code",[e._v("InputFeaturizer")])])])])]),e._v(" "),n("div"),e._v(" "),n("pre",{staticClass:"title"},[n("h2",{attrs:{id:"tokenizerconfiguration"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#tokenizerconfiguration"}},[e._v("#")]),e._v(" TokenizerConfiguration "),n("Badge",{attrs:{text:"Class"}})],1),e._v("\n")]),e._v(" "),n("pre",{staticClass:"language-python"},[n("code",[e._v("\n"),n("span",{staticClass:"token keyword"},[e._v("class")]),e._v(" "),n("span",{staticClass:"ident"},[e._v("TokenizerConfiguration")]),e._v(" ("),e._v("\n    "),n("span",[e._v("lang: str = 'en'")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("max_sequence_length: int = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("max_nr_of_sentences: int = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("text_cleaning: Union[Dict[str, Any], NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("segment_sentences: bool = False")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("use_spacy_tokens: bool = False")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("remove_space_tokens: bool = True")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("start_tokens: Union[List[str], NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("end_tokens: Union[List[str], NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("use_transformers: Union[bool, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("transformers_kwargs: Union[Dict, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n"),n("span",[e._v(")")]),e._v("\n")]),e._v("\n")]),e._v(" "),n("p",[e._v("Configures the "),n("code",[e._v("Tokenizer")])]),e._v(" "),n("h2",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),n("dl",[n("dt",[n("strong",[n("code",[e._v("lang")])])]),e._v(" "),n("dd",[e._v("The "),n("a",{attrs:{href:"https://spacy.io/api/tokenizer"}},[e._v("spaCy model used")]),e._v(' for tokenization is language dependent.\nFor optimal performance, specify the language of your input data (default: "en").')]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("max_sequence_length")])])]),e._v(" "),n("dd",[e._v("Maximum length in characters for input texts truncated with "),n("code",[e._v("[:max_sequence_length]")]),e._v(" after "),n("code",[e._v("TextCleaning")]),e._v(".")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("max_nr_of_sentences")])])]),e._v(" "),n("dd",[e._v("Maximum number of sentences to keep when using "),n("code",[e._v("segment_sentences")]),e._v(" truncated with "),n("code",[e._v("[:max_sequence_length]")]),e._v(".")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("text_cleaning")])])]),e._v(" "),n("dd",[e._v("A "),n("code",[e._v("TextCleaning")]),e._v(" configuration with pre-processing rules for cleaning up and transforming raw input text.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("segment_sentences")])])]),e._v(" "),n("dd",[e._v("Whether to segment input texts into sentences.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("use_spacy_tokens")])])]),e._v(" "),n("dd",[e._v("If True, the tokenized token list contains spacy tokens instead of allennlp tokens")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("remove_space_tokens")])])]),e._v(" "),n("dd",[e._v("If True, all found space tokens will be removed from the final token list.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("start_tokens")])])]),e._v(" "),n("dd",[e._v("A list of token strings to the sequence before tokenized input text.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("end_tokens")])])]),e._v(" "),n("dd",[e._v("A list of token strings to the sequence after tokenized input text.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("use_transformers")])])]),e._v(" "),n("dd",[e._v("If true, we will use a transformers tokenizer from HuggingFace and disregard all other parameters above.\nIf you specify any of the above parameters you want to set this to false.\nIf None, we automatically choose the right value based on your feature and head configuration.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("transformers_kwargs")])])]),e._v(" "),n("dd",[e._v("This dict is passed on to AllenNLP's "),n("code",[e._v("PretrainedTransformerTokenizer")]),e._v(".\nIf no "),n("code",[e._v("model_name")]),e._v(" key is provided, we will infer one from the features configuration.")])]),e._v(" "),n("pre",{staticClass:"title"},[n("h3",{attrs:{id:"ancestors-2"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#ancestors-2"}},[e._v("#")]),e._v(" Ancestors")]),e._v("\n")]),e._v(" "),n("ul",{staticClass:"hlist"},[n("li",[e._v("allennlp.common.from_params.FromParams")])]),e._v(" "),n("div"),e._v(" "),n("pre",{staticClass:"title"},[n("h2",{attrs:{id:"pipelineconfiguration"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#pipelineconfiguration"}},[e._v("#")]),e._v(" PipelineConfiguration "),n("Badge",{attrs:{text:"Class"}})],1),e._v("\n")]),e._v(" "),n("pre",{staticClass:"language-python"},[n("code",[e._v("\n"),n("span",{staticClass:"token keyword"},[e._v("class")]),e._v(" "),n("span",{staticClass:"ident"},[e._v("PipelineConfiguration")]),e._v(" ("),e._v("\n    "),n("span",[e._v("name: str")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("head: "),n("a",{attrs:{title:"biome.text.modules.heads.task_head.TaskHeadConfiguration",href:"modules/heads/task_head.html#biome.text.modules.heads.task_head.TaskHeadConfiguration"}},[e._v("TaskHeadConfiguration")])]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("features: Union["),n("a",{attrs:{title:"biome.text.configuration.FeaturesConfiguration",href:"#biome.text.configuration.FeaturesConfiguration"}},[e._v("FeaturesConfiguration")]),e._v(", NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("tokenizer: Union["),n("a",{attrs:{title:"biome.text.configuration.TokenizerConfiguration",href:"#biome.text.configuration.TokenizerConfiguration"}},[e._v("TokenizerConfiguration")]),e._v(", NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("encoder: Union["),n("a",{attrs:{title:"biome.text.modules.configuration.allennlp_configuration.Seq2SeqEncoderConfiguration",href:"modules/configuration/allennlp_configuration.html#biome.text.modules.configuration.allennlp_configuration.Seq2SeqEncoderConfiguration"}},[e._v("Seq2SeqEncoderConfiguration")]),e._v(", NoneType] = None")]),n("span",[e._v(",")]),e._v("\n"),n("span",[e._v(")")]),e._v("\n")]),e._v("\n")]),e._v(" "),n("p",[e._v("Creates a "),n("code",[e._v("Pipeline")]),e._v(" configuration")]),e._v(" "),n("h2",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),n("dl",[n("dt",[n("strong",[n("code",[e._v("name")])])]),e._v(" "),n("dd",[e._v("The "),n("code",[e._v("name")]),e._v(" for our pipeline")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("features")])])]),e._v(" "),n("dd",[e._v("The input "),n("code",[e._v("features")]),e._v(" to be used by the model pipeline. We define this using a "),n("code",[n("a",{attrs:{title:"biome.text.configuration.FeaturesConfiguration",href:"#biome.text.configuration.FeaturesConfiguration"}},[e._v("FeaturesConfiguration")])]),e._v(" object.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("head")])])]),e._v(" "),n("dd",[e._v("The "),n("code",[e._v("head")]),e._v(" for the task, e.g., a LanguageModelling task, using a "),n("code",[e._v("TaskHeadConfiguration")]),e._v(" object.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("tokenizer")])])]),e._v(" "),n("dd",[e._v("The "),n("code",[e._v("tokenizer")]),e._v(" defined with a "),n("code",[n("a",{attrs:{title:"biome.text.configuration.TokenizerConfiguration",href:"#biome.text.configuration.TokenizerConfiguration"}},[e._v("TokenizerConfiguration")])]),e._v(" object.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("encoder")])])]),e._v(" "),n("dd",[e._v("The core text seq2seq "),n("code",[e._v("encoder")]),e._v(" of our model using a "),n("code",[e._v("Seq2SeqEncoderConfiguration")])])]),e._v(" "),n("pre",{staticClass:"title"},[n("h3",{attrs:{id:"ancestors-3"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#ancestors-3"}},[e._v("#")]),e._v(" Ancestors")]),e._v("\n")]),e._v(" "),n("ul",{staticClass:"hlist"},[n("li",[e._v("allennlp.common.from_params.FromParams")])]),e._v(" "),n("dl",[n("pre",{staticClass:"title"},[n("h3",{attrs:{id:"from-yaml"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#from-yaml"}},[e._v("#")]),e._v(" from_yaml "),n("Badge",{attrs:{text:"Static method"}})],1),e._v("\n")]),e._v(" "),n("dt",[n("div",{staticClass:"language-python extra-class"},[n("pre",{staticClass:"language-python"},[n("code",[e._v("\n"),n("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),n("span",{staticClass:"ident"},[e._v("from_yaml")]),e._v("("),n("span",[e._v("path: str) -> "),n("a",{attrs:{title:"biome.text.configuration.PipelineConfiguration",href:"#biome.text.configuration.PipelineConfiguration"}},[e._v("PipelineConfiguration")])]),e._v("\n")]),e._v("\n")])])]),e._v(" "),n("dd",[n("p",[e._v("Creates a pipeline configuration from a config yaml file")]),e._v(" "),n("h2",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),n("dl",[n("dt",[n("strong",[n("code",[e._v("path")])])]),e._v(" "),n("dd",[e._v("The path to a YAML configuration file")])]),e._v(" "),n("h2",{attrs:{id:"returns"}},[e._v("Returns")]),e._v(" "),n("dl",[n("dt",[n("code",[e._v("pipeline_configuration")])]),e._v(" "),n("dd",[e._v(" ")])])]),e._v(" "),n("pre",{staticClass:"title"},[n("h3",{attrs:{id:"from-dict"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#from-dict"}},[e._v("#")]),e._v(" from_dict "),n("Badge",{attrs:{text:"Static method"}})],1),e._v("\n")]),e._v(" "),n("dt",[n("div",{staticClass:"language-python extra-class"},[n("pre",{staticClass:"language-python"},[n("code",[e._v("\n"),n("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),n("span",{staticClass:"ident"},[e._v("from_dict")]),e._v("("),n("span",[e._v("config_dict: dict) -> "),n("a",{attrs:{title:"biome.text.configuration.PipelineConfiguration",href:"#biome.text.configuration.PipelineConfiguration"}},[e._v("PipelineConfiguration")])]),e._v("\n")]),e._v("\n")])])]),e._v(" "),n("dd",[n("p",[e._v("Creates a pipeline configuration from a config dictionary")]),e._v(" "),n("h2",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),n("dl",[n("dt",[n("strong",[n("code",[e._v("config_dict")])])]),e._v(" "),n("dd",[e._v("A configuration dictionary")])]),e._v(" "),n("h2",{attrs:{id:"returns"}},[e._v("Returns")]),e._v(" "),n("dl",[n("dt",[n("code",[e._v("pipeline_configuration")])]),e._v(" "),n("dd",[e._v(" ")])])])]),e._v(" "),n("dl",[n("pre",{staticClass:"title"},[n("h3",{attrs:{id:"as-dict"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#as-dict"}},[e._v("#")]),e._v(" as_dict "),n("Badge",{attrs:{text:"Method"}})],1),e._v("\n")]),e._v(" "),n("dt",[n("div",{staticClass:"language-python extra-class"},[n("pre",{staticClass:"language-python"},[n("code",[e._v("\n"),n("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),n("span",{staticClass:"ident"},[e._v("as_dict")]),e._v("("),n("span",[e._v("self) -> Dict[str, Any]")]),e._v("\n")]),e._v("\n")])])]),e._v(" "),n("dd",[n("p",[e._v("Returns the configuration as dictionary")]),e._v(" "),n("h2",{attrs:{id:"returns"}},[e._v("Returns")]),e._v(" "),n("dl",[n("dt",[n("code",[e._v("config")])]),e._v(" "),n("dd",[e._v(" ")])])]),e._v(" "),n("pre",{staticClass:"title"},[n("h3",{attrs:{id:"to-yaml"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#to-yaml"}},[e._v("#")]),e._v(" to_yaml "),n("Badge",{attrs:{text:"Method"}})],1),e._v("\n")]),e._v(" "),n("dt",[n("div",{staticClass:"language-python extra-class"},[n("pre",{staticClass:"language-python"},[n("code",[e._v("\n"),n("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),n("span",{staticClass:"ident"},[e._v("to_yaml")]),e._v(" ("),e._v("\n  self,\n  path: str,\n) \n")]),e._v("\n")])])]),e._v(" "),n("dd",[n("p",[e._v("Saves the pipeline configuration to a yaml formatted file")]),e._v(" "),n("h2",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),n("dl",[n("dt",[n("strong",[n("code",[e._v("path")])])]),e._v(" "),n("dd",[e._v("Path to the output file")])])]),e._v(" "),n("pre",{staticClass:"title"},[n("h3",{attrs:{id:"build-tokenizer"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#build-tokenizer"}},[e._v("#")]),e._v(" build_tokenizer "),n("Badge",{attrs:{text:"Method"}})],1),e._v("\n")]),e._v(" "),n("dt",[n("div",{staticClass:"language-python extra-class"},[n("pre",{staticClass:"language-python"},[n("code",[e._v("\n"),n("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),n("span",{staticClass:"ident"},[e._v("build_tokenizer")]),e._v("("),n("span",[e._v("self) -> "),n("a",{attrs:{title:"biome.text.tokenizer.Tokenizer",href:"tokenizer.html#biome.text.tokenizer.Tokenizer"}},[e._v("Tokenizer")])]),e._v("\n")]),e._v("\n")])])]),e._v(" "),n("dd",[n("p",[e._v("Build the pipeline tokenizer")])]),e._v(" "),n("pre",{staticClass:"title"},[n("h3",{attrs:{id:"build-featurizer"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#build-featurizer"}},[e._v("#")]),e._v(" build_featurizer "),n("Badge",{attrs:{text:"Method"}})],1),e._v("\n")]),e._v(" "),n("dt",[n("div",{staticClass:"language-python extra-class"},[n("pre",{staticClass:"language-python"},[n("code",[e._v("\n"),n("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),n("span",{staticClass:"ident"},[e._v("build_featurizer")]),e._v("("),n("span",[e._v("self) -> "),n("a",{attrs:{title:"biome.text.featurizer.InputFeaturizer",href:"featurizer.html#biome.text.featurizer.InputFeaturizer"}},[e._v("InputFeaturizer")])]),e._v("\n")]),e._v("\n")])])]),e._v(" "),n("dd",[n("p",[e._v("Creates the pipeline featurizer")])]),e._v(" "),n("pre",{staticClass:"title"},[n("h3",{attrs:{id:"build-embedder"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#build-embedder"}},[e._v("#")]),e._v(" build_embedder "),n("Badge",{attrs:{text:"Method"}})],1),e._v("\n")]),e._v(" "),n("dt",[n("div",{staticClass:"language-python extra-class"},[n("pre",{staticClass:"language-python"},[n("code",[e._v("\n"),n("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),n("span",{staticClass:"ident"},[e._v("build_embedder")]),e._v(" ("),e._v("\n  self,\n  vocab: allennlp.data.vocabulary.Vocabulary,\n) \n")]),e._v("\n")])])]),e._v(" "),n("dd",[n("p",[e._v("Build the pipeline embedder for aiven dictionary")])])]),e._v(" "),n("div"),e._v(" "),n("pre",{staticClass:"title"},[n("h2",{attrs:{id:"trainerconfiguration"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#trainerconfiguration"}},[e._v("#")]),e._v(" TrainerConfiguration "),n("Badge",{attrs:{text:"Class"}})],1),e._v("\n")]),e._v(" "),n("pre",{staticClass:"language-python"},[n("code",[e._v("\n"),n("span",{staticClass:"token keyword"},[e._v("class")]),e._v(" "),n("span",{staticClass:"ident"},[e._v("TrainerConfiguration")]),e._v(" ("),e._v("\n    "),n("span",[e._v("optimizer: Dict[str, Any] = <factory>")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("validation_metric: str = '-loss'")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("patience: Union[int, NoneType] = 2")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("num_epochs: int = 20")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("cuda_device: Union[int, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("grad_norm: Union[float, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("grad_clipping: Union[float, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("learning_rate_scheduler: Union[Dict[str, Any], NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("warmup_steps: int = 0")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("linear_decay: bool = False")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("training_size: Union[int, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("momentum_scheduler: Union[Dict[str, Any], NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("moving_average: Union[Dict[str, Any], NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("use_amp: bool = False")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("num_serialized_models_to_keep: int = 1")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("batch_size: Union[int, NoneType] = 16")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("data_bucketing: bool = False")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("batches_per_epoch: Union[int, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("random_seed: Union[int, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n"),n("span",[e._v(")")]),e._v("\n")]),e._v("\n")]),e._v(" "),n("p",[e._v("Configures the training of a pipeline")]),e._v(" "),n("p",[e._v("It is passed on to the "),n("code",[e._v("Pipeline.train")]),e._v(" method. Doc strings mainly provided by\n"),n("a",{attrs:{href:"https://docs.allennlp.org/master/api/training/trainer/#gradientdescenttrainer-objects"}},[e._v("AllenNLP")])]),e._v(" "),n("h2",{attrs:{id:"attributes"}},[e._v("Attributes")]),e._v(" "),n("dl",[n("dt",[n("strong",[n("code",[e._v("optimizer")])])]),e._v(" "),n("dd",[n("a",{attrs:{href:"https://pytorch.org/docs/stable/optim.html"}},[e._v("Pytorch optimizers")]),e._v("\nthat can be constructed via the AllenNLP configuration framework")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("validation_metric")])])]),e._v(" "),n("dd",[e._v('Validation metric to measure for whether to stop training using patience\nand whether to serialize an is_best model each epoch.\nThe metric name must be prepended with either "+" or "-",\nwhich specifies whether the metric is an increasing or decreasing function.')]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("patience")])])]),e._v(" "),n("dd",[e._v("Number of epochs to be patient before early stopping:\nthe training is stopped after "),n("code",[e._v("patience")]),e._v(" epochs with no improvement.\nIf given, it must be > 0. If "),n("code",[e._v("None")]),e._v(", early stopping is disabled.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("num_epochs")])])]),e._v(" "),n("dd",[e._v("Number of training epochs")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("cuda_device")])])]),e._v(" "),n("dd",[e._v("An integer specifying the CUDA device to use for this process. If -1, the CPU is used.\nBy default (None) we will automatically use a CUDA device if one is available.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("grad_norm")])])]),e._v(" "),n("dd",[e._v("If provided, gradient norms will be rescaled to have a maximum of this value.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("grad_clipping")])])]),e._v(" "),n("dd",[e._v("If provided, gradients will be clipped during the backward pass to have an (absolute) maximum of this value.\nIf you are getting "),n("code",[e._v("NaN")]),e._v("s in your gradients during training that are not solved by using grad_norm,\nyou may need this.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("learning_rate_scheduler")])])]),e._v(" "),n("dd",[e._v("If specified, the learning rate will be decayed with respect to this schedule at the end of each epoch\n(or batch, if the scheduler implements the step_batch method).\nIf you use "),n("code",[e._v("torch.optim.lr_scheduler.ReduceLROnPlateau")]),e._v(", this will use the "),n("code",[e._v("validation_metric")]),e._v(" provided\nto determine if learning has plateaued.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("warmup_steps")])])]),e._v(" "),n("dd",[e._v("The number of warmup steps. This parameter is ignored if a "),n("code",[e._v("learning_rate_scheduler")]),e._v(" is provided.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("linear_decay")])])]),e._v(" "),n("dd",[e._v("If true, linearly decrease the learning rate to 0 until the end of the training. See also "),n("code",[e._v("training_size")]),e._v(" below!\nThis parameter is ignored if a "),n("code",[e._v("learning_rate_scheduler")]),e._v(" is provided.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("training_size")])])]),e._v(" "),n("dd",[e._v("If you set "),n("code",[e._v("linear_decay")]),e._v(" to true and work with lazily loaded training data,\nyou need to provide the number of examples in your training data.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("momentum_scheduler")])])]),e._v(" "),n("dd",[e._v("If specified, the momentum will be updated at the end of each batch or epoch according to the schedule.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("moving_average")])])]),e._v(" "),n("dd",[e._v("If provided, we will maintain moving averages for all parameters.\nDuring training, we employ a shadow variable for each parameter, which maintains the moving average.\nDuring evaluation, we backup the original parameters and assign the moving averages to corresponding parameters.\nBe careful that when saving the checkpoint, we will save the moving averages of parameters.\nThis is necessary because we want the saved model to perform as well as the validated model if we load it later.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("batch_size")])])]),e._v(" "),n("dd",[e._v("Size of the batch.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("data_bucketing")])])]),e._v(" "),n("dd",[e._v("If enabled, try to apply data bucketing over training batches.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("batches_per_epoch")])])]),e._v(" "),n("dd",[e._v('Determines the number of batches after which a training epoch ends.\nIf the number is smaller than the total amount of batches in your training data,\nthe second "epoch" will take off where the first "epoch" ended.\nIf this is '),n("code",[e._v("None")]),e._v(", then an epoch is set to be one full pass through your training data.\nThis is useful if you want to evaluate your data more frequently on your validation data set during training.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("random_seed")])])]),e._v(" "),n("dd",[e._v("Seed for the underlying random number generators.\nIf None, we take the random seeds provided by AllenNLP's "),n("code",[e._v("prepare_environment")]),e._v(" method.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("use_amp")])])]),e._v(" "),n("dd",[e._v("If "),n("code",[e._v("True")]),e._v(", we'll train using "),n("a",{attrs:{href:"https://pytorch.org/docs/stable/amp.html"}},[e._v("Automatic Mixed Precision")]),e._v(".")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("num_serialized_models_to_keep")])])]),e._v(" "),n("dd",[e._v("Number of previous model checkpoints to retain.\nDefault is to keep 1 checkpoint.\nA value of None or -1 means all checkpoints will be kept.")])]),e._v(" "),n("dl",[n("pre",{staticClass:"title"},[n("h3",{attrs:{id:"to-allennlp-trainer"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#to-allennlp-trainer"}},[e._v("#")]),e._v(" to_allennlp_trainer "),n("Badge",{attrs:{text:"Method"}})],1),e._v("\n")]),e._v(" "),n("dt",[n("div",{staticClass:"language-python extra-class"},[n("pre",{staticClass:"language-python"},[n("code",[e._v("\n"),n("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),n("span",{staticClass:"ident"},[e._v("to_allennlp_trainer")]),e._v("("),n("span",[e._v("self) -> Dict[str, Any]")]),e._v("\n")]),e._v("\n")])])]),e._v(" "),n("dd",[n("p",[e._v("Returns a configuration dict formatted for AllenNLP's trainer")]),e._v(" "),n("h2",{attrs:{id:"returns"}},[e._v("Returns")]),e._v(" "),n("dl",[n("dt",[n("code",[e._v("allennlp_trainer_config")])]),e._v(" "),n("dd",[e._v(" ")])])])]),e._v(" "),n("div"),e._v(" "),n("pre",{staticClass:"title"},[n("h2",{attrs:{id:"vocabularyconfiguration"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#vocabularyconfiguration"}},[e._v("#")]),e._v(" VocabularyConfiguration "),n("Badge",{attrs:{text:"Class"}})],1),e._v("\n")]),e._v(" "),n("pre",{staticClass:"language-python"},[n("code",[e._v("\n"),n("span",{staticClass:"token keyword"},[e._v("class")]),e._v(" "),n("span",{staticClass:"ident"},[e._v("VocabularyConfiguration")]),e._v(" ("),e._v("\n    "),n("span",[e._v("include_valid_data: bool = False")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("max_vocab_size: Union[int, Dict[str, int]] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("min_count: Dict[str, int] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("min_pretrained_embeddings: Dict[str, int] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("only_include_pretrained_words: bool = False")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("pretrained_files: Union[Dict[str, str], NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("tokens_to_add: Dict[str, List[str]] = None")]),n("span",[e._v(",")]),e._v("\n"),n("span",[e._v(")")]),e._v("\n")]),e._v("\n")]),e._v(" "),n("p",[e._v("Configurations for creating the vocabulary")]),e._v(" "),n("p",[e._v("See "),n("a",{attrs:{href:"https://docs.allennlp.org/master/api/data/vocabulary/#vocabulary]"}},[e._v("AllenNLP Vocabulary docs")])]),e._v(" "),n("h2",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),n("dl",[n("dt",[n("strong",[n("code",[e._v("include_valid_data")])])]),e._v(" "),n("dd",[e._v("If passed to the "),n("code",[e._v("Trainer")]),e._v(", this argument allows you to take the validation data into account when creating\nthe vocabulary (apart from the training data). Default: False.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("max_vocab_size")])])]),e._v(" "),n("dd",[e._v("If you want to cap the number of tokens in your vocabulary, you can do so with this\nparameter.\nIf you specify a single integer, every namespace will have its vocabulary fixed\nto be no larger than this.\nIf you specify a dictionary, then each namespace in the\n"),n("code",[e._v("counter")]),e._v(" can have a separate maximum vocabulary size. Any missing key will have a value\nof "),n("code",[e._v("None")]),e._v(", which means no cap on the vocabulary size.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("min_count")])])]),e._v(" "),n("dd",[e._v("Minimum number of appearances of a token to be included in the vocabulary.\nThe key in the dictionary refers to the namespace of the input feature")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("min_pretrained_embeddings")])])]),e._v(" "),n("dd",[e._v("Minimum number of lines to keep from pretrained_files, even for tokens not appearing in the sources.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("only_include_pretrained_words")])])]),e._v(" "),n("dd",[e._v("Only include tokens present in pretrained_files")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("pretrained_files")])])]),e._v(" "),n("dd",[e._v("If provided, this map specifies the path to optional pretrained embedding files for each\nnamespace. This can be used to either restrict the vocabulary to only words which appear\nin this file, or to ensure that any words in this file are included in the vocabulary\nregardless of their count, depending on the value of "),n("code",[e._v("only_include_pretrained_words")]),e._v(".\nWords which appear in the pretrained embedding file but not in the data are NOT included\nin the Vocabulary.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("tokens_to_add")])])]),e._v(" "),n("dd",[e._v("A list of tokens to add to the corresponding namespace of the vocabulary,\neven if they are not present in the "),n("code",[e._v("datasets")])])]),e._v(" "),n("div"),e._v(" "),n("pre",{staticClass:"title"},[n("h2",{attrs:{id:"findlrconfiguration"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#findlrconfiguration"}},[e._v("#")]),e._v(" FindLRConfiguration "),n("Badge",{attrs:{text:"Class"}})],1),e._v("\n")]),e._v(" "),n("pre",{staticClass:"language-python"},[n("code",[e._v("\n"),n("span",{staticClass:"token keyword"},[e._v("class")]),e._v(" "),n("span",{staticClass:"ident"},[e._v("FindLRConfiguration")]),e._v(" ("),e._v("\n    "),n("span",[e._v("start_lr: float = 1e-05")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("end_lr: float = 10")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("num_batches: int = 100")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("linear_steps: bool = False")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("stopping_factor: Union[float, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n"),n("span",[e._v(")")]),e._v("\n")]),e._v("\n")]),e._v(" "),n("p",[e._v("A configuration for finding the learning rate via "),n("code",[e._v("Pipeline.find_lr()")]),e._v(".")]),e._v(" "),n("p",[e._v("The "),n("code",[e._v("Pipeline.find_lr()")]),e._v(" method increases the learning rate from "),n("code",[e._v("start_lr")]),e._v(" to "),n("code",[e._v("end_lr")]),e._v(" recording the losses.")]),e._v(" "),n("h2",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),n("dl",[n("dt",[n("strong",[n("code",[e._v("start_lr")])])]),e._v(" "),n("dd",[e._v("The learning rate to start the search.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("end_lr")])])]),e._v(" "),n("dd",[e._v("The learning rate upto which search is done.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("num_batches")])])]),e._v(" "),n("dd",[e._v("Number of batches to run the learning rate finder.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("linear_steps")])])]),e._v(" "),n("dd",[e._v("Increase learning rate linearly if False exponentially.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("stopping_factor")])])]),e._v(" "),n("dd",[e._v("Stop the search when the current loss exceeds the best loss recorded by\nmultiple of stopping factor. If "),n("code",[e._v("None")]),e._v(" search proceeds till the "),n("code",[e._v("end_lr")])])]),e._v(" "),n("div"),e._v(" "),n("pre",{staticClass:"title"},[n("h2",{attrs:{id:"predictionconfiguration"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#predictionconfiguration"}},[e._v("#")]),e._v(" PredictionConfiguration "),n("Badge",{attrs:{text:"Class"}})],1),e._v("\n")]),e._v(" "),n("pre",{staticClass:"language-python"},[n("code",[e._v("\n"),n("span",{staticClass:"token keyword"},[e._v("class")]),e._v(" "),n("span",{staticClass:"ident"},[e._v("PredictionConfiguration")]),e._v(" ("),e._v("\n    "),n("span",[e._v("add_tokens: bool = False")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("add_attributions: bool = False")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("attributions_kwargs: Dict = <factory>")]),n("span",[e._v(",")]),e._v("\n"),n("span",[e._v(")")]),e._v("\n")]),e._v("\n")]),e._v(" "),n("p",[e._v("Contains configurations for a "),n("code",[e._v("Pipeline.prediction")])]),e._v(" "),n("h2",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),n("dl",[n("dt",[n("strong",[n("code",[e._v("add_tokens")])])]),e._v(" "),n("dd",[e._v(" ")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("add_attributions")])])]),e._v(" "),n("dd",[e._v(" ")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("attributions_kwargs")])])]),e._v(" "),n("dd",[e._v(" ")])]),e._v(" "),n("div"),e._v(" "),n("pre",{staticClass:"title"},[n("h2",{attrs:{id:"lightningtrainerconfiguration"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#lightningtrainerconfiguration"}},[e._v("#")]),e._v(" LightningTrainerConfiguration "),n("Badge",{attrs:{text:"Class"}})],1),e._v("\n")]),e._v(" "),n("pre",{staticClass:"language-python"},[n("code",[e._v("\n"),n("span",{staticClass:"token keyword"},[e._v("class")]),e._v(" "),n("span",{staticClass:"ident"},[e._v("LightningTrainerConfiguration")]),e._v(" ("),e._v("\n    "),n("span",[e._v("logger: Union[pytorch_lightning.loggers.base.LightningLoggerBase, Iterable[pytorch_lightning.loggers.base.LightningLoggerBase], bool] = True")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("checkpoint_callback: bool = True")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("callbacks: Union[List[pytorch_lightning.callbacks.base.Callback], pytorch_lightning.callbacks.base.Callback, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("default_root_dir: str = <factory>")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("gradient_clip_val: float = 0")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("process_position: int = 0")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("num_nodes: int = 1")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("num_processes: int = 1")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("gpus: Union[List[int], str, int, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("auto_select_gpus: bool = False")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("tpu_cores: Union[List[int], str, int, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("log_gpu_memory: Union[str, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("progress_bar_refresh_rate: Union[int, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("overfit_batches: Union[int, float] = 0.0")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("track_grad_norm: Union[int, float, str] = -1")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("check_val_every_n_epoch: int = 1")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("fast_dev_run: Union[int, bool] = False")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("accumulate_grad_batches: Union[int, Dict[int, int], List[list]] = 1")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("max_epochs: Union[int, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("min_epochs: Union[int, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("max_steps: Union[int, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("min_steps: Union[int, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("limit_train_batches: Union[int, float] = 1.0")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("limit_val_batches: Union[int, float] = 1.0")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("limit_test_batches: Union[int, float] = 1.0")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("limit_predict_batches: Union[int, float] = 1.0")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("val_check_interval: Union[int, float] = 1.0")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("flush_logs_every_n_steps: int = 100")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("log_every_n_steps: int = 50")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("accelerator: Union[str, pytorch_lightning.accelerators.accelerator.Accelerator, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("sync_batchnorm: bool = False")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("precision: int = 32")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("weights_summary: Union[str, NoneType] = 'top'")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("weights_save_path: Union[str, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("num_sanity_val_steps: int = 2")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("truncated_bptt_steps: Union[int, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("resume_from_checkpoint: Union[pathlib.Path, str, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("profiler: Union[pytorch_lightning.profiler.profilers.BaseProfiler, bool, str, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("benchmark: bool = False")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("deterministic: bool = False")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("reload_dataloaders_every_epoch: bool = False")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("auto_lr_find: Union[bool, str] = False")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("replace_sampler_ddp: bool = True")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("terminate_on_nan: bool = False")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("auto_scale_batch_size: Union[str, bool] = False")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("prepare_data_per_node: bool = True")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("plugins: Union[pytorch_lightning.plugins.base_plugin.Plugin, str, list, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("amp_backend: str = 'native'")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("amp_level: str = 'O2'")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("distributed_backend: Union[str, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("automatic_optimization: Union[bool, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("move_metrics_to_cpu: bool = False")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("multiple_trainloader_mode: str = 'max_size_cycle'")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("stochastic_weight_avg: bool = False")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("add_csv_logger: bool = True")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("add_tensorboard_logger: bool = True")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("add_wandb_logger: bool = True")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("batch_size: int = 16")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("data_bucketing: bool = False")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("optimizer: Dict[str, Any] = <factory>")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("monitor: str = 'validation_loss'")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("monitor_mode: str = 'min'")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("save_top_k_checkpoints: int = 1")]),n("span",[e._v(",")]),e._v("\n"),n("span",[e._v(")")]),e._v("\n")]),e._v("\n")]),e._v(" "),n("p",[e._v("Configuration for our Lightning Trainer")]),e._v(" "),n("p",[e._v("The docs are mainly a copy from the\n"),n("a",{attrs:{href:"https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.html#pytorch_lightning.trainer.trainer.Trainer"}},[e._v("Lightning Trainer API")]),e._v("\nwith some additional parameters added.")]),e._v(" "),n("h2",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),n("dl",[n("dt",[n("strong",[n("code",[e._v("accelerator")])])]),e._v(" "),n("dd",[e._v("Previously known as distributed_backend (dp, ddp, ddp2, etc…).\nCan also take in an accelerator object for custom hardware.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("accumulate_grad_batches")])])]),e._v(" "),n("dd",[e._v("Accumulates grads every k batches or as set up in the dict.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("add_csv_logger")])])]),e._v(" "),n("dd",[e._v("Adds a default CSV logger if "),n("code",[e._v("logger")]),e._v(" is not False. Default: True")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("add_tensorboard_logger")])])]),e._v(" "),n("dd",[e._v("Adds a default Tensorboard logger if "),n("code",[e._v("logger")]),e._v(" is not False. Default: True")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("add_wandb_logger")])])]),e._v(" "),n("dd",[e._v("Adds a default WandB logger if "),n("code",[e._v("logger")]),e._v(" is not False and wandb is installed. Default: True")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("amp_backend")])])]),e._v(" "),n("dd",[e._v('The mixed precision backend to use ("native" or "apex")')]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("amp_level")])])]),e._v(" "),n("dd",[e._v("The optimization level to use (O1, O2, etc…).")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("auto_lr_find")])])]),e._v(" "),n("dd",[e._v("If set to True, will make trainer.tune() run a learning rate finder,\ntrying to optimize initial learning for faster convergence. trainer.tune() method will\nset the suggested learning rate in self.lr or self.learning_rate in the LightningModule.\nTo use a different key set a string instead of True with the key name.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("auto_scale_batch_size")])])]),e._v(" "),n("dd",[e._v("If set to True, will "),n("code",[e._v("initially")]),e._v(" run a batch size\nfinder trying to find the largest batch size that fits into memory.\nThe result will be stored in self.batch_size in the LightningModule.\nAdditionally, can be set to either "),n("code",[e._v("power")]),e._v(" that estimates the batch size through\na power search or "),n("code",[e._v("binsearch")]),e._v(" that estimates the batch size through a binary search.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("auto_select_gpus")])])]),e._v(" "),n("dd",[e._v("If enabled and "),n("code",[e._v("gpus")]),e._v(' is an integer, pick available\ngpus automatically. This is especially useful when\nGPUs are configured to be in "exclusive mode", such\nthat only one process at a time can access them.')]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("batch_size")])])]),e._v(" "),n("dd",[e._v("Size of the batch.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("benchmark")])])]),e._v(" "),n("dd",[e._v("If true enables cudnn.benchmark.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("callbacks")])])]),e._v(" "),n("dd",[e._v("Add a callback or list of callbacks.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("checkpoint_callback")])])]),e._v(" "),n("dd",[e._v("If True, enable checkpointing.\nIt will configure a default "),n("code",[e._v("ModelCheckpointWithVocab")]),e._v(" callback if there is no user-defined ModelCheckpoint in\n"),n("code",[e._v("callbacks")]),e._v(". Default: True.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("check_val_every_n_epoch")])])]),e._v(" "),n("dd",[e._v("Check val every n train epochs.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("data_bucketing")])])]),e._v(" "),n("dd",[e._v("If enabled, try to apply data bucketing over training batches.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("default_root_dir")])])]),e._v(" "),n("dd",[e._v("Default path for logs and weights when no logger/ckpt_callback passed.\nCan be remote file paths such as 's3://mybucket/path' or 'hdfs://path/'\nDefault: './training_logs'.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("deterministic")])])]),e._v(" "),n("dd",[e._v("If true enables cudnn.deterministic.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("fast_dev_run")])])]),e._v(" "),n("dd",[e._v("runs n if set to "),n("code",[e._v("n")]),e._v(" (int) else 1 if set to "),n("code",[e._v("True")]),e._v(" batch(es)\nof train, val and test to find any bugs (ie: a sort of unit test).")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("flush_logs_every_n_steps")])])]),e._v(" "),n("dd",[e._v("How often to flush logs to disk (defaults to every 100 steps).")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("gpus")])])]),e._v(" "),n("dd",[e._v("number of gpus to train on (int) or which GPUs to train on (list or str) applied per node")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("gradient_clip_val")])])]),e._v(" "),n("dd",[e._v("0 means don't clip.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("limit_train_batches")])])]),e._v(" "),n("dd",[e._v("How much of training dataset to check (floats = percent, int = num_batches)")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("limit_val_batches")])])]),e._v(" "),n("dd",[e._v("How much of validation dataset to check (floats = percent, int = num_batches)")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("limit_test_batches")])])]),e._v(" "),n("dd",[e._v("How much of test dataset to check (floats = percent, int = num_batches)")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("log_every_n_steps")])])]),e._v(" "),n("dd",[e._v("How often to log within steps (defaults to every 50 steps).")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("log_gpu_memory")])])]),e._v(" "),n("dd",[e._v("None, 'min_max', 'all'. Might slow performance")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("logger")])])]),e._v(" "),n("dd",[e._v("Logger (or iterable collection of loggers) for experiment tracking.\nIf not False, we will add some loggers by default, see "),n("code",[e._v("add_[csv, tensorboard, wandb]_logger")]),e._v(".\nDefault: True")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("prepare_data_per_node")])])]),e._v(" "),n("dd",[e._v("If True, each LOCAL_RANK=0 will call prepare data.\nOtherwise only NODE_RANK=0, LOCAL_RANK=0 will prepare data")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("process_position")])])]),e._v(" "),n("dd",[e._v("orders the progress bar when running multiple models on same machine.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("progress_bar_refresh_rate")])])]),e._v(" "),n("dd",[e._v("How often to refresh progress bar (in steps). Value "),n("code",[e._v("0")]),e._v(" disables progress bar.\nIgnored when a custom progress bar is passed to :paramref:"),n("code",[e._v("~Trainer.callbacks")]),e._v(". Default: None, means\na suitable value will be chosen based on the environment (terminal, Google COLAB, etc.).")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("profiler")])])]),e._v(" "),n("dd",[e._v("To profile individual steps during training and assist in identifying bottlenecks. Passing bool\nvalue is deprecated in v1.1 and will be removed in v1.3.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("overfit_batches")])])]),e._v(" "),n("dd",[e._v("Overfit a percent of training data (float) or a set number of batches (int). Default: 0.0")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("plugins")])])]),e._v(" "),n("dd",[e._v("Plugins allow modification of core behavior like ddp and amp, and enable custom lightning plugins.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("precision")])])]),e._v(" "),n("dd",[e._v("Full precision (32), half precision (16). Can be used on CPU, GPU or TPUs.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("max_epochs")])])]),e._v(" "),n("dd",[e._v("Stop training once this number of epochs is reached. Disabled by default (None).\nIf both max_epochs and max_steps are not specified, defaults to "),n("code",[e._v("max_epochs")]),e._v(" = 1000.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("min_epochs")])])]),e._v(" "),n("dd",[e._v("Force training for at least these many epochs. Disabled by default (None).\nIf both min_epochs and min_steps are not specified, defaults to "),n("code",[e._v("min_epochs")]),e._v(" = 1.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("max_steps")])])]),e._v(" "),n("dd",[e._v("Stop training after this number of steps. Disabled by default (None).")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("min_steps")])])]),e._v(" "),n("dd",[e._v("Force training for at least these number of steps. Disabled by default (None)")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("monitor")])])]),e._v(" "),n("dd",[e._v("Metric to monitor. Will be used to load the best weights after the training.\nHas no effect if "),n("code",[e._v("checkpoint_callback")]),e._v(" is False. Default: 'validation_loss'.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("monitor_mode")])])]),e._v(" "),n("dd",[e._v("Either 'min' or 'max'. If "),n("code",[e._v("save_top_k_checkpoints != 0")]),e._v(", the decision to overwrite the current save file is made\nbased on either the maximization or the minimization of the monitored metric. Default: 'min'.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("num_nodes")])])]),e._v(" "),n("dd",[e._v("number of GPU nodes for distributed training.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("num_processes")])])]),e._v(" "),n("dd",[e._v('number of processes for distributed training with distributed_backend="ddp_cpu"')]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("num_sanity_val_steps")])])]),e._v(" "),n("dd",[e._v("Sanity check runs n validation batches before starting the training routine.\nSet it to "),n("code",[e._v("-1")]),e._v(" to run all batches in all validation dataloaders. Default: 2")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("optimizer")])])]),e._v(" "),n("dd",[e._v("Configuration for an "),n("a",{attrs:{href:"https://docs.allennlp.org/main/api/training/optimizers/"}},[e._v("AllenNLP/PyTorch optimizer")]),e._v("\nthat is constructed via the AllenNLP configuration framework.\nDefault: "),n("code",[e._v('{"type": "adam", "lr": 0.001}')])]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("reload_dataloaders_every_epoch")])])]),e._v(" "),n("dd",[e._v("Set to True to reload dataloaders every epoch.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("replace_sampler_ddp")])])]),e._v(" "),n("dd",[e._v("Explicitly enables or disables sampler replacement. If not specified this\nwill toggled automatically when DDP is used. By default it will add "),n("code",[e._v("shuffle=True")]),e._v(" for\ntrain sampler and "),n("code",[e._v("shuffle=False")]),e._v(" for val/test sampler. If you want to customize it,\nyou can set "),n("code",[e._v("replace_sampler_ddp=False")]),e._v(" and add your own distributed sampler.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("resume_from_checkpoint")])])]),e._v(" "),n("dd",[e._v("Path/URL of the checkpoint from which training is resumed. If there is\nno checkpoint file at the path, start from scratch. If resuming from mid-epoch checkpoint,\ntraining will start from the beginning of the next epoch.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("sync_batchnorm")])])]),e._v(" "),n("dd",[e._v("Synchronize batch norm layers between process groups/whole world.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("terminate_on_nan")])])]),e._v(" "),n("dd",[e._v("If set to True, will terminate training (by raising a "),n("code",[e._v("ValueError")]),e._v(") at the\nend of each training batch, if any of the parameters or the loss are NaN or +/-inf.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("tpu_cores")])])]),e._v(" "),n("dd",[e._v("How many TPU cores to train on (1 or 8) / Single TPU to train on [1]")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("track_grad_norm")])])]),e._v(" "),n("dd",[e._v("-1 no tracking. Otherwise tracks that p-norm. May be set to 'inf' infinity-norm.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("truncated_bptt_steps")])])]),e._v(" "),n("dd",[e._v("Truncated back prop breaks performs backprop every k steps of much longer\nsequence.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("val_check_interval")])])]),e._v(" "),n("dd",[e._v("How often to check the validation set. Use float to check within a training epoch,\nuse int to check every n steps (batches).")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("weights_summary")])])]),e._v(" "),n("dd",[e._v("Prints a summary of the weights when training begins.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("weights_save_path")])])]),e._v(" "),n("dd",[e._v("Where to save weights if specified. Will override default_root_dir\nfor checkpoints only. Use this if for whatever reason you need the checkpoints\nstored in a different place than the logs written in "),n("code",[e._v("default_root_dir")]),e._v(".\nCan be remote file paths such as "),n("code",[e._v("s3://mybucket/path")]),e._v(" or 'hdfs://path/'\nDefaults to "),n("code",[e._v("default_root_dir")]),e._v(".")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("move_metrics_to_cpu")])])]),e._v(" "),n("dd",[e._v("Whether to force internal logged metrics to be moved to cpu.\nThis can save some gpu memory, but can make training slower. Use with attention.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("multiple_trainloader_mode")])])]),e._v(" "),n("dd",[e._v("How to loop over the datasets when there are multiple train loaders.\nIn 'max_size_cycle' mode, the trainer ends one epoch when the largest dataset is traversed,\nand smaller datasets reload when running out of their data. In 'min_size' mode, all the datasets\nreload when reaching the minimum length of datasets.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("save_top_k_checkpoints")])])]),e._v(" "),n("dd",[e._v("If "),n("code",[e._v("save_top_k_checkpoints == k")]),e._v(", the best k models according to the metric monitored will be saved.\nIf "),n("code",[e._v("save_top_k_checkpoints == 0")]),e._v(", no models are saved. If "),n("code",[e._v("save_top_k_checkpoints == -1")]),e._v(", all models are saved.\nHas no effect if "),n("code",[e._v("checkpoint_callback")]),e._v(" is False. Default: 1.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("stochastic_weight_avg")])])]),e._v(" "),n("dd",[e._v("Whether to use "),n("code",[e._v("Stochastic Weight Averaging (SWA)\n<https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/>_")])])]),e._v(" "),n("dl",[n("pre",{staticClass:"title"},[n("h3",{attrs:{id:"as-dict-2"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#as-dict-2"}},[e._v("#")]),e._v(" as_dict "),n("Badge",{attrs:{text:"Method"}})],1),e._v("\n")]),e._v(" "),n("dt",[n("div",{staticClass:"language-python extra-class"},[n("pre",{staticClass:"language-python"},[n("code",[e._v("\n"),n("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),n("span",{staticClass:"ident"},[e._v("as_dict")]),e._v("("),n("span",[e._v("self) -> Dict")]),e._v("\n")]),e._v("\n")])])]),e._v(" "),n("dd",[n("p",[e._v("Returns the dataclass as dict without a deepcopy, in contrast to "),n("code",[e._v("dataclasses.asdict")])])])])])}),[],!1,null,null,null);t.default=s.exports}}]);