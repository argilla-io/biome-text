(window.webpackJsonp=window.webpackJsonp||[]).push([[28],{434:function(t,e,a){"use strict";a.r(e);var s=a(26),r=Object(s.a)({},(function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"biome-text-featurizer"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#biome-text-featurizer"}},[t._v("#")]),t._v(" biome.text.featurizer "),a("Badge",{attrs:{text:"Module"}})],1),t._v(" "),a("div"),t._v(" "),a("div"),t._v(" "),a("pre",{staticClass:"title"},[a("h2",{attrs:{id:"inputfeaturizer"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#inputfeaturizer"}},[t._v("#")]),t._v(" InputFeaturizer "),a("Badge",{attrs:{text:"Class"}})],1),t._v("\n")]),t._v(" "),a("pre",{staticClass:"language-python"},[a("code",[t._v("\n"),a("span",{staticClass:"token keyword"},[t._v("class")]),t._v(" "),a("span",{staticClass:"ident"},[t._v("InputFeaturizer")]),t._v(" (tokenizer: "),a("a",{attrs:{title:"biome.text.tokenizer.Tokenizer",href:"tokenizer.html#biome.text.tokenizer.Tokenizer"}},[t._v("Tokenizer")]),t._v(", indexer: Dict[str, allennlp.data.token_indexers.token_indexer.TokenIndexer])"),t._v("\n")]),t._v("\n")]),t._v(" "),a("p",[t._v("Transforms input text (words and/or characters) into indexes and embedding vectors.")]),t._v(" "),a("p",[t._v("This class defines two input features, words and chars for embeddings at word and character level respectively.")]),t._v(" "),a("p",[t._v("You can provide additional features by manually specify "),a("code",[t._v("indexer")]),t._v(" and "),a("code",[t._v("embedder")]),t._v(" configurations within each\ninput feature.")]),t._v(" "),a("h2",{attrs:{id:"attributes"}},[t._v("Attributes")]),t._v(" "),a("dl",[a("dt",[a("strong",[a("code",[t._v("tokenizer")])]),t._v(" : "),a("code",[t._v("Tokenizer")])]),t._v(" "),a("dd",[t._v("Tokenizes the input depending on its type (str, List[str], Dict[str, Any])")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("indexer")])]),t._v(" : "),a("code",[t._v("Dict[str, TokenIdexer]")])]),t._v(" "),a("dd",[t._v("Features dictionary for token indexing. Built from "),a("code",[t._v("FeaturesConfiguration")])])]),t._v(" "),a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"instance-variables"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#instance-variables"}},[t._v("#")]),t._v(" Instance variables")]),t._v("\n")]),t._v(" "),a("dl",[a("dt",{attrs:{id:"biome.text.featurizer.InputFeaturizer.has_word_features"}},[a("code",{staticClass:"name"},[t._v("var "),a("span",{staticClass:"ident"},[t._v("has_word_features")]),t._v(" : bool")])]),t._v(" "),a("dd",[a("p",[t._v("Checks if word features are already configured as part of the featurization")])])]),t._v(" "),a("div"),t._v(" "),a("pre",{staticClass:"title"},[a("h2",{attrs:{id:"featurizeerror"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#featurizeerror"}},[t._v("#")]),t._v(" FeaturizeError "),a("Badge",{attrs:{text:"Class"}})],1),t._v("\n")]),t._v(" "),a("pre",{staticClass:"language-python"},[a("code",[t._v("\n"),a("span",{staticClass:"token keyword"},[t._v("class")]),t._v(" "),a("span",{staticClass:"ident"},[t._v("FeaturizeError")]),t._v(" (...)"),t._v("\n")]),t._v("\n")]),t._v(" "),a("p",[t._v("Base class for exceptions in this module")]),t._v(" "),a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"ancestors"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#ancestors"}},[t._v("#")]),t._v(" Ancestors")]),t._v("\n")]),t._v(" "),a("ul",{staticClass:"hlist"},[a("li",[t._v("builtins.Exception")]),t._v(" "),a("li",[t._v("builtins.BaseException")])])])}),[],!1,null,null,null);e.default=r.exports}}]);