<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>biome.text.trainer | biome.text</title>
    <meta name="generator" content="VuePress 1.8.2">
    <link rel="shortcut icon" href="/biome-text/master/favicon.ico">
    <meta name="description" content="biome.text practical NLP open source library.">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta property="og:image" content="https://www.recogn.ai/images/biome_og.png">
    
    <link rel="preload" href="/biome-text/master/assets/css/0.styles.26498fa5.css" as="style"><link rel="preload" href="/biome-text/master/assets/js/app.eb668dca.js" as="script"><link rel="preload" href="/biome-text/master/assets/js/4.d8b631f7.js" as="script"><link rel="preload" href="/biome-text/master/assets/js/3.708237ef.js" as="script"><link rel="preload" href="/biome-text/master/assets/js/51.e254110a.js" as="script"><link rel="preload" href="/biome-text/master/assets/js/6.a484b546.js" as="script"><link rel="prefetch" href="/biome-text/master/assets/js/10.629a5f99.js"><link rel="prefetch" href="/biome-text/master/assets/js/11.7a154e19.js"><link rel="prefetch" href="/biome-text/master/assets/js/12.d59b8327.js"><link rel="prefetch" href="/biome-text/master/assets/js/13.366419b6.js"><link rel="prefetch" href="/biome-text/master/assets/js/14.47fb6094.js"><link rel="prefetch" href="/biome-text/master/assets/js/15.f337d446.js"><link rel="prefetch" href="/biome-text/master/assets/js/16.c32916ca.js"><link rel="prefetch" href="/biome-text/master/assets/js/17.7929c5fc.js"><link rel="prefetch" href="/biome-text/master/assets/js/18.da9659ac.js"><link rel="prefetch" href="/biome-text/master/assets/js/19.2447f479.js"><link rel="prefetch" href="/biome-text/master/assets/js/20.0810e7ad.js"><link rel="prefetch" href="/biome-text/master/assets/js/21.d1f244d4.js"><link rel="prefetch" href="/biome-text/master/assets/js/22.e3f6a1e8.js"><link rel="prefetch" href="/biome-text/master/assets/js/23.a44ff3e6.js"><link rel="prefetch" href="/biome-text/master/assets/js/24.b2560863.js"><link rel="prefetch" href="/biome-text/master/assets/js/25.e7d36ee0.js"><link rel="prefetch" href="/biome-text/master/assets/js/26.b631f124.js"><link rel="prefetch" href="/biome-text/master/assets/js/27.6a0695ab.js"><link rel="prefetch" href="/biome-text/master/assets/js/28.a850c8ae.js"><link rel="prefetch" href="/biome-text/master/assets/js/29.8c191027.js"><link rel="prefetch" href="/biome-text/master/assets/js/30.65c4b971.js"><link rel="prefetch" href="/biome-text/master/assets/js/31.cfa05ee2.js"><link rel="prefetch" href="/biome-text/master/assets/js/32.b42baa1c.js"><link rel="prefetch" href="/biome-text/master/assets/js/33.cb334840.js"><link rel="prefetch" href="/biome-text/master/assets/js/34.752ae461.js"><link rel="prefetch" href="/biome-text/master/assets/js/35.b05e2db3.js"><link rel="prefetch" href="/biome-text/master/assets/js/36.44108821.js"><link rel="prefetch" href="/biome-text/master/assets/js/37.9b5ac1c0.js"><link rel="prefetch" href="/biome-text/master/assets/js/38.ba403a4a.js"><link rel="prefetch" href="/biome-text/master/assets/js/39.6407ef44.js"><link rel="prefetch" href="/biome-text/master/assets/js/40.f5829105.js"><link rel="prefetch" href="/biome-text/master/assets/js/41.407157b0.js"><link rel="prefetch" href="/biome-text/master/assets/js/42.2ff70d68.js"><link rel="prefetch" href="/biome-text/master/assets/js/43.3202abfa.js"><link rel="prefetch" href="/biome-text/master/assets/js/44.bf56e90b.js"><link rel="prefetch" href="/biome-text/master/assets/js/45.2a6920e1.js"><link rel="prefetch" href="/biome-text/master/assets/js/46.93dd9ea3.js"><link rel="prefetch" href="/biome-text/master/assets/js/47.68ef83d4.js"><link rel="prefetch" href="/biome-text/master/assets/js/48.b0069fca.js"><link rel="prefetch" href="/biome-text/master/assets/js/49.ed781cee.js"><link rel="prefetch" href="/biome-text/master/assets/js/5.73ece6cb.js"><link rel="prefetch" href="/biome-text/master/assets/js/50.81440c40.js"><link rel="prefetch" href="/biome-text/master/assets/js/52.1553c490.js"><link rel="prefetch" href="/biome-text/master/assets/js/53.23ce77fa.js"><link rel="prefetch" href="/biome-text/master/assets/js/54.fb427963.js"><link rel="prefetch" href="/biome-text/master/assets/js/55.0dd67893.js"><link rel="prefetch" href="/biome-text/master/assets/js/56.01780ed9.js"><link rel="prefetch" href="/biome-text/master/assets/js/57.244e57d1.js"><link rel="prefetch" href="/biome-text/master/assets/js/58.275f7d31.js"><link rel="prefetch" href="/biome-text/master/assets/js/59.ea6c1e19.js"><link rel="prefetch" href="/biome-text/master/assets/js/60.1c3c184c.js"><link rel="prefetch" href="/biome-text/master/assets/js/61.5a9c7766.js"><link rel="prefetch" href="/biome-text/master/assets/js/62.ca401ace.js"><link rel="prefetch" href="/biome-text/master/assets/js/63.3dd964b2.js"><link rel="prefetch" href="/biome-text/master/assets/js/7.5737c154.js"><link rel="prefetch" href="/biome-text/master/assets/js/8.cd6cd879.js"><link rel="prefetch" href="/biome-text/master/assets/js/9.44c821fe.js"><link rel="prefetch" href="/biome-text/master/assets/js/vendors~docsearch.b3708bf9.js">
    <link rel="stylesheet" href="/biome-text/master/assets/css/0.styles.26498fa5.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container" data-v-348088ed><header class="navbar" data-v-348088ed><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/biome-text/master/" class="home-link router-link-active"><!----> <span class="site-name">biome<span>.text</span></span></a> <div class="links"><form id="search-form" role="search" class="algolia-search-wrapper search-box"><input id="algolia-search-input" class="search-query"></form> <nav class="nav-links can-hide"><div class="nav-item"><a href="/biome-text/master/api/" class="nav-link router-link-active">
  API
</a></div><div class="nav-item"><a href="/biome-text/master/documentation/" class="nav-link">
  Documentation
</a></div><div class="nav-item"><a href="https://github.com/recognai/biome-text" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span class="external__icon"><vp-icon color="#4A4A4A" name="blank" size="12px"></vp-icon></span></a></div><div class="nav-item"><a href="https://recogn.ai" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Recognai
  <span class="external__icon"><vp-icon color="#4A4A4A" name="blank" size="12px"></vp-icon></span></a></div> <!----></nav></div></header> <div class="sidebar-mask" data-v-348088ed></div> <aside class="sidebar" data-v-348088ed><div class="sidebar__link"><a href="/biome-text/master/"><img src="/biome-text/master/assets/img/biome.svg" class="sidebar__img"></a></div> <!----> <nav class="nav-links"><div class="nav-item"><a href="/biome-text/master/api/" class="nav-link router-link-active">
  API
</a></div><div class="nav-item"><a href="/biome-text/master/documentation/" class="nav-link">
  Documentation
</a></div><div class="nav-item"><a href="https://github.com/recognai/biome-text" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span class="external__icon"><vp-icon color="#4A4A4A" name="blank" size="12px"></vp-icon></span></a></div><div class="nav-item"><a href="https://recogn.ai" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Recognai
  <span class="external__icon"><vp-icon color="#4A4A4A" name="blank" size="12px"></vp-icon></span></a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group depth-0"><p class="sidebar-heading open"><span>API</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/biome-text/master/api/biome/text/backbone.html" class="sidebar-link">biome.text.backbone</a></li><li><a href="/biome-text/master/api/biome/text/cli/cli.html" class="sidebar-link">biome.text.cli.cli</a></li><li><a href="/biome-text/master/api/biome/text/cli/evaluate.html" class="sidebar-link">biome.text.cli.evaluate</a></li><li><a href="/biome-text/master/api/biome/text/cli/serve.html" class="sidebar-link">biome.text.cli.serve</a></li><li><a href="/biome-text/master/api/biome/text/cli/train.html" class="sidebar-link">biome.text.cli.train</a></li><li><a href="/biome-text/master/api/biome/text/commons.html" class="sidebar-link">biome.text.commons</a></li><li><a href="/biome-text/master/api/biome/text/configuration.html" class="sidebar-link">biome.text.configuration</a></li><li><a href="/biome-text/master/api/biome/text/dataset.html" class="sidebar-link">biome.text.dataset</a></li><li><a href="/biome-text/master/api/biome/text/errors.html" class="sidebar-link">biome.text.errors</a></li><li><a href="/biome-text/master/api/biome/text/features.html" class="sidebar-link">biome.text.features</a></li><li><a href="/biome-text/master/api/biome/text/featurizer.html" class="sidebar-link">biome.text.featurizer</a></li><li><a href="/biome-text/master/api/biome/text/helpers.html" class="sidebar-link">biome.text.helpers</a></li><li><a href="/biome-text/master/api/biome/text/hpo.html" class="sidebar-link">biome.text.hpo</a></li><li><a href="/biome-text/master/api/biome/text/metrics.html" class="sidebar-link">biome.text.metrics</a></li><li><a href="/biome-text/master/api/biome/text/mlflow_model.html" class="sidebar-link">biome.text.mlflow_model</a></li><li><a href="/biome-text/master/api/biome/text/model.html" class="sidebar-link">biome.text.model</a></li><li><a href="/biome-text/master/api/biome/text/modules/configuration/allennlp_configuration.html" class="sidebar-link">biome.text.modules.configuration.allennlp_configuration</a></li><li><a href="/biome-text/master/api/biome/text/modules/configuration/defs.html" class="sidebar-link">biome.text.modules.configuration.defs</a></li><li><a href="/biome-text/master/api/biome/text/modules/encoders/time_distributed_encoder.html" class="sidebar-link">biome.text.modules.encoders.timedistributedencoder</a></li><li><a href="/biome-text/master/api/biome/text/modules/heads/classification/classification.html" class="sidebar-link">biome.text.modules.heads.classification.classification</a></li><li><a href="/biome-text/master/api/biome/text/modules/heads/classification/doc_classification.html" class="sidebar-link">biome.text.modules.heads.classification.doc_classification</a></li><li><a href="/biome-text/master/api/biome/text/modules/heads/classification/record_classification.html" class="sidebar-link">biome.text.modules.heads.classification.record_classification</a></li><li><a href="/biome-text/master/api/biome/text/modules/heads/classification/record_pair_classification.html" class="sidebar-link">biome.text.modules.heads.classification.recordpairclassification</a></li><li><a href="/biome-text/master/api/biome/text/modules/heads/classification/relation_classification.html" class="sidebar-link">biome.text.modules.heads.classification.relation_classification</a></li><li><a href="/biome-text/master/api/biome/text/modules/heads/classification/text_classification.html" class="sidebar-link">biome.text.modules.heads.classification.text_classification</a></li><li><a href="/biome-text/master/api/biome/text/modules/heads/language_modelling.html" class="sidebar-link">biome.text.modules.heads.language_modelling</a></li><li><a href="/biome-text/master/api/biome/text/modules/heads/task_head.html" class="sidebar-link">biome.text.modules.heads.task_head</a></li><li><a href="/biome-text/master/api/biome/text/modules/heads/task_prediction.html" class="sidebar-link">biome.text.modules.heads.task_prediction</a></li><li><a href="/biome-text/master/api/biome/text/modules/heads/token_classification.html" class="sidebar-link">biome.text.modules.heads.token_classification</a></li><li><a href="/biome-text/master/api/biome/text/pipeline.html" class="sidebar-link">biome.text.pipeline</a></li><li><a href="/biome-text/master/api/biome/text/text_cleaning.html" class="sidebar-link">biome.text.text_cleaning</a></li><li><a href="/biome-text/master/api/biome/text/tokenizer.html" class="sidebar-link">biome.text.tokenizer</a></li><li><a href="/biome-text/master/api/biome/text/trainer.html" aria-current="page" class="active sidebar-link">biome.text.trainer</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/biome-text/master/api/biome/text/trainer.html#trainer" class="sidebar-link">Trainer</a></li><li class="sidebar-sub-header"><a href="/biome-text/master/api/biome/text/trainer.html#modelcheckpointwithvocab" class="sidebar-link">ModelCheckpointWithVocab</a></li></ul></li><li><a href="/biome-text/master/api/biome/text/vocabulary.html" class="sidebar-link">biome.text.vocabulary</a></li></ul></section></li></ul> </aside> <main class="page" data-v-348088ed> <div class="theme-default-content content__default"><h1 id="biome-text-trainer"><a href="#biome-text-trainer" class="header-anchor">#</a> biome.text.trainer <span class="badge tip" style="vertical-align:top;" data-v-15b7b770>Module</span></h1> <div></div> <pre class="title"><h3 id="create-dataloader"><a href="#create-dataloader" class="header-anchor">#</a> create_dataloader <span class="badge tip" style="vertical-align:top;" data-v-15b7b770>Function</span></h3>
</pre> <dt><div class="language-python extra-class"><pre class="language-python"><code>
<span class="token keyword">def</span> <span class="ident">create_dataloader</span> (
  instance_dataset: Union[<a title="biome.text.dataset.AllennlpDataset" href="dataset.html#biome.text.dataset.AllennlpDataset">AllennlpDataset</a>, <a title="biome.text.dataset.AllennlpLazyDataset" href="dataset.html#biome.text.dataset.AllennlpLazyDataset">AllennlpLazyDataset</a>],
  batch_size: int = 16,
  num_workers: int = 0,
)  -&gt; torch.utils.data.dataloader.DataLoader
</code>
</pre></div></dt> <dd><p>Returns a pytorch DataLoader for AllenNLP instances</p> <h2 id="parameters">Parameters</h2> <dl><dt><strong><code>instance_dataset</code></strong></dt> <dd>The dataset of instances for the DataLoader</dd> <dt><strong><code>batch_size</code></strong></dt> <dd>Batch size</dd> <dt><strong><code>num_workers</code></strong></dt> <dd>How many subprocesses to use for data loading. 0 means that the data will be loaded in the main process.
Default: 0</dd></dl> <h2 id="returns">Returns</h2> <dl><dt><code>data_loader</code></dt> <dd> </dd></dl></dd> <pre class="title"><h3 id="allennlp-collate"><a href="#allennlp-collate" class="header-anchor">#</a> allennlp_collate <span class="badge tip" style="vertical-align:top;" data-v-15b7b770>Function</span></h3>
</pre> <dt><div class="language-python extra-class"><pre class="language-python"><code>
<span class="token keyword">def</span> <span class="ident">allennlp_collate</span>(<span>instances: List[allennlp.data.instance.Instance]) -&gt; Dict[str, Union[torch.Tensor, Dict[str, torch.Tensor]]]</span>
</code>
</pre></div></dt> <dd></dd> <div></div> <pre class="title"><h2 id="trainer"><a href="#trainer" class="header-anchor">#</a> Trainer <span class="badge tip" style="vertical-align:top;" data-v-15b7b770>Class</span></h2>
</pre> <pre class="language-python"><code>
<span class="token keyword">class</span> <span class="ident">Trainer</span> (
    <span>pipeline: Pipeline</span><span>,</span>
    <span>train_dataset: Union[<a title="biome.text.dataset.Dataset" href="dataset.html#biome.text.dataset.Dataset">Dataset</a>, <a title="biome.text.dataset.AllennlpDataset" href="dataset.html#biome.text.dataset.AllennlpDataset">AllennlpDataset</a>, <a title="biome.text.dataset.AllennlpLazyDataset" href="dataset.html#biome.text.dataset.AllennlpLazyDataset">AllennlpLazyDataset</a>, NoneType] = None</span><span>,</span>
    <span>valid_dataset: Union[<a title="biome.text.dataset.Dataset" href="dataset.html#biome.text.dataset.Dataset">Dataset</a>, <a title="biome.text.dataset.AllennlpDataset" href="dataset.html#biome.text.dataset.AllennlpDataset">AllennlpDataset</a>, <a title="biome.text.dataset.AllennlpLazyDataset" href="dataset.html#biome.text.dataset.AllennlpLazyDataset">AllennlpLazyDataset</a>, NoneType] = None</span><span>,</span>
    <span>trainer_config: Union[<a title="biome.text.configuration.TrainerConfiguration" href="configuration.html#biome.text.configuration.TrainerConfiguration">TrainerConfiguration</a>, NoneType] = None</span><span>,</span>
    <span>vocab_config: Union[str, <a title="biome.text.configuration.VocabularyConfiguration" href="configuration.html#biome.text.configuration.VocabularyConfiguration">VocabularyConfiguration</a>, NoneType] = 'default'</span><span>,</span>
    <span>lazy: bool = False</span><span>,</span>
<span>)</span>
</code>
</pre> <p>Class for training and testing a <code>biome.text.Pipeline</code>.</p> <p>It is basically a light wrapper around the awesome Pytorch Lightning Trainer to define custom defaults and
facilitate the interaction with our pipelines.</p> <h2 id="parameters">Parameters</h2> <dl><dt><strong><code>pipeline</code></strong></dt> <dd>Pipeline to train</dd> <dt><strong><code>train_dataset</code></strong></dt> <dd>The training dataset. Default: <code>None</code>.</dd> <dt><strong><code>valid_dataset</code></strong></dt> <dd>The validation dataset. Default: <code>None</code>.</dd> <dt><strong><code>trainer_config</code></strong></dt> <dd>The configuration of the trainer. Default: <code>TrainerConfiguration()</code>.</dd> <dt><strong><code>vocab_config</code></strong></dt> <dd>A <code>VocabularyConfiguration</code> to create/extend the pipeline's vocabulary.
If <code>&quot;default&quot;</code> (str), we will use the default configuration <code>VocabularyConfiguration()</code>.
If None, we will leave the pipeline's vocabulary untouched. Default: <code>&quot;default&quot;</code>.</dd> <dt><strong><code>lazy</code></strong></dt> <dd>If True, instances are lazily loaded from disk, otherwise they are loaded into memory.
Ignored when passing in <code>InstanceDataset</code>s. Default: False.</dd></dl> <dl><pre class="title"><h3 id="fit"><a href="#fit" class="header-anchor">#</a> fit <span class="badge tip" style="vertical-align:top;" data-v-15b7b770>Method</span></h3>
</pre> <dt><div class="language-python extra-class"><pre class="language-python"><code>
<span class="token keyword">def</span> <span class="ident">fit</span> (
  self,
  output_dir: Union[pathlib.Path, str, NoneType] = 'output',
  exist_ok: bool = False,
) 
</code>
</pre></div></dt> <dd><p>Train the pipeline</p> <p>At the end of the training the pipeline will load the weights from the best checkpoint.</p> <h2 id="parameters">Parameters</h2> <dl><dt><strong><code>output_dir</code></strong></dt> <dd>If specified, save the trained pipeline to this directory. Default: 'output'.</dd> <dt><strong><code>exist_ok</code></strong></dt> <dd>If True, overwrite the content of <code>output_dir</code>. Default: False.</dd></dl></dd> <pre class="title"><h3 id="test"><a href="#test" class="header-anchor">#</a> test <span class="badge tip" style="vertical-align:top;" data-v-15b7b770>Method</span></h3>
</pre> <dt><div class="language-python extra-class"><pre class="language-python"><code>
<span class="token keyword">def</span> <span class="ident">test</span> (
  self,
  test_dataset: Union[<a title="biome.text.dataset.Dataset" href="dataset.html#biome.text.dataset.Dataset">Dataset</a>, <a title="biome.text.dataset.AllennlpDataset" href="dataset.html#biome.text.dataset.AllennlpDataset">AllennlpDataset</a>, <a title="biome.text.dataset.AllennlpLazyDataset" href="dataset.html#biome.text.dataset.AllennlpLazyDataset">AllennlpLazyDataset</a>],
  batch_size: Union[int, NoneType] = None,
  output_dir: Union[pathlib.Path, str, NoneType] = None,
  verbose: bool = True,
)  -&gt; Dict[str, Any]
</code>
</pre></div></dt> <dd><p>Evaluate your model on a test dataset</p> <h2 id="parameters">Parameters</h2> <dl><dt><strong><code>test_dataset</code></strong></dt> <dd>The test data set.</dd> <dt><strong><code>batch_size</code></strong></dt> <dd>The batch size. If None (default), we will use the batch size specified in the <code>TrainerConfiguration</code>.</dd> <dt><strong><code>output_dir</code></strong></dt> <dd>Save a <code>metrics.json</code> to this output directory. Default: None.</dd> <dt><strong><code>verbose</code></strong></dt> <dd>If True, prints the test results. Default: True.</dd></dl> <h2 id="returns">Returns</h2> <dl><dt><code>Dict[str, Any]</code></dt> <dd>A dictionary with the metrics</dd></dl></dd></dl> <div></div> <pre class="title"><h2 id="modelcheckpointwithvocab"><a href="#modelcheckpointwithvocab" class="header-anchor">#</a> ModelCheckpointWithVocab <span class="badge tip" style="vertical-align:top;" data-v-15b7b770>Class</span></h2>
</pre> <pre class="language-python"><code>
<span class="token keyword">class</span> <span class="ident">ModelCheckpointWithVocab</span> (
    <span>dirpath: Union[str, pathlib.Path, NoneType] = None</span><span>,</span>
    <span>filename: Union[str, NoneType] = None</span><span>,</span>
    <span>monitor: Union[str, NoneType] = None</span><span>,</span>
    <span>verbose: bool = False</span><span>,</span>
    <span>save_last: Union[bool, NoneType] = None</span><span>,</span>
    <span>save_top_k: Union[int, NoneType] = None</span><span>,</span>
    <span>save_weights_only: bool = False</span><span>,</span>
    <span>mode: str = 'min'</span><span>,</span>
    <span>auto_insert_metric_name: bool = True</span><span>,</span>
    <span>every_n_train_steps: Union[int, NoneType] = None</span><span>,</span>
    <span>every_n_val_epochs: Union[int, NoneType] = None</span><span>,</span>
    <span>period: Union[int, NoneType] = None</span><span>,</span>
<span>)</span>
</code>
</pre> <p>Save the model periodically by monitoring a quantity. Every metric logged with
:meth:<code>~pytorch_lightning.core.lightning.log</code> or :meth:<code>~pytorch_lightning.core.lightning.log_dict</code> in
LightningModule is a candidate for the monitor key. For more information, see
:ref:<code>weights_loading</code>.</p> <p>After training finishes, use :attr:<code>best_model_path</code> to retrieve the path to the
best checkpoint file and :attr:<code>best_model_score</code> to retrieve its score.</p> <p>Args:
dirpath: directory to save the model file.</p> <pre><code>    Example::
<div class="language- extra-class"><pre><code>    # custom path
    # saves a file like: my/path/epoch=0-step=10.ckpt
    &amp;gt;&amp;gt;&amp;gt; checkpoint_callback = ModelCheckpoint(dirpath='my/path/')

By default, dirpath is &amp;lt;code&amp;gt;None&amp;lt;/code&amp;gt; and will be set at runtime to the location
specified by :class:`~pytorch_lightning.trainer.trainer.Trainer`'s
:paramref:`~pytorch_lightning.trainer.trainer.Trainer.default_root_dir` or
:paramref:`~pytorch_lightning.trainer.trainer.Trainer.weights_save_path` arguments,
and if the Trainer uses a logger, the path will also contain logger name and version.
</code></pre></div><p>filename: checkpoint filename. Can contain named formatting options to be auto-filled.</p> <div class="language- extra-class"><pre><code>Example::

    # save any arbitrary metrics like &amp;lt;code&amp;gt;val\_loss&amp;lt;/code&amp;gt;, etc. in name
    # saves a file like: my/path/epoch=2-val_loss=0.02-other_metric=0.03.ckpt
    &amp;gt;&amp;gt;&amp;gt; checkpoint_callback = ModelCheckpoint(
    ...     dirpath='my/path',
    ...     filename='{epoch}-{val_loss:.2f}-{other_metric:.2f}'
    ... )

By default, filename is &amp;lt;code&amp;gt;None&amp;lt;/code&amp;gt; and will be set to ``'{epoch}-{step}'``.
</code></pre></div><p>monitor: quantity to monitor. By default it is &lt;code&gt;None&lt;/code&gt; which saves a checkpoint only for the last epoch.
verbose: verbosity mode. Default: &lt;code&gt;False&lt;/code&gt;.
save_last: When &lt;code&gt;True&lt;/code&gt;, always saves the model at the end of the epoch to
a file &lt;code&gt;last.ckpt&lt;/code&gt;. Default: &lt;code&gt;None&lt;/code&gt;.
save_top_k: if <code>save_top_k == k</code>,
the best k models according to
the quantity monitored will be saved.
if <code>save_top_k == 0</code>, no models are saved.
if <code>save_top_k == -1</code>, all models are saved.
Please note that the monitors are checked every &lt;code&gt;period&lt;/code&gt; epochs.
if <code>save_top_k &amp;gt;= 2</code> and the callback is called multiple
times inside an epoch, the name of the saved file will be
appended with a version count starting with &lt;code&gt;v1&lt;/code&gt;.
mode: one of {min, max}.
If <code>save_top_k != 0</code>, the decision to overwrite the current save file is made
based on either the maximization or the minimization of the monitored quantity.
For <code>'val_acc'</code>, this should be <code>'max'</code>, for <code>'val_loss'</code> this should be <code>'min'</code>, etc.
save_weights_only: if &lt;code&gt;True&lt;/code&gt;, then only the model's weights will be
saved (&lt;code&gt;model.save_weights(filepath)&lt;/code&gt;), else the full model
is saved (&lt;code&gt;model.save(filepath)&lt;/code&gt;).
every_n_train_steps: Number of training steps between checkpoints.
If <code>every_n_train_steps == None or every_n_train_steps == 0</code>, we skip saving during training
To disable, set <code>every_n_train_steps = 0</code>. This value must be &lt;code&gt;None&lt;/code&gt; non-negative.
This must be mutually exclusive with &lt;code&gt;every_n_val_epochs&lt;/code&gt;.
every_n_val_epochs: Number of validation epochs between checkpoints.
If <code>every_n_val_epochs == None or every_n_val_epochs == 0</code>, we skip saving on validation end
To disable, set <code>every_n_val_epochs = 0</code>. This value must be &lt;code&gt;None&lt;/code&gt; or non-negative.
This must be mutually exclusive with &lt;code&gt;every_n_train_steps&lt;/code&gt;.
Setting both <code>ModelCheckpoint(..., every_n_val_epochs=V)</code> and
<code>Trainer(max_epochs=N, check_val_every_n_epoch=M)</code>
will only save checkpoints at epochs 0 &lt; E &lt;= N
where both values for &lt;code&gt;every_n_val_epochs&lt;/code&gt; and &lt;code&gt;check_val_every_n_epoch&lt;/code&gt; evenly divide E.
period: Interval (number of epochs) between checkpoints.</p> <div class="language- extra-class"><pre><code>!!! warning &quot;Warning&quot;
    This argument has been deprecated in v1.3 and will be removed in v1.5.

Use &amp;lt;code&amp;gt;every\_n\_val\_epochs&amp;lt;/code&amp;gt; instead.
</code></pre></div><p></p></code></pre> <p>Note:
For extra customization, ModelCheckpoint includes the following attributes:</p> <pre><code>- <code>CHECKPOINT_JOIN_CHAR = &quot;-&quot;</code><p></p>
<ul>
<li><code>CHECKPOINT_NAME_LAST = &quot;last&quot;</code></li>
<li><code>FILE_EXTENSION = &quot;.ckpt&quot;</code></li>
<li><code>STARTING_VERSION = 1</code></li>
</ul>
<p>For example, you can change the default last checkpoint name by doing
<code>checkpoint_callback.CHECKPOINT_NAME_LAST = &quot;{epoch}-last&quot;</code>
</p></code></pre> <p>Raises:
MisconfigurationException:
If <code>save_top_k</code> is neither <code>None</code> nor more than or equal to <code>-1</code>,
if <code>monitor</code> is <code>None</code> and <code>save_top_k</code> is none of <code>None</code>, <code>-1</code>, and <code>0</code>, or
if <code>mode</code> is none of <code>&quot;min&quot;</code> or <code>&quot;max&quot;</code>.
ValueError:
If <code>trainer.save_checkpoint</code> is <code>None</code>.</p> <p>Example::</p> <pre><code>&gt;&gt;&gt; from pytorch_lightning import Trainer
&gt;&gt;&gt; from pytorch_lightning.callbacks import ModelCheckpoint<p></p>
<h1 id="saves-checkpoints-to-my-path-at-every-epoch"><a href="#saves-checkpoints-to-my-path-at-every-epoch" class="header-anchor">#</a> saves checkpoints to 'my/path/' at every epoch</h1>
<p>&gt;&gt;&gt; checkpoint_callback = ModelCheckpoint(dirpath='my/path/')
&gt;&gt;&gt; trainer = Trainer(callbacks=[checkpoint_callback])</p>
<h1 id="save-epoch-and-val-loss-in-name"><a href="#save-epoch-and-val-loss-in-name" class="header-anchor">#</a> save epoch and val_loss in name</h1>
<h1 id="saves-a-file-like-my-path-sample-mnist-epoch-02-val-loss-0-32-ckpt"><a href="#saves-a-file-like-my-path-sample-mnist-epoch-02-val-loss-0-32-ckpt" class="header-anchor">#</a> saves a file like: my/path/sample-mnist-epoch=02-val_loss=0.32.ckpt</h1>
<p>&gt;&gt;&gt; checkpoint_callback = ModelCheckpoint(
...     monitor='val_loss',
...     dirpath='my/path/',
...     filename='sample-mnist-{epoch:02d}-{val_loss:.2f}'
... )</p>
<h1 id="save-epoch-and-val-loss-in-name-but-specify-the-formatting-yourself-e-g-to-avoid-problems-with-tensorboard"><a href="#save-epoch-and-val-loss-in-name-but-specify-the-formatting-yourself-e-g-to-avoid-problems-with-tensorboard" class="header-anchor">#</a> save epoch and val_loss in name, but specify the formatting yourself (e.g. to avoid problems with Tensorboard</h1>
<h1 id="or-neptune-due-to-the-presence-of-characters-like-or"><a href="#or-neptune-due-to-the-presence-of-characters-like-or" class="header-anchor">#</a> or Neptune, due to the presence of characters like '=' or '/')</h1>
<h1 id="saves-a-file-like-my-path-sample-mnist-epoch02-val-loss0-32-ckpt"><a href="#saves-a-file-like-my-path-sample-mnist-epoch02-val-loss0-32-ckpt" class="header-anchor">#</a> saves a file like: my/path/sample-mnist-epoch02-val_loss0.32.ckpt</h1>
<p>&gt;&gt;&gt; checkpoint_callback = ModelCheckpoint(
...     monitor='val/loss',
...     dirpath='my/path/',
...     filename='sample-mnist-epoch{epoch:02d}-val_loss{val/loss:.2f}',
...     auto_insert_metric_name=False
... )</p>
<h1 id="retrieve-the-best-checkpoint-after-training"><a href="#retrieve-the-best-checkpoint-after-training" class="header-anchor">#</a> retrieve the best checkpoint after training</h1>
<p>checkpoint_callback = ModelCheckpoint(dirpath='my/path/')
trainer = Trainer(callbacks=[checkpoint_callback])
model = ...
trainer.fit(model)
checkpoint_callback.best_model_path
</p></code></pre> <pre class="title"><p></p>
<h3 id="ancestors"><a href="#ancestors" class="header-anchor">#</a> Ancestors</h3>
</pre> <ul class="hlist"><li>pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint</li> <li>pytorch_lightning.callbacks.base.Callback</li> <li>abc.ABC</li></ul> <dl><pre class="title"><h3 id="on-pretrain-routine-start"><a href="#on-pretrain-routine-start" class="header-anchor">#</a> on_pretrain_routine_start <span class="badge tip" style="vertical-align:top;" data-v-15b7b770>Method</span></h3>
</pre> <dt><div class="language-python extra-class"><pre class="language-python"><code>
<span class="token keyword">def</span> <span class="ident">on_pretrain_routine_start</span> (
  self,
  trainer,
  pl_module: PipelineModel,
) 
</code>
</pre></div></dt> <dd><p>When pretrain routine starts we build the ckpt dir on the fly</p></dd></dl></div> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><span class="page-nav__button prev"><a href="/biome-text/master/api/biome/text/tokenizer.html" class="prev"><span class="page-nav__button__icon"><vp-icon color="#4A4A4A" name="chev-left" size="18px"></vp-icon></span> <span class="page-nav__button__text">
          biome.text.tokenizer
        </span></a></span> <span class="page-nav__button next"><a href="/biome-text/master/api/biome/text/vocabulary.html"><span class="page-nav__button__text">
          biome.text.vocabulary
        </span> <span class="page-nav__button__icon"><vp-icon color="#4A4A4A" name="chev-right" size="18px"></vp-icon></span></a></span></p></div> <footer class="footer" data-v-348088ed><div data-v-348088ed>
          Maintained by
          <a href="https://www.recogn.ai/" target="_blank" data-v-348088ed><img width="70px" src="/biome-text/master/assets/img/recognai.png" class="footer__img" data-v-348088ed></a></div></footer> </main></div><div class="global-ui"><!----></div></div>
    <script src="/biome-text/master/assets/js/app.eb668dca.js" defer></script><script src="/biome-text/master/assets/js/4.d8b631f7.js" defer></script><script src="/biome-text/master/assets/js/3.708237ef.js" defer></script><script src="/biome-text/master/assets/js/51.e254110a.js" defer></script><script src="/biome-text/master/assets/js/6.a484b546.js" defer></script>
  </body>
</html>
