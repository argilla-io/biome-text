<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>biome.text.trainer | biome.text</title>
    <meta name="generator" content="VuePress 1.8.2">
    <link rel="shortcut icon" href="/biome-text/master/favicon.ico">
    <meta name="description" content="biome.text practical NLP open source library.">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta property="og:image" content="https://www.recogn.ai/images/biome_og.png">
    
    <link rel="preload" href="/biome-text/master/assets/css/0.styles.26498fa5.css" as="style"><link rel="preload" href="/biome-text/master/assets/js/app.d65d4499.js" as="script"><link rel="preload" href="/biome-text/master/assets/js/4.8c7282f9.js" as="script"><link rel="preload" href="/biome-text/master/assets/js/3.c1cfb773.js" as="script"><link rel="preload" href="/biome-text/master/assets/js/56.b3992fb5.js" as="script"><link rel="preload" href="/biome-text/master/assets/js/6.4b76f008.js" as="script"><link rel="prefetch" href="/biome-text/master/assets/js/10.324582b8.js"><link rel="prefetch" href="/biome-text/master/assets/js/11.57a9d37d.js"><link rel="prefetch" href="/biome-text/master/assets/js/12.3884da43.js"><link rel="prefetch" href="/biome-text/master/assets/js/13.b4f4abc2.js"><link rel="prefetch" href="/biome-text/master/assets/js/14.9a88c2db.js"><link rel="prefetch" href="/biome-text/master/assets/js/15.28967300.js"><link rel="prefetch" href="/biome-text/master/assets/js/16.d5eb5412.js"><link rel="prefetch" href="/biome-text/master/assets/js/17.dc87945c.js"><link rel="prefetch" href="/biome-text/master/assets/js/18.e9a8c3c2.js"><link rel="prefetch" href="/biome-text/master/assets/js/19.4f9d38be.js"><link rel="prefetch" href="/biome-text/master/assets/js/20.2acd7bc5.js"><link rel="prefetch" href="/biome-text/master/assets/js/21.cc9a045d.js"><link rel="prefetch" href="/biome-text/master/assets/js/22.73902552.js"><link rel="prefetch" href="/biome-text/master/assets/js/23.e0234264.js"><link rel="prefetch" href="/biome-text/master/assets/js/24.b96a7ddd.js"><link rel="prefetch" href="/biome-text/master/assets/js/25.d1cbe125.js"><link rel="prefetch" href="/biome-text/master/assets/js/26.4f5ecab6.js"><link rel="prefetch" href="/biome-text/master/assets/js/27.e356ded2.js"><link rel="prefetch" href="/biome-text/master/assets/js/28.485bc4e7.js"><link rel="prefetch" href="/biome-text/master/assets/js/29.2af114e2.js"><link rel="prefetch" href="/biome-text/master/assets/js/30.0455164f.js"><link rel="prefetch" href="/biome-text/master/assets/js/31.26bd8e5a.js"><link rel="prefetch" href="/biome-text/master/assets/js/32.c2c03dbe.js"><link rel="prefetch" href="/biome-text/master/assets/js/33.f5c1d9f0.js"><link rel="prefetch" href="/biome-text/master/assets/js/34.3abafb22.js"><link rel="prefetch" href="/biome-text/master/assets/js/35.385f9a6a.js"><link rel="prefetch" href="/biome-text/master/assets/js/36.1b225f4e.js"><link rel="prefetch" href="/biome-text/master/assets/js/37.a959dc0c.js"><link rel="prefetch" href="/biome-text/master/assets/js/38.993aff5c.js"><link rel="prefetch" href="/biome-text/master/assets/js/39.81fc7c5c.js"><link rel="prefetch" href="/biome-text/master/assets/js/40.d88c670a.js"><link rel="prefetch" href="/biome-text/master/assets/js/41.cfa51658.js"><link rel="prefetch" href="/biome-text/master/assets/js/42.24fef139.js"><link rel="prefetch" href="/biome-text/master/assets/js/43.cdc2fec7.js"><link rel="prefetch" href="/biome-text/master/assets/js/44.5e07a493.js"><link rel="prefetch" href="/biome-text/master/assets/js/45.000d95fb.js"><link rel="prefetch" href="/biome-text/master/assets/js/46.ec85fe42.js"><link rel="prefetch" href="/biome-text/master/assets/js/47.f19730b4.js"><link rel="prefetch" href="/biome-text/master/assets/js/48.d9ac7bea.js"><link rel="prefetch" href="/biome-text/master/assets/js/49.7fdb0e8e.js"><link rel="prefetch" href="/biome-text/master/assets/js/5.73ece6cb.js"><link rel="prefetch" href="/biome-text/master/assets/js/50.de6ee286.js"><link rel="prefetch" href="/biome-text/master/assets/js/51.581cdc52.js"><link rel="prefetch" href="/biome-text/master/assets/js/52.7c287157.js"><link rel="prefetch" href="/biome-text/master/assets/js/53.6027f680.js"><link rel="prefetch" href="/biome-text/master/assets/js/54.77f4ccbe.js"><link rel="prefetch" href="/biome-text/master/assets/js/55.761b0603.js"><link rel="prefetch" href="/biome-text/master/assets/js/57.9f8e761f.js"><link rel="prefetch" href="/biome-text/master/assets/js/58.cd6e80c0.js"><link rel="prefetch" href="/biome-text/master/assets/js/59.1b5c164a.js"><link rel="prefetch" href="/biome-text/master/assets/js/60.ccde12e8.js"><link rel="prefetch" href="/biome-text/master/assets/js/61.5733ddbb.js"><link rel="prefetch" href="/biome-text/master/assets/js/62.a5b1d2ea.js"><link rel="prefetch" href="/biome-text/master/assets/js/63.a97520be.js"><link rel="prefetch" href="/biome-text/master/assets/js/64.91ffc21c.js"><link rel="prefetch" href="/biome-text/master/assets/js/65.c4a00c0b.js"><link rel="prefetch" href="/biome-text/master/assets/js/66.2ee948d4.js"><link rel="prefetch" href="/biome-text/master/assets/js/67.63d18121.js"><link rel="prefetch" href="/biome-text/master/assets/js/68.1c09a5f5.js"><link rel="prefetch" href="/biome-text/master/assets/js/69.f7318f0e.js"><link rel="prefetch" href="/biome-text/master/assets/js/7.5737c154.js"><link rel="prefetch" href="/biome-text/master/assets/js/70.c5eec0af.js"><link rel="prefetch" href="/biome-text/master/assets/js/71.4587089a.js"><link rel="prefetch" href="/biome-text/master/assets/js/8.6718f9cd.js"><link rel="prefetch" href="/biome-text/master/assets/js/9.58d92766.js"><link rel="prefetch" href="/biome-text/master/assets/js/vendors~docsearch.b3708bf9.js">
    <link rel="stylesheet" href="/biome-text/master/assets/css/0.styles.26498fa5.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container" data-v-348088ed><header class="navbar" data-v-348088ed><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/biome-text/master/" class="home-link router-link-active"><!----> <span class="site-name">biome<span>.text</span></span></a> <div class="links"><form id="search-form" role="search" class="algolia-search-wrapper search-box"><input id="algolia-search-input" class="search-query"></form> <nav class="nav-links can-hide"><div class="nav-item"><a href="/biome-text/master/api/" class="nav-link router-link-active">
  API
</a></div><div class="nav-item"><a href="/biome-text/master/documentation/" class="nav-link">
  Documentation
</a></div><div class="nav-item"><a href="https://github.com/recognai/biome-text" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span class="external__icon"><vp-icon color="#4A4A4A" name="blank" size="12px"></vp-icon></span></a></div><div class="nav-item"><a href="https://recogn.ai" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Recognai
  <span class="external__icon"><vp-icon color="#4A4A4A" name="blank" size="12px"></vp-icon></span></a></div> <!----></nav></div></header> <div class="sidebar-mask" data-v-348088ed></div> <aside class="sidebar" data-v-348088ed><div class="sidebar__link"><a href="/biome-text/master/"><img src="/biome-text/master/assets/img/biome.svg" class="sidebar__img"></a></div> <!----> <nav class="nav-links"><div class="nav-item"><a href="/biome-text/master/api/" class="nav-link router-link-active">
  API
</a></div><div class="nav-item"><a href="/biome-text/master/documentation/" class="nav-link">
  Documentation
</a></div><div class="nav-item"><a href="https://github.com/recognai/biome-text" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span class="external__icon"><vp-icon color="#4A4A4A" name="blank" size="12px"></vp-icon></span></a></div><div class="nav-item"><a href="https://recogn.ai" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Recognai
  <span class="external__icon"><vp-icon color="#4A4A4A" name="blank" size="12px"></vp-icon></span></a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group depth-0"><p class="sidebar-heading open"><span>API</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/biome-text/master/api/biome/text/backbone.html" class="sidebar-link">biome.text.backbone</a></li><li><a href="/biome-text/master/api/biome/text/cli/cli.html" class="sidebar-link">biome.text.cli.cli</a></li><li><a href="/biome-text/master/api/biome/text/cli/evaluate.html" class="sidebar-link">biome.text.cli.evaluate</a></li><li><a href="/biome-text/master/api/biome/text/cli/serve.html" class="sidebar-link">biome.text.cli.serve</a></li><li><a href="/biome-text/master/api/biome/text/cli/train.html" class="sidebar-link">biome.text.cli.train</a></li><li><a href="/biome-text/master/api/biome/text/commons.html" class="sidebar-link">biome.text.commons</a></li><li><a href="/biome-text/master/api/biome/text/configuration.html" class="sidebar-link">biome.text.configuration</a></li><li><a href="/biome-text/master/api/biome/text/constants.html" class="sidebar-link">biome.text.constants</a></li><li><a href="/biome-text/master/api/biome/text/dataset.html" class="sidebar-link">biome.text.dataset</a></li><li><a href="/biome-text/master/api/biome/text/environment.html" class="sidebar-link">biome.text.environment</a></li><li><a href="/biome-text/master/api/biome/text/errors.html" class="sidebar-link">biome.text.errors</a></li><li><a href="/biome-text/master/api/biome/text/explore.html" class="sidebar-link">biome.text.explore</a></li><li><a href="/biome-text/master/api/biome/text/features.html" class="sidebar-link">biome.text.features</a></li><li><a href="/biome-text/master/api/biome/text/featurizer.html" class="sidebar-link">biome.text.featurizer</a></li><li><a href="/biome-text/master/api/biome/text/helpers.html" class="sidebar-link">biome.text.helpers</a></li><li><a href="/biome-text/master/api/biome/text/hpo.html" class="sidebar-link">biome.text.hpo</a></li><li><a href="/biome-text/master/api/biome/text/loggers.html" class="sidebar-link">biome.text.loggers</a></li><li><a href="/biome-text/master/api/biome/text/metrics.html" class="sidebar-link">biome.text.metrics</a></li><li><a href="/biome-text/master/api/biome/text/mlflow_model.html" class="sidebar-link">biome.text.mlflow_model</a></li><li><a href="/biome-text/master/api/biome/text/model.html" class="sidebar-link">biome.text.model</a></li><li><a href="/biome-text/master/api/biome/text/modules/configuration/allennlp_configuration.html" class="sidebar-link">biome.text.modules.configuration.allennlp_configuration</a></li><li><a href="/biome-text/master/api/biome/text/modules/configuration/defs.html" class="sidebar-link">biome.text.modules.configuration.defs</a></li><li><a href="/biome-text/master/api/biome/text/modules/encoders/time_distributed_encoder.html" class="sidebar-link">biome.text.modules.encoders.timedistributedencoder</a></li><li><a href="/biome-text/master/api/biome/text/modules/heads/classification/classification.html" class="sidebar-link">biome.text.modules.heads.classification.classification</a></li><li><a href="/biome-text/master/api/biome/text/modules/heads/classification/doc_classification.html" class="sidebar-link">biome.text.modules.heads.classification.doc_classification</a></li><li><a href="/biome-text/master/api/biome/text/modules/heads/classification/record_classification.html" class="sidebar-link">biome.text.modules.heads.classification.record_classification</a></li><li><a href="/biome-text/master/api/biome/text/modules/heads/classification/record_pair_classification.html" class="sidebar-link">biome.text.modules.heads.classification.recordpairclassification</a></li><li><a href="/biome-text/master/api/biome/text/modules/heads/classification/relation_classification.html" class="sidebar-link">biome.text.modules.heads.classification.relation_classification</a></li><li><a href="/biome-text/master/api/biome/text/modules/heads/classification/text_classification.html" class="sidebar-link">biome.text.modules.heads.classification.text_classification</a></li><li><a href="/biome-text/master/api/biome/text/modules/heads/language_modelling.html" class="sidebar-link">biome.text.modules.heads.language_modelling</a></li><li><a href="/biome-text/master/api/biome/text/modules/heads/task_head.html" class="sidebar-link">biome.text.modules.heads.task_head</a></li><li><a href="/biome-text/master/api/biome/text/modules/heads/task_prediction.html" class="sidebar-link">biome.text.modules.heads.task_prediction</a></li><li><a href="/biome-text/master/api/biome/text/modules/heads/token_classification.html" class="sidebar-link">biome.text.modules.heads.token_classification</a></li><li><a href="/biome-text/master/api/biome/text/pipeline.html" class="sidebar-link">biome.text.pipeline</a></li><li><a href="/biome-text/master/api/biome/text/text_cleaning.html" class="sidebar-link">biome.text.text_cleaning</a></li><li><a href="/biome-text/master/api/biome/text/tokenizer.html" class="sidebar-link">biome.text.tokenizer</a></li><li><a href="/biome-text/master/api/biome/text/trainer.html" aria-current="page" class="active sidebar-link">biome.text.trainer</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/biome-text/master/api/biome/text/trainer.html#trainer" class="sidebar-link">Trainer</a></li></ul></li><li><a href="/biome-text/master/api/biome/text/training_results.html" class="sidebar-link">biome.text.training_results</a></li><li><a href="/biome-text/master/api/biome/text/ui/app.html" class="sidebar-link">biome.text.ui.app</a></li><li><a href="/biome-text/master/api/biome/text/ui/ui.html" class="sidebar-link">biome.text.ui.ui</a></li><li><a href="/biome-text/master/api/biome/text/vocabulary.html" class="sidebar-link">biome.text.vocabulary</a></li></ul></section></li></ul> </aside> <main class="page" data-v-348088ed> <div class="theme-default-content content__default"><h1 id="biome-text-trainer"><a href="#biome-text-trainer" class="header-anchor">#</a> biome.text.trainer <span class="badge tip" style="vertical-align:top;" data-v-15b7b770>Module</span></h1> <div></div> <pre class="title"><h3 id="create-dataloader"><a href="#create-dataloader" class="header-anchor">#</a> create_dataloader <span class="badge tip" style="vertical-align:top;" data-v-15b7b770>Function</span></h3>
</pre> <dt><div class="language-python extra-class"><pre class="language-python"><code>
<span class="token keyword">def</span> <span class="ident">create_dataloader</span> (
  instance_dataset: Union[allennlp.data.dataset_readers.dataset_reader.AllennlpDataset, allennlp.data.dataset_readers.dataset_reader.AllennlpLazyDataset],
  batch_size: int = 16,
  data_bucketing: bool = False,
)  -&gt; allennlp.data.dataloader.PyTorchDataLoader
</code>
</pre></div></dt> <dd><p>Returns a pytorch DataLoader for AllenNLP instances</p> <h2 id="parameters">Parameters</h2> <dl><dt><strong><code>instance_dataset</code></strong></dt> <dd>The dataset of instances for the DataLoader</dd></dl> <h2 id="returns">Returns</h2> <dl><dt><code>data_loader</code></dt> <dd> </dd></dl></dd> <div></div> <pre class="title"><h2 id="trainer"><a href="#trainer" class="header-anchor">#</a> Trainer <span class="badge tip" style="vertical-align:top;" data-v-15b7b770>Class</span></h2>
</pre> <pre class="language-python"><code>
<span class="token keyword">class</span> <span class="ident">Trainer</span> (
    <span>logger: Union[pytorch_lightning.loggers.base.LightningLoggerBase, Iterable[pytorch_lightning.loggers.base.LightningLoggerBase], bool] = True</span><span>,</span>
    <span>checkpoint_callback: bool = True</span><span>,</span>
    <span>callbacks: Union[List[pytorch_lightning.callbacks.base.Callback], pytorch_lightning.callbacks.base.Callback, NoneType] = None</span><span>,</span>
    <span>default_root_dir: Union[str, NoneType] = None</span><span>,</span>
    <span>gradient_clip_val: float = 0</span><span>,</span>
    <span>process_position: int = 0</span><span>,</span>
    <span>num_nodes: int = 1</span><span>,</span>
    <span>num_processes: int = 1</span><span>,</span>
    <span>gpus: Union[List[int], str, int, NoneType] = None</span><span>,</span>
    <span>auto_select_gpus: bool = False</span><span>,</span>
    <span>tpu_cores: Union[List[int], str, int, NoneType] = None</span><span>,</span>
    <span>log_gpu_memory: Union[str, NoneType] = None</span><span>,</span>
    <span>progress_bar_refresh_rate: Union[int, NoneType] = None</span><span>,</span>
    <span>overfit_batches: Union[int, float] = 0.0</span><span>,</span>
    <span>track_grad_norm: Union[int, float, str] = -1</span><span>,</span>
    <span>check_val_every_n_epoch: int = 1</span><span>,</span>
    <span>fast_dev_run: Union[int, bool] = False</span><span>,</span>
    <span>accumulate_grad_batches: Union[int, Dict[int, int], List[list]] = 1</span><span>,</span>
    <span>max_epochs: Union[int, NoneType] = None</span><span>,</span>
    <span>min_epochs: Union[int, NoneType] = None</span><span>,</span>
    <span>max_steps: Union[int, NoneType] = None</span><span>,</span>
    <span>min_steps: Union[int, NoneType] = None</span><span>,</span>
    <span>limit_train_batches: Union[int, float] = 1.0</span><span>,</span>
    <span>limit_val_batches: Union[int, float] = 1.0</span><span>,</span>
    <span>limit_test_batches: Union[int, float] = 1.0</span><span>,</span>
    <span>limit_predict_batches: Union[int, float] = 1.0</span><span>,</span>
    <span>val_check_interval: Union[int, float] = 1.0</span><span>,</span>
    <span>flush_logs_every_n_steps: int = 100</span><span>,</span>
    <span>log_every_n_steps: int = 50</span><span>,</span>
    <span>accelerator: Union[str, pytorch_lightning.accelerators.accelerator.Accelerator, NoneType] = None</span><span>,</span>
    <span>sync_batchnorm: bool = False</span><span>,</span>
    <span>precision: int = 32</span><span>,</span>
    <span>weights_summary: Union[str, NoneType] = 'top'</span><span>,</span>
    <span>weights_save_path: Union[str, NoneType] = None</span><span>,</span>
    <span>num_sanity_val_steps: int = 2</span><span>,</span>
    <span>truncated_bptt_steps: Union[int, NoneType] = None</span><span>,</span>
    <span>resume_from_checkpoint: Union[pathlib.Path, str, NoneType] = None</span><span>,</span>
    <span>profiler: Union[pytorch_lightning.profiler.profilers.BaseProfiler, bool, str, NoneType] = None</span><span>,</span>
    <span>benchmark: bool = False</span><span>,</span>
    <span>deterministic: bool = False</span><span>,</span>
    <span>reload_dataloaders_every_epoch: bool = False</span><span>,</span>
    <span>auto_lr_find: Union[bool, str] = False</span><span>,</span>
    <span>replace_sampler_ddp: bool = True</span><span>,</span>
    <span>terminate_on_nan: bool = False</span><span>,</span>
    <span>auto_scale_batch_size: Union[str, bool] = False</span><span>,</span>
    <span>prepare_data_per_node: bool = True</span><span>,</span>
    <span>plugins: Union[pytorch_lightning.plugins.base_plugin.Plugin, str, list, NoneType] = None</span><span>,</span>
    <span>amp_backend: str = 'native'</span><span>,</span>
    <span>amp_level: str = 'O2'</span><span>,</span>
    <span>distributed_backend: Union[str, NoneType] = None</span><span>,</span>
    <span>automatic_optimization: Union[bool, NoneType] = None</span><span>,</span>
    <span>move_metrics_to_cpu: bool = False</span><span>,</span>
    <span>multiple_trainloader_mode: str = 'max_size_cycle'</span><span>,</span>
    <span>stochastic_weight_avg: bool = False</span><span>,</span>
    <span>batch_size: int = 16</span><span>,</span>
    <span>data_bucketing: bool = False</span><span>,</span>
    <span>optimizer: Union[Dict, NoneType] = None</span><span>,</span>
<span>)</span>
</code>
</pre> <p>A class for training a <code>biome.text.Pipeline</code>.</p> <p>It is basically a light wrapper around the awesome Pytorch Lightning Trainer to facilitate the interaction
with our pipelines. The docs are mainly a copy from the
<a href="https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.html#pytorch_lightning.trainer.trainer.Trainer">Lightning Trainer API</a>
with some additional parameters added.</p> <h2 id="parameters">Parameters</h2> <dl><dt><strong><code>accelerator</code></strong></dt> <dd>Previously known as distributed_backend (dp, ddp, ddp2, etc…).
Can also take in an accelerator object for custom hardware.</dd> <dt><strong><code>accumulate_grad_batches</code></strong></dt> <dd>Accumulates grads every k batches or as set up in the dict.</dd> <dt><strong><code>amp_backend</code></strong></dt> <dd>The mixed precision backend to use (&quot;native&quot; or &quot;apex&quot;)</dd> <dt><strong><code>amp_level</code></strong></dt> <dd>The optimization level to use (O1, O2, etc…).</dd> <dt><strong><code>auto_lr_find</code></strong></dt> <dd>If set to True, will make trainer.tune() run a learning rate finder,
trying to optimize initial learning for faster convergence. trainer.tune() method will
set the suggested learning rate in self.lr or self.learning_rate in the LightningModule.
To use a different key set a string instead of True with the key name.</dd> <dt><strong><code>auto_scale_batch_size</code></strong></dt> <dd>If set to True, will <code>initially</code> run a batch size
finder trying to find the largest batch size that fits into memory.
The result will be stored in self.batch_size in the LightningModule.
Additionally, can be set to either <code>power</code> that estimates the batch size through
a power search or <code>binsearch</code> that estimates the batch size through a binary search.</dd> <dt><strong><code>auto_select_gpus</code></strong></dt> <dd>If enabled and <code>gpus</code> is an integer, pick available
gpus automatically. This is especially useful when
GPUs are configured to be in &quot;exclusive mode&quot;, such
that only one process at a time can access them.</dd> <dt><strong><code>batch_size</code></strong></dt> <dd>Size of the batch.</dd> <dt><strong><code>benchmark</code></strong></dt> <dd>If true enables cudnn.benchmark.</dd> <dt><strong><code>callbacks</code></strong></dt> <dd>Add a callback or list of callbacks.</dd> <dt><strong><code>checkpoint_callback</code></strong></dt> <dd>If <code>True</code>, enable checkpointing.
It will configure a default ModelCheckpoint callback if there is no user-defined ModelCheckpoint in
:paramref:<code>~pytorch_lightning.trainer.trainer.Trainer.callbacks</code>. Default: <code>True</code>.</dd> <dt><strong><code>check_val_every_n_epoch</code></strong></dt> <dd>Check val every n train epochs.</dd> <dt><strong><code>data_bucketing</code></strong></dt> <dd>If enabled, try to apply data bucketing over training batches.</dd> <dt><strong><code>default_root_dir</code></strong></dt> <dd>Default path for logs and weights when no logger/ckpt_callback passed.
Default: <code>os.getcwd()</code>.
Can be remote file paths such as <code>s3://mybucket/path</code> or 'hdfs://path/'</dd> <dt><strong><code>deterministic</code></strong></dt> <dd>If true enables cudnn.deterministic.</dd> <dt><strong><code>fast_dev_run</code></strong></dt> <dd>runs n if set to <code>n</code> (int) else 1 if set to <code>True</code> batch(es)
of train, val and test to find any bugs (ie: a sort of unit test).</dd> <dt><strong><code>flush_logs_every_n_steps</code></strong></dt> <dd>How often to flush logs to disk (defaults to every 100 steps).</dd> <dt><strong><code>gpus</code></strong></dt> <dd>number of gpus to train on (int) or which GPUs to train on (list or str) applied per node</dd> <dt><strong><code>gradient_clip_val</code></strong></dt> <dd>0 means don't clip.</dd> <dt><strong><code>limit_train_batches</code></strong></dt> <dd>How much of training dataset to check (floats = percent, int = num_batches)</dd> <dt><strong><code>limit_val_batches</code></strong></dt> <dd>How much of validation dataset to check (floats = percent, int = num_batches)</dd> <dt><strong><code>limit_test_batches</code></strong></dt> <dd>How much of test dataset to check (floats = percent, int = num_batches)</dd> <dt><strong><code>log_every_n_steps</code></strong></dt> <dd>How often to log within steps (defaults to every 50 steps).</dd> <dt><strong><code>log_gpu_memory</code></strong></dt> <dd>None, 'min_max', 'all'. Might slow performance</dd> <dt><strong><code>logger</code></strong></dt> <dd>Logger (or iterable collection of loggers) for experiment tracking.</dd> <dt><strong><code>prepare_data_per_node</code></strong></dt> <dd>If True, each LOCAL_RANK=0 will call prepare data.
Otherwise only NODE_RANK=0, LOCAL_RANK=0 will prepare data</dd> <dt><strong><code>process_position</code></strong></dt> <dd>orders the progress bar when running multiple models on same machine.</dd> <dt><strong><code>progress_bar_refresh_rate</code></strong></dt> <dd>How often to refresh progress bar (in steps). Value <code>0</code> disables progress bar.
Ignored when a custom progress bar is passed to :paramref:<code>~Trainer.callbacks</code>. Default: None, means
a suitable value will be chosen based on the environment (terminal, Google COLAB, etc.).</dd> <dt><strong><code>profiler</code></strong></dt> <dd>To profile individual steps during training and assist in identifying bottlenecks. Passing bool
value is deprecated in v1.1 and will be removed in v1.3.</dd> <dt><strong><code>overfit_batches</code></strong></dt> <dd>Overfit a percent of training data (float) or a set number of batches (int). Default: 0.0</dd> <dt><strong><code>plugins</code></strong></dt> <dd>Plugins allow modification of core behavior like ddp and amp, and enable custom lightning plugins.</dd> <dt><strong><code>precision</code></strong></dt> <dd>Full precision (32), half precision (16). Can be used on CPU, GPU or TPUs.</dd> <dt><strong><code>max_epochs</code></strong></dt> <dd>Stop training once this number of epochs is reached. Disabled by default (None).
If both max_epochs and max_steps are not specified, defaults to <code>max_epochs</code> = 1000.</dd> <dt><strong><code>min_epochs</code></strong></dt> <dd>Force training for at least these many epochs. Disabled by default (None).
If both min_epochs and min_steps are not specified, defaults to <code>min_epochs</code> = 1.</dd> <dt><strong><code>max_steps</code></strong></dt> <dd>Stop training after this number of steps. Disabled by default (None).</dd> <dt><strong><code>min_steps</code></strong></dt> <dd>Force training for at least these number of steps. Disabled by default (None).</dd> <dt><strong><code>num_nodes</code></strong></dt> <dd>number of GPU nodes for distributed training.</dd> <dt><strong><code>num_processes</code></strong></dt> <dd>number of processes for distributed training with distributed_backend=&quot;ddp_cpu&quot;</dd> <dt><strong><code>num_sanity_val_steps</code></strong></dt> <dd>Sanity check runs n validation batches before starting the training routine.
Set it to <code>-1</code> to run all batches in all validation dataloaders. Default: 2</dd> <dt><strong><code>optimizer</code></strong></dt> <dd>Configuration for an <a href="https://docs.allennlp.org/main/api/training/optimizers/">AllenNLP/PyTorch optimizer</a>
that is constructed via the AllenNLP configuration framework.
The default is:<blockquote><blockquote><blockquote><p>optimizer={&quot;type&quot;: &quot;adam&quot;, &quot;lr&quot;: 0.001}</p></blockquote></blockquote></blockquote></dd> <dt><strong><code>reload_dataloaders_every_epoch</code></strong></dt> <dd>Set to True to reload dataloaders every epoch.</dd> <dt><strong><code>replace_sampler_ddp</code></strong></dt> <dd>Explicitly enables or disables sampler replacement. If not specified this
will toggled automatically when DDP is used. By default it will add <code>shuffle=True</code> for
train sampler and <code>shuffle=False</code> for val/test sampler. If you want to customize it,
you can set <code>replace_sampler_ddp=False</code> and add your own distributed sampler.</dd> <dt><strong><code>resume_from_checkpoint</code></strong></dt> <dd>Path/URL of the checkpoint from which training is resumed. If there is
no checkpoint file at the path, start from scratch. If resuming from mid-epoch checkpoint,
training will start from the beginning of the next epoch.</dd> <dt><strong><code>sync_batchnorm</code></strong></dt> <dd>Synchronize batch norm layers between process groups/whole world.</dd> <dt><strong><code>terminate_on_nan</code></strong></dt> <dd>If set to True, will terminate training (by raising a <code>ValueError</code>) at the
end of each training batch, if any of the parameters or the loss are NaN or +/-inf.</dd> <dt><strong><code>tpu_cores</code></strong></dt> <dd>How many TPU cores to train on (1 or 8) / Single TPU to train on [1]</dd> <dt><strong><code>track_grad_norm</code></strong></dt> <dd>-1 no tracking. Otherwise tracks that p-norm. May be set to 'inf' infinity-norm.</dd> <dt><strong><code>truncated_bptt_steps</code></strong></dt> <dd>Truncated back prop breaks performs backprop every k steps of much longer
sequence.</dd> <dt><strong><code>val_check_interval</code></strong></dt> <dd>How often to check the validation set. Use float to check within a training epoch,
use int to check every n steps (batches).</dd> <dt><strong><code>weights_summary</code></strong></dt> <dd>Prints a summary of the weights when training begins.</dd> <dt><strong><code>weights_save_path</code></strong></dt> <dd>Where to save weights if specified. Will override default_root_dir
for checkpoints only. Use this if for whatever reason you need the checkpoints
stored in a different place than the logs written in <code>default_root_dir</code>.
Can be remote file paths such as <code>s3://mybucket/path</code> or 'hdfs://path/'
Defaults to <code>default_root_dir</code>.</dd> <dt><strong><code>move_metrics_to_cpu</code></strong></dt> <dd>Whether to force internal logged metrics to be moved to cpu.
This can save some gpu memory, but can make training slower. Use with attention.</dd> <dt><strong><code>multiple_trainloader_mode</code></strong></dt> <dd>How to loop over the datasets when there are multiple train loaders.
In 'max_size_cycle' mode, the trainer ends one epoch when the largest dataset is traversed,
and smaller datasets reload when running out of their data. In 'min_size' mode, all the datasets
reload when reaching the minimum length of datasets.</dd> <dt><strong><code>stochastic_weight_avg</code></strong></dt> <dd>Whether to use <code>Stochastic Weight Averaging (SWA)
&lt;https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/&gt;_</code></dd></dl> <dl><pre class="title"><h3 id="fit"><a href="#fit" class="header-anchor">#</a> fit <span class="badge tip" style="vertical-align:top;" data-v-15b7b770>Method</span></h3>
</pre> <dt><div class="language-python extra-class"><pre class="language-python"><code>
<span class="token keyword">def</span> <span class="ident">fit</span> (
  self,
  pipeline: <a title="biome.text.pipeline.Pipeline" href="pipeline.html#biome.text.pipeline.Pipeline">Pipeline</a>,
  train_dataset: <a title="biome.text.dataset.Dataset" href="dataset.html#biome.text.dataset.Dataset">Dataset</a>,
  valid_dataset: <a title="biome.text.dataset.Dataset" href="dataset.html#biome.text.dataset.Dataset">Dataset</a>,
  vocab_config: Union[<a title="biome.text.configuration.VocabularyConfiguration" href="configuration.html#biome.text.configuration.VocabularyConfiguration">VocabularyConfiguration</a>, str, NoneType] = 'default',
  lazy: bool = False,
) 
</code>
</pre></div></dt> <dd><p>Train the pipeline</p> <h2 id="parameters">Parameters</h2> <dl><dt><strong><code>pipeline</code></strong></dt> <dd>Pipeline to train</dd> <dt><strong><code>train_dataset</code></strong></dt> <dd>The training dataset</dd> <dt><strong><code>valid_dataset</code></strong></dt> <dd>The validation dataset</dd> <dt><strong><code>vocab_config</code></strong></dt> <dd>A <code>VocabularyConfiguration</code> to create/extend the pipeline's vocabulary.
If 'default' (str), we will use the default configuration
<code>VocabularyConfiguration(datasets=[training_data])</code>.
If None, we will leave the pipeline's vocabulary untouched.</dd> <dt><strong><code>lazy</code></strong></dt> <dd>If true, instances are lazily loaded from disk, otherwise they are loaded into memory.</dd></dl></dd></dl></div> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><span class="page-nav__button prev"><a href="/biome-text/master/api/biome/text/tokenizer.html" class="prev"><span class="page-nav__button__icon"><vp-icon color="#4A4A4A" name="chev-left" size="18px"></vp-icon></span> <span class="page-nav__button__text">
          biome.text.tokenizer
        </span></a></span> <span class="page-nav__button next"><a href="/biome-text/master/api/biome/text/training_results.html"><span class="page-nav__button__text">
          biome.text.training_results
        </span> <span class="page-nav__button__icon"><vp-icon color="#4A4A4A" name="chev-right" size="18px"></vp-icon></span></a></span></p></div> <footer class="footer" data-v-348088ed><div data-v-348088ed>
          Maintained by
          <a href="https://www.recogn.ai/" target="_blank" data-v-348088ed><img width="70px" src="/biome-text/master/assets/img/recognai.png" class="footer__img" data-v-348088ed></a></div></footer> </main></div><div class="global-ui"><!----></div></div>
    <script src="/biome-text/master/assets/js/app.d65d4499.js" defer></script><script src="/biome-text/master/assets/js/4.8c7282f9.js" defer></script><script src="/biome-text/master/assets/js/3.c1cfb773.js" defer></script><script src="/biome-text/master/assets/js/56.b3992fb5.js" defer></script><script src="/biome-text/master/assets/js/6.4b76f008.js" defer></script>
  </body>
</html>
