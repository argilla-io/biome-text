{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization with Ray Tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://recognai.github.io/biome-text/v3.2.0/documentation/tutorials/3-Hyperparameter_optimization_with_Ray_Tune.html\"><img class=\"icon\" src=\"https://recognai.github.io/biome-text/v3.2.0/assets/img/biome-isotype.svg\" width=24 /></a>\n",
    "[View on recogn.ai](https://recognai.github.io/biome-text/v3.2.0/documentation/tutorials/3-Hyperparameter_optimization_with_Ray_Tune.html)\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/recognai/biome-text/blob/v3.2.0/docs/docs/documentation/tutorials/3-Hyperparameter_optimization_with_Ray_Tune.ipynb\"><img class=\"icon\" src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" width=24 /></a>\n",
    "[Run in Google Colab](https://colab.research.google.com/github/recognai/biome-text/blob/v3.2.0/docs/docs/documentation/tutorials/3-Hyperparameter_optimization_with_Ray_Tune.ipynb)\n",
    "\n",
    "<a target=\"_blank\" href=\"https://github.com/recognai/biome-text/blob/v3.2.0/docs/docs/documentation/tutorials/3-Hyperparameter_optimization_with_Ray_Tune.ipynb\"><img class=\"icon\" src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\" width=24 /></a>\n",
    "[View source on GitHub](https://github.com/recognai/biome-text/blob/v3.2.0/docs/docs/documentation/tutorials/3-Hyperparameter_optimization_with_Ray_Tune.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running this tutorial in Google Colab, make sure to install *biome.text* and *ray tune* first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pip\n",
    "!pip install -U biome-text\n",
    "exit(0)  # Force restart of the runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to log your runs with [WandB](https://wandb.ai), don't forget to install its client and log in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb\n",
    "!wandb login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: tip Note\n",
    "\n",
    "In this tutorial we will use a GPU by default.\n",
    "So when running this tutorial in Google Colab, make sure that you request one (*Edit -> Notebook settings*).\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will optimize the hyperparameters of the short-text classifier from [this tutorial](https://recognai.github.io/biome-text/v3.2.0/documentation/tutorials/1-Training_a_text_classifier.html), hence we recommend to have a look at it first before going through this tutorial.\n",
    "For the Hyper-Parameter Optimization (HPO) we rely on the awesome [Ray Tune library](https://docs.ray.io/en/latest/tune.html#tune-index).\n",
    "\n",
    "For a short introduction to HPO with Ray Tune you can have a look at this nice [talk](https://www.youtube.com/watch?v=VX7HvEoMrsA) by Richard Liaw. \n",
    "We will follow his terminology and use the term *trial* to refer to a training run of one set of hyperparameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "First let's import all the stuff we need for this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from biome.text import Pipeline, Dataset\n",
    "from biome.text.configuration import TrainerConfiguration\n",
    "from biome.text.hpo import TuneExperiment\n",
    "from ray import tune\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step we will download the training and validation data to our local machine, and create our datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O https://biome-tutorials-data.s3-eu-west-1.amazonaws.com/text_classifier/business.cat.train.csv\n",
    "!curl -O https://biome-tutorials-data.s3-eu-west-1.amazonaws.com/text_classifier/business.cat.valid.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_csv(\"business.cat.train.csv\")\n",
    "valid_ds = Dataset.from_csv(\"business.cat.valid.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the pipeline and the search space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the introduction we will use the same pipeline configuration as used in the [base tutorial](https://recognai.github.io/biome-text/v3.2.0/documentation/tutorials/1-Training_a_text_classifier.html)).\n",
    "\n",
    "To perform a random hyperparameter search (as well as a grid search) we simply have to replace the parameters we want to optimize with methods from the [Random Distributions API](https://docs.ray.io/en/latest/tune/api_docs/search_space.html#random-distributions-api) or the [Grid Search API](https://docs.ray.io/en/latest/tune/api_docs/search_space.html#grid-search-api) in our configuration dictionaries.\n",
    "For a complete description of both APIs and how they interplay with each other, see the corresponding section in the [Ray Tune docs](https://docs.ray.io/en/latest/tune/api_docs/search_space.html). \n",
    "\n",
    "In our case we will tune 9 parameters:\n",
    "- the output dimensions of our `word` and `char` features\n",
    "- the dropout of our `char` feature\n",
    "- the architecture of our pooler (*GRU* versus *LSTM*)\n",
    "- number of layers and hidden size of our pooler, as well as if it should be bidirectional\n",
    "- hidden dimension of our feed forward network\n",
    "- and the learning rate\n",
    "\n",
    "For most of the parameters we will provide discrete values from which Tune will sample randomly, while for the dropout and learning rate we will provide a continuous linear and logarithmic range, respectively.\n",
    "Since we want to directly compare the outcome of the optimization with the base configuration of the [underlying tutorial](https://recognai.github.io/biome-text/v3.2.0/documentation/tutorials/1-Training_a_text_classifier.html), we will fix the number of epochs to 3.\n",
    "\n",
    "Not all of the parameters above are worth tuning, but we want to stress the flexibility that *Ray Tune* and *biome.text* offers you.\n",
    "\n",
    "::: tip Tip\n",
    "\n",
    "Keep in mind that the learning rate \"*is often the single most important hyper-parameter and one should always make sure that it has been tuned (up to approximately a factor of 2). ... If there is only time to optimize one hyper-parameter and one uses stochastic gradient descent, then this is the hyper-parameter that is worth tuning.*\" ([Yoshua Bengio](https://arxiv.org/abs/1206.5533)).\n",
    "\n",
    ":::\n",
    "\n",
    "In the following configuration dictionaries we replaced the relevant parameters with tune's search spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the search spaces in our pipeline config\n",
    "pipeline_config = {\n",
    "    \"name\": \"german_business_names\",\n",
    "    \"tokenizer\": {\n",
    "        \"text_cleaning\": {\n",
    "            \"rules\": [\"strip_spaces\"]\n",
    "        }\n",
    "    },\n",
    "    \"features\": {\n",
    "        \"word\": {\n",
    "            \"embedding_dim\": tune.choice([32, 64]),\n",
    "            \"lowercase_tokens\": True,\n",
    "        },\n",
    "        \"char\": {\n",
    "            \"embedding_dim\": 32,\n",
    "            \"lowercase_characters\": True,\n",
    "            \"encoder\": {\n",
    "                \"type\": \"gru\",\n",
    "                \"num_layers\": 1,\n",
    "                \"hidden_size\": tune.choice([32, 64]),\n",
    "                \"bidirectional\": True,\n",
    "            },\n",
    "            \"dropout\": tune.uniform(0, 0.5),\n",
    "        },\n",
    "    },\n",
    "    \"head\": {\n",
    "        \"type\": \"TextClassification\",\n",
    "        \"labels\": list(set(train_ds[\"label\"])),\n",
    "        \"pooler\": {\n",
    "            \"type\": tune.choice([\"gru\", \"lstm\"]),\n",
    "            \"num_layers\": tune.choice([1, 2]),\n",
    "            \"hidden_size\": tune.choice([32, 64]),\n",
    "            \"bidirectional\": tune.choice([True, False]),\n",
    "        },\n",
    "        \"feedforward\": {\n",
    "            \"num_layers\": 1,\n",
    "            \"hidden_dims\": tune.choice([32, 64]),\n",
    "            \"activations\": [\"relu\"],\n",
    "            \"dropout\": [0.0],\n",
    "        },\n",
    "    },       \n",
    "}\n",
    "\n",
    "# defining the search spaces in our trainer config\n",
    "trainer_config = TrainerConfiguration(\n",
    "    optimizer={\n",
    "        \"type\": \"adam\",\n",
    "        \"lr\": tune.loguniform(0.001, 0.01)\n",
    "    },\n",
    "    max_epochs=3,\n",
    "    monitor=\"validation_accuracy\",\n",
    "    monitor_mode=\"max\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to make sure that the model with the highest accuracy is saved in the end, that is why we specified the `monitor` and `monitor_mode` argument in the trainer configuration.\n",
    "\n",
    "::: tip Note\n",
    "\n",
    "By default we will use a GPU if available.\n",
    "If you do not want to use your GPU, just set `gpus=0` in your `TrainerConfiguration` above.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the random search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting our random hyperparameter search we first have to create a `TuneExperiment` instance with our configurations dicts and our datasets.\n",
    "We also set a name that will mainly be used as project and experiment name for the integrated WandB and MLFlow logger, respectively.\n",
    "\n",
    "Furthermore, we can pass on all the parameters available for the underlying [`tune.Experiment`](https://docs.ray.io/en/master/tune/api_docs/execution.html#tune-experiment) class:\n",
    "\n",
    "- The number of trials our search will go through depends on the `num_samples` parameter.\n",
    "In our case, a random search, it equals the number of trials, whereas in the case of a grid search the total number of trials is `num_samples` times the grid configurations (see the [Tune docs](https://docs.ray.io/en/latest/tune/api_docs/search_space.html#overview) for illustrative examples).\n",
    "\n",
    "- The `local_dir` parameter defines the output directory of the HPO results and will also contain the training results of each trial (that is the model weights and metrics).\n",
    "\n",
    "- The number of parallel running trials depends on your `resources_per_trial` configuration and your local resources.\n",
    "The default value is `{\"cpu\": 1, \"gpu\": 0}` and results, for example, in 8 parallel running trials on a machine with 8 CPUs.\n",
    "You can also use fractional values. To share a GPU between 2 trials, for example, pass on `{\"gpu\": 0.5}`. \n",
    "\n",
    "::: tip Note\n",
    "\n",
    "Keep in mind: to run your HPO on GPUs, you have to specify them in the `resources_per_trial` parameter when calling `tune.run()`.\n",
    "If you do not want to use a GPU, just set the value to 0 `{\"cpu\": 1, \"gpu\": 0}`.\n",
    "\n",
    ":::\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_random_search = TuneExperiment(\n",
    "    pipeline_config=pipeline_config,\n",
    "    trainer_config=trainer_config,\n",
    "    train_dataset=train_ds,\n",
    "    valid_dataset=valid_ds,\n",
    "    name=\"My first random search\",\n",
    "    # `tune.Experiment` kwargs:\n",
    "    num_samples=50,\n",
    "    local_dir=\"tune_runs\",\n",
    "    resources_per_trial={\"cpu\": 1, \"gpu\": 0.5},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our `TuneExperiment` object at hand, we simply have to pass it on to the [`tune.run`](https://docs.ray.io/en/master/tune/api_docs/execution.html#tune-run) function to start our random search.\n",
    "\n",
    "In this tutorial we will perform a random search together with the [Asynchronous Successive Halving Algorithm (ASHA)](https://blog.ml.cmu.edu/2018/12/12/massively-parallel-hyperparameter-optimization/) to schedule our trials.\n",
    "The Ray Tune developers advocate this `scheduler` as a good starting point for its aggressive termination of low-performing trials.\n",
    "You can look up the available configurations in the [ASHAScheduler docs](https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#asha-tune-schedulers-ashascheduler), here we will just use the default parameters.\n",
    "\n",
    "We also have to specify on what `metric` to optimize and its `mode` (should the metric be *minimized* (`min`) or *maximized* (`max`) ). \n",
    "This should be the same as the `validation_metric` specified in your trainer configuration. This guarantees the alignment of the patience mechanism and the trial scheduler, and also makes sure that the best model weights correspond to the best metrics reported by ray tune.\n",
    "\n",
    "The `progress_reporter` is a nice feature to keep track of the progress inside a Jupyter Notebook, for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: tip Tip\n",
    "\n",
    "You can reduce the size of the training output by specifying `progress_bar_refresh_rate=0` in your `TrainingConfiguration` to disable the progress bar.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = tune.run(\n",
    "    my_random_search,\n",
    "    scheduler=tune.schedulers.ASHAScheduler(), \n",
    "    metric=\"validation_accuracy\", \n",
    "    mode=\"max\",\n",
    "    progress_reporter=tune.JupyterNotebookReporter(overwrite=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: tip Tip\n",
    "\n",
    "You can also create an `Analysis` object from the output directory of the HPO run, once it has finished:\n",
    "```python\n",
    "from ray.tune.analysis.experiment_analysis import Analysis\n",
    "analysis = Analysis(\n",
    "    experiment_dir='tune_runs/My first random search\")\n",
    "    default_metric=\"validation_accuracy\",\n",
    "    default_mode=\"max\",\n",
    ")\n",
    "```\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following the progress with tensorboard (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ray Tune automatically logs its results with [TensorBoard](https://www.tensorflow.org/tensorboard/).\n",
    "We can take advantage of this and launch a TensorBoard instance before starting the hyperparameter search to follow its progress.\n",
    "The `RayTuneTrainable` class will also log the metrics to MLFlow and WandB, if you prefer those platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir ./runs/tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Screenshot of TensorBoard with Ray Tune](./img/hpo_tensorboard.png)\n",
    "*Screenshot of TensorBoard*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *analysis* object returned by `tune.run` can be accessed through a *pandas DataFrame*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Event though with 50 trials we visit just a small space of our possible configurations, we should have achieved an accuracy of ~0.94, an increase of roughly 3 points compared to the original configuration of the [base tutorial](https://recognai.github.io/biome-text/v3.2.0/documentation/tutorials/1-Training_a_text_classifier.html).\n",
    "\n",
    "In a real-life example, though, you probably should increase the number of epochs, since the validation loss in general seems to be decreasing further.\n",
    "\n",
    "A next step could be to fix some of the tuned parameters to the preferred value, and tune other parameters further or limit their value space.\n",
    "\n",
    "::: tip Tip\n",
    "\n",
    "To obtain insights about the importance and tendencies of each hyperparameter for the model, we recommend using TensorBoard's *HPARAM* section and follow Richard Liaw's suggestions at the end of his [talk](https://www.youtube.com/watch?v=VX7HvEoMrsA). \n",
    "Another very useful tool is the parallel coordinates panel in [Weights & Biases](https://wandb.ai/site), see this [quick walkthrough](https://youtu.be/91HhNtmb0B4) \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the best performing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *analysis* object also provides some convenient methods to obtain the best performing configuration, as well as the `logdir` where the results of the trial are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.get_best_config(\"validation_accuracy\", \"max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `best_logdir` to create a pipeline with the best performing model and start making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_logdir = analysis.get_best_logdir(\"validation_accuracy\", \"max\")\n",
    "best_model = os.path.join(best_logdir, \"output\", \"model.tar.gz\")\n",
    "pl_trained = Pipeline.from_pretrained(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_trained.predict(text=\"Autohaus Recognai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "::: tip Note\n",
    "\n",
    "For an unbiased evaluation of the model you should use a test dataset that was not used during the HPO!\n",
    "\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
