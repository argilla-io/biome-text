{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a sequence tagger for Slot Filling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://recognai.github.io/biome-text/v3.2.0/documentation/tutorials/2-Training_a_sequence_tagger_for_Slot_Filling.html\"><img class=\"icon\" src=\"https://recognai.github.io/biome-text/v3.2.0/assets/img/biome-isotype.svg\" width=24 /></a>\n",
    "[View on recogn.ai](https://recognai.github.io/biome-text/v3.2.0/documentation/tutorials/2-Training_a_sequence_tagger_for_Slot_Filling.html)\n",
    "    \n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/recognai/biome-text/blob/v3.2.0/docs/docs/documentation/tutorials/2-Training_a_sequence_tagger_for_Slot_Filling.ipynb\"><img class=\"icon\" src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" width=24 /></a>\n",
    "[Run in Google Colab](https://colab.research.google.com/github/recognai/biome-text/blob/v3.2.0/docs/docs/documentation/tutorials/2-Training_a_sequence_tagger_for_Slot_Filling.ipynb)\n",
    "        \n",
    "<a target=\"_blank\" href=\"https://github.com/recognai/biome-text/blob/v3.2.0/docs/docs/documentation/tutorials/2-Training_a_sequence_tagger_for_Slot_Filling.ipynb\"><img class=\"icon\" src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\" width=24 /></a>\n",
    "[View source on GitHub](https://github.com/recognai/biome-text/blob/v3.2.0/docs/docs/documentation/tutorials/2-Training_a_sequence_tagger_for_Slot_Filling.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running this tutorial in Google Colab, make sure to install *biome.text* first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pip\n",
    "!pip install -U biome-text\n",
    "exit(0)  # Force restart of the runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If* you want to log your runs with [WandB](https://wandb.ai/home), don't forget to install its client and log in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb\n",
    "!wandb login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial we will train a sequence tagger for filling slots in spoken requests.\n",
    "The goal is to look for specific pieces of information in the request and tag the corresponding tokens accordingly. \n",
    "The requests will include several intents, from getting weather information to adding a song to a playlist, each requiring its own set of slots.\n",
    "Therefore, slot filling often goes hand in hand with intent classification.\n",
    "In this tutorial, however, we will only focus on the slot filling task.\n",
    "\n",
    "Slot filling is closely related to [Named-entity recognition (NER)](https://en.wikipedia.org/wiki/Named-entity_recognition) and the model of this tutorial can also be used to train a NER system.\n",
    "\n",
    "In this tutorial we will use the [SNIPS data set](https://github.com/snipsco/nlu-benchmark/tree/master/2017-06-custom-intent-engines) adapted by [Su Zhu](https://github.com/sz128/slot_filling_and_intent_detection_of_SLU/tree/master/data/snips) and our simple [data preparation notebook](https://biome-tutorials-data.s3-eu-west-1.amazonaws.com/token_classifier/data_prep.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Let us first import all the stuff we need for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from biome.text import Pipeline, Dataset, PipelineConfiguration, VocabularyConfiguration, Trainer\n",
    "from biome.text.configuration import FeaturesConfiguration, WordFeatures, CharFeatures\n",
    "from biome.text.modules.configuration import Seq2SeqEncoderConfiguration\n",
    "from biome.text.modules.heads import TokenClassificationConfiguration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data before starting with the configuration of our pipeline.\n",
    "For this we create a `Dataset` instance providing a path to our downloaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O https://biome-tutorials-data.s3-eu-west-1.amazonaws.com/token_classifier/train.json\n",
    "!curl -O https://biome-tutorials-data.s3-eu-west-1.amazonaws.com/token_classifier/valid.json\n",
    "!curl -O https://biome-tutorials-data.s3-eu-west-1.amazonaws.com/token_classifier/test.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_json(\"train.json\")\n",
    "valid_ds = Dataset.from_json(\"valid.json\")\n",
    "test_ds = Dataset.from_json(\"test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Dataset](https://recognai.github.io/biome-text/v3.2.0/api/biome/text/dataset.html#dataset) class is a very thin wrapper around HuggingFace's awesome [datasets.Dataset](https://huggingface.co/docs/datasets/master/package_reference/main_classes.html#datasets.Dataset).\n",
    "Most of HuggingFace's `Dataset` API is exposed and you can checkout their nice [documentation](https://huggingface.co/docs/datasets/master/processing.html) on how to work with data in a `Dataset`. For example, let's quickly check the size of our training data and print the first 10 examples as a pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training data size:\", len(train_ds))\n",
    "\n",
    "train_ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we have two relevant columns for our task: *text* and *labels*. \n",
    "The *intent* column will be ignored in this tutorial. \n",
    "\n",
    "The text input already comes pre-tokenized as a list of strings and each token in the *text* column has a label/tag in the *labels* column, this means that both list always have the same length.\n",
    "The labels are given in the [BIO tagging scheme](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)), which is widely used in Slot Filling/NER systems.\n",
    "\n",
    "We can quickly check how many different labels there are in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {tag[2:] for tags in train_ds[\"labels\"] for tag in tags if tag != \"O\"}\n",
    "print(\"number of lables:\", len(labels))\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the the [TaskHead](https://recognai.github.io/biome-text/v3.2.0/api/biome/text/modules/heads/task_head.html#taskhead) of our model (the [TokenClassification](https://recognai.github.io/biome-text/v3.2.0/api/biome/text/modules/heads/token_classification.html#tokenclassification) head) expects a *text* and a *tags* column to be present in the Dataset, we need to rename the *labels* column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in [train_ds, valid_ds, test_ds]:\n",
    "     ds.rename_column_(\"labels\", \"tags\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: tip Tip\n",
    "\n",
    "The [TokenClassification](https://recognai.github.io/biome-text/v3.2.0/api/biome/text/modules/heads/token_classification.html#tokenclassification) head also supports a *entities* column instead of a *tags* column, in which case the entities have to be a list of python dictionaries with a `start`, `end` and `label` key that correspond to the char indexes and the label of the entity, respectively.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure your *biome.text* Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical [Pipeline](https://recognai.github.io/biome-text/v3.2.0/api/biome/text/pipeline.html#pipeline) consists of tokenizing the input, extracting features, applying a language encoding (optionally) and executing a task-specific head in the end.\n",
    "After training a pipeline, you can use it to make predictions\n",
    "\n",
    "A *biome.text* pipeline has the following main components:\n",
    "\n",
    "```yaml\n",
    "name: # a descriptive name of your pipeline\n",
    "\n",
    "tokenizer: # how to tokenize the input\n",
    "\n",
    "features: # input features of the model\n",
    "\n",
    "encoder: # the language encoder\n",
    "\n",
    "head: # your task configuration\n",
    "\n",
    "```\n",
    "\n",
    "See the [Configuration section](https://recognai.github.io/biome-text/v3.2.0/documentation/user-guides/2-configuration.html) for a detailed description of how these main components can be configured.\n",
    "\n",
    "In this tutorial we will create a [PipelineConfiguration](https://recognai.github.io/biome-text/v3.2.0/api/biome/text/configuration.html#pipelineconfiguration) programmatically, and use it to initialize the [Pipeline](https://recognai.github.io/biome-text/v3.2.0/api/biome/text/pipeline.html#pipeline).\n",
    "You can also create your pipelines by providing a [python dictionary](https://recognai.github.io/biome-text/v3.2.0/api/biome/text/pipeline.html#from-config) (see the text classification [tutorial](https://recognai.github.io/biome-text/v3.2.0/documentation/tutorials/1-Training_a_text_classifier.html)), a YAML [configuration file](https://recognai.github.io/biome-text/v3.2.0/api/biome/text/pipeline.html#from-yaml) or a [pretrained model](https://recognai.github.io/biome-text/v3.2.0/api/biome/text/pipeline.html#from-pretrained).\n",
    "\n",
    "A pipeline configuration is composed of several other [configuration classes](https://recognai.github.io/biome-text/v3.2.0/api/biome/text/configuration.html#biome-text-configuration), each one corresponding to one of the main components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first configure the features of our pipeline.\n",
    "For our `word` feature we will use pretrained embeddings from [fasttext](https://fasttext.cc/docs/en/english-vectors.html), and our `char` feature will use the last hidden state of a [GRU](https://en.wikipedia.org/wiki/Gated_recurrent_unit) encoder to represent a word based on its characters.\n",
    "Keep in mind that the `embedding_dim` parameter for the `word` feature must be equal to the dimensions of the pretrained embeddings!\n",
    "\n",
    "::: tip Tip\n",
    "\n",
    "If you do not provide any feature configurations, we will choose a very basic `word` feature by default.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_feature = WordFeatures(\n",
    "    embedding_dim=300,\n",
    "    weights_file=\"https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\",\n",
    ")\n",
    "\n",
    "char_feature = CharFeatures(\n",
    "    embedding_dim=32,\n",
    "    encoder={\n",
    "        \"type\": \"gru\",\n",
    "        \"bidirectional\": True,\n",
    "        \"num_layers\": 1,\n",
    "        \"hidden_size\": 32,\n",
    "    },\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "features_config = FeaturesConfiguration(\n",
    "    word=word_feature, \n",
    "    char=char_feature\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will configure our encoder that takes as input a sequence of embedded word vectors and returns a sequence of encoded word vectors.\n",
    "For this encoding we will use another larger GRU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_config = Seq2SeqEncoderConfiguration(\n",
    "    type=\"gru\",\n",
    "    bidirectional=True,\n",
    "    num_layers=1,\n",
    "    hidden_size=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final configuration belongs to our [TaskHead](https://recognai.github.io/biome-text/v3.2.0/api/biome/text/modules/heads/task_head.html#taskhead).\n",
    "It reflects the task our problem belongs to and can be easily exchanged with other types of heads keeping the same features and encoder.\n",
    "\n",
    "::: tip Tip\n",
    "\n",
    "Exchanging the heads you can easily pretrain a model on a certain task, such as [language modelling](https://recognai.github.io/biome-text/v3.2.0/api/biome/text/modules/heads/language_modelling.html#languagemodelling), and use its pretrained features and encoder for training the model on another task.\n",
    "\n",
    ":::\n",
    "\n",
    "For our task we will use a [TokenClassification](https://recognai.github.io/biome-text/v3.2.0/api/biome/text/modules/heads/token_classification.html#tokenclassification) head that allows us to tag each token individually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_config = TokenClassificationConfiguration(\n",
    "    labels=list(labels),\n",
    "    label_encoding=\"BIO\",\n",
    "    top_k=1,\n",
    "    feedforward={\n",
    "        \"num_layers\": 1,\n",
    "        \"hidden_dims\": [128],\n",
    "        \"activations\": [\"relu\"],\n",
    "        \"dropout\": [0.1],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a [PipelineConfiguration](https://recognai.github.io/biome-text/v3.2.0/api/biome/text/configuration.html#pipelineconfiguration) and finally initialize our [Pipeline](https://recognai.github.io/biome-text/v3.2.0/api/biome/text/pipeline.html#pipeline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_config = PipelineConfiguration(\n",
    "    name=\"slot_filling_tutorial\",\n",
    "    features=features_config,\n",
    "    encoder=encoder_config,\n",
    "    head=head_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = Pipeline.from_config(pipeline_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default behavior of *biome.text* is to add all tokens from the training data set to the pipeline's vocabulary. \n",
    "This is done automatically when training the pipeline for the first time.\n",
    "\n",
    "For this tutorial we get rid of the rarest words by adding the `min_count` argument and set it to 2 for the word feature vocabulary.\n",
    "Since we use pretrained word embeddings we will not only consider the training data, but also the validation data when creating the vocabulary by setting `include_valid_data=True`. \n",
    "For a complete list of available arguments see the [VocabularyConfiguration API](https://recognai.github.io/biome-text/v3.2.0/api/biome/text/configuration.html#vocabularyconfiguration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_config = VocabularyConfiguration(min_count={\"word\": 2}, include_valid_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything ready to start the training of our model:\n",
    "- training data set\n",
    "- pipeline\n",
    "\n",
    "We will use the default configuration for our [`Trainer`](https://recognai.github.io/biome-text/v3.2.0/api/biome/text/trainer.html) that has sensible values and works alright for our experiment.\n",
    "[This tutorial](https://recognai.github.io/biome-text/v3.2.0/documentation/tutorials/1-Training_a_text_classifier.html) provides more information about the `Trainer` and gives you an example how to configure it.\n",
    "\n",
    "::: tip Tip\n",
    "\n",
    "If you want to configure the trainer you can pass on a `TrainerConfiguration` instance to the `Trainer`s init. \n",
    "See the [TrainerConfiguration API](https://recognai.github.io/biome-text/v3.2.0/api/biome/text/configuration.html#trainerconfiguration) for a complete list of available configurations.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    pipeline=pl,\n",
    "    train_dataset=train_ds,\n",
    "    valid_dataset=valid_ds,\n",
    "    vocab_config=vocab_config,\n",
    "    trainer_config=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start the training we simply call the `Trainer.fit()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from estimating the generalization error with a validation data set, you might want to evaluate your model against a test data set to compare several models after optimizing their hyperparameters.\n",
    "For this you can use the `Pipeline.evaluate()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model above achieves an overall F1 score of around **0.95**, which is not bad when compared to [published results](https://nlpprogress.com/english/intent_detection_slot_filling.html) of the same data set.\n",
    "You could continue the experiment changing the encoder to an LSTM network, try out a transformer architecture or fine tune the trainer.\n",
    "But for now we will go on and make our first predictions with this trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make your first predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we trained our model we can go on to make our first predictions.\n",
    "\n",
    "We provide the input expected by our `TaskHead` to the `Pipeline.predict()` method.\n",
    "In our case it is a `TokenClassification` head that classifies a `text` input. \n",
    "\n",
    "You can either provide pretokenized tokens (list of strings) **or** a raw string to the `predict` method. In the first case, you should make sure that those tokens were tokenized the same way the training data was tokenized, in the latter case you should make sure that the pipeline uses the same tokenizer as was used for the training data.\n",
    "\n",
    "The prediction of the `TokenClassification` head will always consist of a `tags` and `entities` key. Both keys will include the `top_k` most likely tag/entity sequences for the input, where `top_k` is a parameter specified in the `TokenClassificationConfiguration` before the training.\n",
    "\n",
    "::: tip Tip\n",
    "\n",
    "We can also load the trained pipeline from the training output. This is useful in case you trained the pipeline in some earlier session, and want to continue your work with the inference steps: \n",
    "\n",
    "```python\n",
    "pl = Pipeline.from_pretrained(\"output/model.tar.gz\")`\n",
    "```\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pretokenized input \n",
    "\n",
    "For pretokenized input, the `entities` key of the output holds dictionaries with the `start_token` id, `end_token` id and the label of the entity: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbreg": {
     "diff_ignore": [
      "/outputs/*"
     ]
    }
   },
   "outputs": [],
   "source": [
    "text = \"Can you play biome text by backstreet recognais on Spotify ?\".split()\n",
    "prediction = pl.predict(text=text)\n",
    "\n",
    "print(\"Predicted tags:\\n\", list(zip(text, prediction[\"tags\"][0])))\n",
    "print(\"Predicted entities:\\n\", prediction[\"entities\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### string input\n",
    "\n",
    "For a raw string input, the `entities` key of the output holds dictionaries with the `start_token` id, `end_token` id, `start` char id, `end` char id and the label of the entity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Can you play biome text by backstreet recognais on Spotify ?\"\n",
    "prediction = pl.predict(text=text)\n",
    "\n",
    "print(\"Predicted tags:\\n\", list(zip(text.split(), prediction[\"tags\"][0])))\n",
    "print(\"Predicted entities:\\n\", prediction[\"entities\"][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "nbreg": {
   "diff_ignore": [
    "/metadata/widgets",
    "/metadata/language_info",
    "/cells/*/execution_count"
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
