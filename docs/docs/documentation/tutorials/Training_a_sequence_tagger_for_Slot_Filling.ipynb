{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a sequence tagger for Slot Filling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.recogn.ai/biome-text/documentation/tutorials/Training_a_sequence_tagger_for_Slot_Filling.html\"><img src=\"https://www.recogn.ai/biome-text/assets/img/biome-isotype.svg\" width=32 />View on recogn.ai</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/recognai/biome-text/blob/master/docs/docs/documentation/tutorials/Training_a_sequence_tagger_for_Slot_Filling.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/recognai/biome-text/blob/master/docs/docs/documentation/tutorials/Training_a_sequence_tagger_for_Slot_Filling.ipynb\"><img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\" width=32 />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will train a sequence tagger for filling slots in spoken requests.\n",
    "The goal is to look for specific pieces of information in the request and tag the corresponding tokens accordingly. \n",
    "The requests will include several intents, from getting weather information to adding a song to a playlist, each requiring its own set of slots.\n",
    "Therefore, slot filling often goes hand in hand with intent classification.\n",
    "In this tutorial, however, we will only focus on the slot filling task.\n",
    "\n",
    "Slot filling is closely related to [Named-entity recognition (NER)](https://en.wikipedia.org/wiki/Named-entity_recognition) and the model of this tutorial can also be used to train a NER system.\n",
    "\n",
    "In this tutorial we will use the [SNIPS data set](https://github.com/snipsco/nlu-benchmark/tree/master/2017-06-custom-intent-engines) adapted by [Su Zhu](https://github.com/sz128/slot_filling_and_intent_detection_of_SLU/tree/master/data/snips) and our simple [data preparation notebook](https://biome-tutorials-data.s3-eu-west-1.amazonaws.com/token_classifier/data_prep.ipynb).\n",
    "\n",
    "When running this tutorial in Google Colab, make sure to install *biome.text* first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U git+https://github.com/recognai/biome-text.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore warnings and don't forget to restart your runtime afterwards (*Runtime -> Restart runtime*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data before starting with the configuration of our pipeline.\n",
    "For this we create a `DataSource` instance providing a path to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from biome.text.data import DataSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>intent</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Find, the, schedule, for, Across, the, Line, ...</td>\n",
       "      <td>[O, O, B-object_type, O, B-movie_name, I-movie...</td>\n",
       "      <td>SearchScreeningEvent</td>\n",
       "      <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[play, Party, Ben, on, Slacker]</td>\n",
       "      <td>[O, B-artist, I-artist, O, B-service]</td>\n",
       "      <td>PlayMusic</td>\n",
       "      <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[play, a, 1988, soundtrack]</td>\n",
       "      <td>[O, O, B-year, B-music_item]</td>\n",
       "      <td>PlayMusic</td>\n",
       "      <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Can, you, play, The, Change, Is, Made, on, Ne...</td>\n",
       "      <td>[O, O, O, B-track, I-track, I-track, I-track, ...</td>\n",
       "      <td>PlayMusic</td>\n",
       "      <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[what, is, the, forecast, for, colder, in, Ans...</td>\n",
       "      <td>[O, O, O, O, O, B-condition_temperature, O, B-...</td>\n",
       "      <td>GetWeather</td>\n",
       "      <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[What's, the, weather, in, Totowa, WY, one, mi...</td>\n",
       "      <td>[O, O, O, O, B-city, B-state, B-timeRange, I-t...</td>\n",
       "      <td>GetWeather</td>\n",
       "      <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[Play, a, tune, from, Space, Mandino, .]</td>\n",
       "      <td>[O, O, B-music_item, O, B-artist, I-artist, O]</td>\n",
       "      <td>PlayMusic</td>\n",
       "      <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[give, five, out, of, 6, stars, to, current, e...</td>\n",
       "      <td>[O, B-rating_value, O, O, B-best_rating, B-rat...</td>\n",
       "      <td>RateBook</td>\n",
       "      <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[Play, some, chanson, style, music.]</td>\n",
       "      <td>[O, O, B-genre, O, O]</td>\n",
       "      <td>PlayMusic</td>\n",
       "      <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[I, would, give, French, Poets, and, Novelists...</td>\n",
       "      <td>[O, O, O, B-object_name, I-object_name, I-obje...</td>\n",
       "      <td>RateBook</td>\n",
       "      <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  [Find, the, schedule, for, Across, the, Line, ...   \n",
       "1                    [play, Party, Ben, on, Slacker]   \n",
       "2                        [play, a, 1988, soundtrack]   \n",
       "3  [Can, you, play, The, Change, Is, Made, on, Ne...   \n",
       "4  [what, is, the, forecast, for, colder, in, Ans...   \n",
       "5  [What's, the, weather, in, Totowa, WY, one, mi...   \n",
       "6           [Play, a, tune, from, Space, Mandino, .]   \n",
       "7  [give, five, out, of, 6, stars, to, current, e...   \n",
       "8               [Play, some, chanson, style, music.]   \n",
       "9  [I, would, give, French, Poets, and, Novelists...   \n",
       "\n",
       "                                              labels                 intent  \\\n",
       "0  [O, O, B-object_type, O, B-movie_name, I-movie...   SearchScreeningEvent   \n",
       "1              [O, B-artist, I-artist, O, B-service]              PlayMusic   \n",
       "2                       [O, O, B-year, B-music_item]              PlayMusic   \n",
       "3  [O, O, O, B-track, I-track, I-track, I-track, ...              PlayMusic   \n",
       "4  [O, O, O, O, O, B-condition_temperature, O, B-...             GetWeather   \n",
       "5  [O, O, O, O, B-city, B-state, B-timeRange, I-t...             GetWeather   \n",
       "6     [O, O, B-music_item, O, B-artist, I-artist, O]              PlayMusic   \n",
       "7  [O, B-rating_value, O, O, B-best_rating, B-rat...               RateBook   \n",
       "8                              [O, O, B-genre, O, O]              PlayMusic   \n",
       "9  [O, O, O, B-object_name, I-object_name, I-obje...               RateBook   \n",
       "\n",
       "                                                path  \n",
       "0  https://biome-tutorials-data.s3-eu-west-1.amaz...  \n",
       "1  https://biome-tutorials-data.s3-eu-west-1.amaz...  \n",
       "2  https://biome-tutorials-data.s3-eu-west-1.amaz...  \n",
       "3  https://biome-tutorials-data.s3-eu-west-1.amaz...  \n",
       "4  https://biome-tutorials-data.s3-eu-west-1.amaz...  \n",
       "5  https://biome-tutorials-data.s3-eu-west-1.amaz...  \n",
       "6  https://biome-tutorials-data.s3-eu-west-1.amaz...  \n",
       "7  https://biome-tutorials-data.s3-eu-west-1.amaz...  \n",
       "8  https://biome-tutorials-data.s3-eu-west-1.amaz...  \n",
       "9  https://biome-tutorials-data.s3-eu-west-1.amaz...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = DataSource(source=\"https://biome-tutorials-data.s3-eu-west-1.amazonaws.com/token_classifier/train.json\")\n",
    "train_ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we have two relevant columns for our task: *text* and *labels*. \n",
    "The *intent* column will be ignored in this tutorial. \n",
    "The *path* column is added automatically by the [DataSource](../../api/biome/text/data/datasource.html#datasource) class to keep track of the source file.\n",
    "\n",
    "The input already comes pre-tokenized and each token in the *text* column has a label/tag in the *labels* column, this means that both list always have the same length.\n",
    "The labels are given in the [BIO tagging scheme](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)), which is widely used in Slot Filling/NER systems.\n",
    "\n",
    "When specifying the [TokenClassification](../../api/biome/text/modules/heads/token_classification.html#tokenclassification) head (see [below](#Configure-your-biome.text-Pipeline)), the tokenization step in the pipeline is automatically disabled and the input is expected to be a list of tokens.\n",
    "\n",
    "The [DataSource](../../api/biome/text/data/datasource.html#datasource) class stores the data in an underlying [Dask DataFrame](https://docs.dask.org/en/latest/dataframe.html) that you can easily access.\n",
    "For example, let's check the size of our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13084"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds.to_dataframe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or let's check how many different labels/tags we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = train_ds.to_dataframe().compute()\n",
    "labels_total = df.labels.sum()\n",
    "len(set(labels_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and how they are distributed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbreg": {
     "diff_ignore": [
      "/outputs/*"
     ]
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O                               59610\n",
       "I-object_name                    7400\n",
       "I-playlist                       3230\n",
       "B-object_type                    3023\n",
       "B-object_name                    2778\n",
       "                                ...  \n",
       "I-cuisine                          28\n",
       "I-facility                         14\n",
       "I-object_select                     3\n",
       "I-object_part_of_series_type        3\n",
       "I-playlist_owner                    1\n",
       "Length: 72, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.Series(labels_total).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: tip\n",
    "\n",
    "The [TaskHead](../../api/biome/text/modules/heads/task_head.html#taskhead) of our model (the [TokenClassification](../../api/biome/text/modules/heads/token_classification.html#tokenclassification)) will expect a *text* and a *labels* column to be present in the dataframe. \n",
    "Since they are already present, there is no need for a `mapping` in the [DataSource](../../api/biome/text/data/datasource.html#datasource).\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure your *biome.text* Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical [Pipeline](../../api/biome/text/pipeline.html#pipeline) consists of tokenizing the input, extracting features, applying a language encoding (optionally) and executing a task-specific head in the end.\n",
    "\n",
    "After training a pipeline, you can use it to make predictions or explore the underlying model via the [explore UI](../../documentation/user-guides/02.explore.html).\n",
    "\n",
    "As a first step we must define a configuration for our pipeline. \n",
    "In this tutorial we will create a configuration dictionary and use the `Pipeline.from_config()` method to create our pipeline, but there are [other ways](../../api/biome/text/pipeline.html#pipeline).\n",
    "\n",
    "A *biome.text* pipeline has the following main components:\n",
    "\n",
    "```yaml\n",
    "name: # a descriptive name of your pipeline\n",
    "\n",
    "tokenizer: # how to tokenize the input\n",
    "\n",
    "features: # input features of the model\n",
    "\n",
    "encoder: # the language encoder\n",
    "\n",
    "head: # your task configuration\n",
    "\n",
    "```\n",
    "\n",
    "See the [Configuration section](../../documentation/user-guides/05.configuration.html) for a detailed description of how these main components can be configured.\n",
    "\n",
    "Our complete configuration for this tutorial will be following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_dict = {\n",
    "    \"name\": \"slot_filling\",\n",
    "    \"features\": {\n",
    "        \"word\": {\n",
    "            \"embedding_dim\": 300,\n",
    "            \"weights_file\": \"https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\",\n",
    "        },\n",
    "        \"char\": {\n",
    "            \"embedding_dim\": 32,\n",
    "            \"encoder\": {\n",
    "                \"type\": \"gru\",\n",
    "                \"bidirectional\": True,\n",
    "                \"num_layers\": 1,\n",
    "                \"hidden_size\": 32,\n",
    "            },\n",
    "            \"dropout\": 0.1,\n",
    "        },\n",
    "    },\n",
    "    \"encoder\": {\n",
    "        \"type\": \"gru\",\n",
    "        \"hidden_size\": 128,\n",
    "        \"num_layers\": 1,\n",
    "        \"bidirectional\": True,\n",
    "    },\n",
    "    \"head\": {\n",
    "        \"type\": \"TokenClassification\",\n",
    "        \"labels\": list(set(labels_total)),\n",
    "        \"label_encoding\": \"BIO\",\n",
    "        \"feedforward\": {\n",
    "            \"num_layers\": 1,\n",
    "            \"hidden_dims\": [128],\n",
    "            \"activations\": [\"relu\"],\n",
    "            \"dropout\": [0.1],\n",
    "        },\n",
    "        \"dropout\": 0.1,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the `features` section, we will use pretrained word embeddings from [fasttext](https://fasttext.cc/docs/en/english-vectors.html) for our `word` features.\n",
    "Keep in mind that your `embedding_dim` parameter must be equal to the dimensions of those pretrained embeddings.\n",
    "\n",
    "With the dictionary above we can now create a `Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from biome.text import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = Pipeline.from_config(pipeline_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a vocabulary\n",
    "\n",
    "Before we can start the training we need to create the vocabulary for our model.\n",
    "For this we define a `VocabularyConfiguration`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from biome.text import VocabularyConfiguration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we use pretrained word embeddings we will also consider the validation data when creating the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ds = DataSource(source=\"https://biome-tutorials-data.s3-eu-west-1.amazonaws.com/token_classifier/valid.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also get rid of the rarest words by adding the `min_count` argument and set it to 2 for the word feature vocabulary.\n",
    "For a complete list of available arguments see the [VocabularyConfiguration API](../../api/biome/text/configuration.html#vocabularyconfiguration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_config = VocabularyConfiguration(\n",
    "    sources=[train_ds, valid_ds],\n",
    "    min_count={\"word\": 2},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then pass this configuration to our `Pipeline` to create the vocabulary.\n",
    "Apart from the loading bar of building the vocabulary, there will be two more loading bars corresponding to the `weights_file` provided in the word feature:\n",
    " - the progress of downloading the file (this file will be cached)\n",
    " - the progress loading the weights from the file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "nbreg": {
     "diff_ignore": [
      "/outputs/*"
     ]
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd917fff2cda4f1e8e8ccd063d95457c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3823d97e11145ea9b688c2c467ce2bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=999994.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pl.create_vocabulary(vocab_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the vovocab_configbulary we can check the size of our entire model in terms of trainable parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1989112"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.trainable_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the trainer\n",
    "\n",
    "As a next step we have to configure the *trainer*.\n",
    "\n",
    "For this tutorial we will use the default configuration that has sensible values and works alright for our experiment.\n",
    "You can have a look at the [TrainerConfiguration API](../../api/biome/text/configuration.html#trainerconfiguration) for a complete list of available configurations.\n",
    "\n",
    "::: tip\n",
    "\n",
    "In case you have a cuda device available, you also specify it here.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from biome.text import TrainerConfiguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = TrainerConfiguration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your model\n",
    "\n",
    "Now we have everything ready to start the training of our model:\n",
    "- training data set\n",
    "- vocabulary\n",
    "- trainer\n",
    "\n",
    "Apart from the validation data source to estimate the generalization error, we will also pass in a test data set in case we want to do some Hyperparameter optimization and compare different encoder architectures in the end. \n",
    "For this we will create another `DataSource` pointing to our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = DataSource(source=\"https://biome-tutorials-data.s3-eu-west-1.amazonaws.com/token_classifier/test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training output will be saved in a folder specified by the `output` argument of the `train` method. \n",
    "It will contain the trained model weights and the metrics, as well as the vocabulary and a *log* folder for visualizing the training process with [tensorboard](https://www.tensorflow.org/tensorboard/).\n",
    "\n",
    "When the training has finished it will automatically make a pass over the test data with the best weights to gather the test metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbreg": {
     "diff_ignore": [
      "/outputs/*"
     ]
    }
   },
   "outputs": [],
   "source": [
    "pl.train(\n",
    "    output=\"output\",\n",
    "    training=train_ds,\n",
    "    validation=valid_ds,\n",
    "    test=test_ds,\n",
    "    trainer=trainer_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model above achieves an overall F1 score of around **0.95**, which is not bad when compared to [published results](https://nlpprogress.com/english/intent_detection_slot_filling.html) of the same data set.\n",
    "You could continue the experiment changing the encoder to an LSTM network, try out a transformer architecture or fine tune the trainer.\n",
    "But for now we will go on and make our first predictions with this trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make your first predictions\n",
    "\n",
    "Now that we trained our model we can go on to make our first predictions.\n",
    "First we must load our trained model into a new `Pipeline`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_trained = Pipeline.from_pretrained(\"output/model.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then provide the input expected by our `TaskHead` of the model to the `Pipeline.predict()` method.\n",
    "In our case it is a `TokenClassification` head that classifies a `text` input. **Remember that the input has to be pre-tokenized!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "nbreg": {
     "diff_ignore": [
      "/outputs/*"
     ]
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('can', 'O'),\n",
       " ('you', 'O'),\n",
       " ('play', 'O'),\n",
       " ('biome', 'B-track'),\n",
       " ('text', 'I-track'),\n",
       " ('by', 'O'),\n",
       " ('backstreet', 'B-artist'),\n",
       " ('recognais', 'I-artist'),\n",
       " ('on', 'O'),\n",
       " ('Spotify', 'B-service')]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"can you play biome text by backstreet recognais on Spotify\".split()\n",
    "prediction = pl_trained.predict(text)\n",
    "list(zip(text, prediction[\"tags\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from the most likely *tags*, the `prediction` dictionary contains the *logits* and *probs* of each of the label for each of the input token."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "nbreg": {
   "diff_ignore": [
    "/metadata/widgets",
    "/metadata/language_info",
    "/cells/*/execution_count"
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
