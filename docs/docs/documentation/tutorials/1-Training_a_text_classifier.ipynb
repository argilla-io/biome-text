{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a short text classifier of German business names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://www.recogn.ai/biome-text/master/documentation/tutorials/1-Training_a_text_classifier.html\"><img class=\"icon\" src=\"https://www.recogn.ai/biome-text/master/assets/img/biome-isotype.svg\" width=24 /></a>\n",
    "[View on recogn.ai](https://www.recogn.ai/biome-text/master/documentation/tutorials/1-Training_a_text_classifier.html)\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/recognai/biome-text/blob/master/docs/docs/documentation/tutorials/1-Training_a_text_classifier.ipynb\"><img class=\"icon\" src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" width=24 /></a>\n",
    "[Run in Google Colab](https://colab.research.google.com/github/recognai/biome-text/blob/master/docs/docs/documentation/tutorials/1-Training_a_text_classifier.ipynb)\n",
    "\n",
    "<a target=\"_blank\" href=\"https://github.com/recognai/biome-text/blob/master/docs/docs/documentation/tutorials/1-Training_a_text_classifier.ipynb\"><img class=\"icon\" src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\" width=24 /></a>\n",
    "[View source on GitHub](https://github.com/recognai/biome-text/blob/master/docs/docs/documentation/tutorials/1-Training_a_text_classifier.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running this tutorial in Google Colab, make sure to install *biome.text* first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pip\n",
    "!pip install -U biome-text\n",
    "exit(0)  # Force restart of the runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If* you want to log your runs with [WandB](https://wandb.ai/home), don't forget to install its client and log in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb\n",
    "!wandb login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial we will train a basic short-text classifier for predicting the sector of a business based only on its business name. \n",
    "For this we will use a training data set with business names and business categories in German."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Let us first import all the stuff we need for this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from biome.text import Pipeline, Dataset\n",
    "from biome.text.configuration import VocabularyConfiguration, WordFeatures, TrainerConfiguration\n",
    "from biome.text import explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the training data\n",
    "\n",
    "Let's take a look at the data we will use for training. For this we will use the [`Dataset`](https://www.recogn.ai/biome-text/master/api/biome/text/dataset.html#dataset) class that is a very thin wrapper around HuggingFace's awesome [datasets.Dataset](https://huggingface.co/docs/datasets/master/package_reference/main_classes.html#datasets.Dataset). \n",
    "We will download the data first to create `Dataset` instances.\n",
    "\n",
    "Apart from the training data we will also download an optional validation data set to estimate the generalization error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the dataset first\n",
    "!curl -O https://biome-tutorials-data.s3-eu-west-1.amazonaws.com/text_classifier/business.cat.train.csv\n",
    "!curl -O https://biome-tutorials-data.s3-eu-west-1.amazonaws.com/text_classifier/business.cat.valid.csv\n",
    "\n",
    "# Loading from local\n",
    "train_ds = Dataset.from_csv(\"business.cat.train.csv\")\n",
    "valid_ds = Dataset.from_csv(\"business.cat.valid.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of HuggingFace's `Dataset` API is exposed and you can checkout their nice [documentation](https://huggingface.co/docs/datasets/master/processing.html) on how to work with data in a `Dataset`. For example, let's quickly check the size of our training data and print the first 10 examples as a pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Edv</td>\n",
       "      <td>Cse Gmbh Computer Edv-service Bürobedarf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maler</td>\n",
       "      <td>Malerfachbetrieb U. Nee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gebrauchtwagen</td>\n",
       "      <td>Sippl Automobilverkäufer Hausmann</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Handelsvermittler Und -vertreter</td>\n",
       "      <td>Strenge Handelsagentur Werth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gebrauchtwagen</td>\n",
       "      <td>Dzengel Autohaus Gordemitz Rusch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Apotheken</td>\n",
       "      <td>Schinkel-apotheke Bitzer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tiefbau</td>\n",
       "      <td>Franz Möbius Mehrings-bau-hude Und Stigge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Handelsvermittler Und -vertreter</td>\n",
       "      <td>Kontze Hdl.vertr. Lau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Autowerkstätten</td>\n",
       "      <td>Keßler Kfz-handel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Gebrauchtwagen</td>\n",
       "      <td>Diko Lack Und Schrift Betriebsteil Der Autocen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              label  \\\n",
       "0                               Edv   \n",
       "1                             Maler   \n",
       "2                    Gebrauchtwagen   \n",
       "3  Handelsvermittler Und -vertreter   \n",
       "4                    Gebrauchtwagen   \n",
       "5                         Apotheken   \n",
       "6                           Tiefbau   \n",
       "7  Handelsvermittler Und -vertreter   \n",
       "8                   Autowerkstätten   \n",
       "9                    Gebrauchtwagen   \n",
       "\n",
       "                                                text  \n",
       "0           Cse Gmbh Computer Edv-service Bürobedarf  \n",
       "1                            Malerfachbetrieb U. Nee  \n",
       "2                  Sippl Automobilverkäufer Hausmann  \n",
       "3                       Strenge Handelsagentur Werth  \n",
       "4                   Dzengel Autohaus Gordemitz Rusch  \n",
       "5                           Schinkel-apotheke Bitzer  \n",
       "6          Franz Möbius Mehrings-bau-hude Und Stigge  \n",
       "7                              Kontze Hdl.vertr. Lau  \n",
       "8                                  Keßler Kfz-handel  \n",
       "9  Diko Lack Und Schrift Betriebsteil Der Autocen...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we have two relevant columns *label* and *text*. Our classifier will be trained to predict the *label* given the *text*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: tip Tip\n",
    "\n",
    "The [TaskHead](https://www.recogn.ai/biome-text/master/api/biome/text/modules/heads/task_head.html#taskhead) of our model below will expect a *text* and a *label* column to be present in the `Dataset`. In our data set this is already the case, otherwise we would need to change or map the corresponding column names via `Dataset.rename_column_()` or `Dataset.map()`.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also quickly check the distribution of our labels. Use `Dataset.head(None)` to return the complete data set as a pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unternehmensberatungen              632\n",
       "Friseure                            564\n",
       "Tiefbau                             508\n",
       "Dienstleistungen                    503\n",
       "Gebrauchtwagen                      449\n",
       "Elektriker                          430\n",
       "Restaurants                         422\n",
       "Architekturbüros                    417\n",
       "Vereine                             384\n",
       "Versicherungsvermittler             358\n",
       "Maler                               330\n",
       "Sanitärinstallationen               323\n",
       "Edv                                 318\n",
       "Werbeagenturen                      294\n",
       "Apotheken                           289\n",
       "Physiotherapie                      286\n",
       "Vermittlungen                       277\n",
       "Hotels                              274\n",
       "Autowerkstätten                     263\n",
       "Elektrotechnik                      261\n",
       "Allgemeinärzte                      216\n",
       "Handelsvermittler Und -vertreter    202\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.head(None)[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Dataset` class also provides access to Hugging Face's extensive NLP datasets collection via the `Dataset.load_dataset()` method. Have a look at their [quicktour](https://huggingface.co/docs/datasets/master/quicktour.html) for more details about their awesome library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure your *biome.text* Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical [Pipeline](https://www.recogn.ai/biome-text/master/api/biome/text/pipeline.html#pipeline) consists of tokenizing the input, extracting features, applying a language encoding (optionally) and executing a task-specific head in the end.\n",
    "\n",
    "After training a pipeline, you can use it to make predictions or explore the underlying model via the explore UI.\n",
    "\n",
    "As a first step we must define a configuration for our pipeline. \n",
    "In this tutorial we will create a configuration dictionary and use the `Pipeline.from_config()` method to create our pipeline, but there are [other ways](https://www.recogn.ai/biome-text/master/api/biome/text/pipeline.html#pipeline).\n",
    "\n",
    "A *biome.text* pipeline has the following main components:\n",
    "\n",
    "```yaml\n",
    "name: # a descriptive name of your pipeline\n",
    "\n",
    "tokenizer: # how to tokenize the input\n",
    "\n",
    "features: # input features of the model\n",
    "\n",
    "encoder: # the language encoder\n",
    "\n",
    "head: # your task configuration\n",
    "\n",
    "```\n",
    "\n",
    "See the [Configuration section](https://www.recogn.ai/biome-text/master/documentation/user-guides/2-configuration.html) for a detailed description of how these main components can be configured.\n",
    "\n",
    "Our complete configuration for this tutorial will be following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_dict = {\n",
    "    \"name\": \"german_business_names\",\n",
    "    \"tokenizer\": {\n",
    "        \"text_cleaning\": {\n",
    "            \"rules\": [\"strip_spaces\"]\n",
    "        }\n",
    "    },\n",
    "    \"features\": {\n",
    "        \"word\": {\n",
    "            \"embedding_dim\": 64,\n",
    "            \"lowercase_tokens\": True,\n",
    "        },\n",
    "        \"char\": {\n",
    "            \"embedding_dim\": 32,\n",
    "            \"lowercase_characters\": True,\n",
    "            \"encoder\": {\n",
    "                \"type\": \"gru\",\n",
    "                \"num_layers\": 1,\n",
    "                \"hidden_size\": 32,\n",
    "                \"bidirectional\": True,\n",
    "            },\n",
    "            \"dropout\": 0.1,\n",
    "        },\n",
    "    },\n",
    "    \"head\": {\n",
    "        \"type\": \"TextClassification\",\n",
    "        \"labels\": train_ds.unique(\"label\"),\n",
    "        \"pooler\": {\n",
    "            \"type\": \"gru\",\n",
    "            \"num_layers\": 1,\n",
    "            \"hidden_size\": 32,\n",
    "            \"bidirectional\": True,\n",
    "        },\n",
    "        \"feedforward\": {\n",
    "            \"num_layers\": 1,\n",
    "            \"hidden_dims\": [32],\n",
    "            \"activations\": [\"relu\"],\n",
    "            \"dropout\": [0.0],\n",
    "        },\n",
    "    },       \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this dictionary we can now create a `Pipeline`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = Pipeline.from_config(pipeline_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the vocabulary\n",
    "\n",
    "The default behavior of *biome.text* is to add all tokens from the training data set to the pipeline's vocabulary. \n",
    "This is done automatically when training the pipeline for the first time.\n",
    "\n",
    "If you want to have more control over this step, you can define a `VocabularyConfiguration` and pass it to the `Pipeline.train()` method later on.\n",
    "In our business name classifier we only want to include words with a general meaning to our word feature vocabulary (like \"Computer\" or \"Autohaus\", for example), and want to exclude specific names that will not help to generally classify the kind of business.\n",
    "This can be achieved by including only the most frequent words in our training set via the `min_count` argument. For a complete list of available arguments see the [VocabularyConfiguration API](https://www.recogn.ai/biome-text/master/api/biome/text/configuration.html#vocabularyconfiguration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_config = VocabularyConfiguration(datasets=[train_ds], min_count={WordFeatures.namespace: 20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the trainer\n",
    "\n",
    "As a next step we have to configure the *trainer*.\n",
    "\n",
    "The default trainer has sensible defaults and should work alright for most of your cases.\n",
    "In this tutorial, however, we want to tune a bit the learning rate and limit the training time to three epochs only.\n",
    "For a complete list of available arguments see the [TrainerConfiguration API](https://www.recogn.ai/biome-text/master/api/biome/text/configuration.html#trainerconfiguration).\n",
    "\n",
    "::: tip Tip\n",
    "\n",
    "By default we will use a CUDA device if one is available. If you have several devices or prefer not to use it, you can specify it here via the `cuda_device` argument.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = TrainerConfiguration(\n",
    "    optimizer={\n",
    "        \"type\": \"adam\",\n",
    "        \"lr\": 0.01,\n",
    "    },\n",
    "    num_epochs=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your model\n",
    "\n",
    "Now we have everything ready to start the training of our model:\n",
    "- training data set\n",
    "- pipeline\n",
    "- trainer\n",
    "\n",
    "The training output will be saved in a folder specified by the `output` argument. It contains the trained model weights and the metrics, as well as the vocabulary and a *log* folder for visualizing the training process with [tensorboard](https://www.tensorflow.org/tensorboard/). \n",
    "The `Pipeline.train()` method outputs a `TrainingResults` data class that contains the\n",
    "- `model_path`: file path of the serialized trained model\n",
    "- `metrics`: metrics of the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbreg": {
     "diff_ignore": [
      "/outputs/*"
     ]
    }
   },
   "outputs": [],
   "source": [
    "pl.train(\n",
    "    output=\"output\",\n",
    "    training=train_ds,\n",
    "    validation=valid_ds,\n",
    "    trainer=trainer_config,\n",
    "    vocab_config=vocab_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 3 epochs we achieve a validation accuracy of about 0.91.\n",
    "The validation loss seems to be decreasing further, though, so we could probably train the model for a few more epochs without overfitting the training data.\n",
    "\n",
    "::: tip Tip\n",
    "\n",
    "If for some reason the training gets interrupted, you can continue where you left off by setting the `restore` argument in the `Pipeline.train()` method to `True`. \n",
    "If you want to train your model for a few more epochs, you can also use the `restore` argument, but you have to modify the `epochs` argument in your `TrainerConfiguration` to reflect the total amount of epochs you aim for.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make your first predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we trained our model we can go on to make our first predictions. \n",
    "We provide the input expected by our `TaskHead` of the model to the `Pipeline.predict()` method.\n",
    "In our case it is a `TextClassification` head that classifies a `text` input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbreg": {
     "diff_ignore": [
      "/outputs/*"
     ]
    }
   },
   "outputs": [],
   "source": [
    "pl.predict(text=\"Autohaus biome.text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the `Pipeline.predict()` method is a dictionary with a `labels` and `probabilities` key containing a list of labels and their corresponding probabilities, ordered from most to less likely. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: tip Tip\n",
    "\n",
    "When configuring the pipeline in the first place, we recommend to check that it is correctly setup by using the `predict` method.\n",
    "Since the pipeline is still not trained at that moment, the predictions will be arbitrary.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also load the trained pipeline from the training output. This is useful in case you trained the pipeline in some earlier session, and want to continue your work with the inference steps: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_trained = Pipeline.from_pretrained(\"output/model.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the model's predictions\n",
    "\n",
    "To check and understand the predictions of the model, we can use the *biome.text explore UI*.\n",
    "Just calling the [explore.create](https://www.recogn.ai/biome-text/master/api/biome/text/explore.html#create) method will open the UI in the output of our cell.\n",
    "We will set the `explain` argument to true, which automatically visualizes the attribution of each token by means of [integrated gradients](https://arxiv.org/abs/1703.01365).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: warning Warning\n",
    "\n",
    "For the UI to work you need a running [Elasticsearch](https://www.elastic.co/guide/en/elasticsearch/reference/current/install-elasticsearch.html) instance.\n",
    "We recommend installing [Elasticsearch with docker](https://www.elastic.co/guide/en/elasticsearch/reference/7.7/docker.html#docker-cli-run-dev-mode).\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbreg": {
     "diff_ignore": [
      "/outputs/*"
     ]
    }
   },
   "outputs": [],
   "source": [
    "explore.create(pl_trained, valid_ds, explain=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Screenshot of the biome.text explore UI](./img/text_classifier_explore_screenshot.png)\n",
    "*Screenshot of the biome.text explore UI*\n",
    "\n",
    "Exploring our model we could take advantage of the F1 scores of each label to figure out which labels to prioritize when gathering new training data.\n",
    "For example, although \"Allgemeinärzte\" is the second rarest label in our training data, it still seems relatively easy to classify for our model due to the distinctive words \"Dr.\" and \"Allgemeinmedizin\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "nbreg": {
   "diff_ignore": [
    "/metadata/widgets",
    "/metadata/language_info",
    "/cells/*/execution_count"
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
