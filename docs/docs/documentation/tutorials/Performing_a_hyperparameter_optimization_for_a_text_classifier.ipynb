{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing a hyperparameter optimization for a text classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://www.recogn.ai/biome-text/documentation/tutorials/Performing_a_hyperparameter_optimization_for_a_text_classifier.html\"><img class=\"icon\" src=\"https://www.recogn.ai/biome-text/assets/img/biome-isotype.svg\" width=24 /></a>\n",
    "[View on recogn.ai](https://www.recogn.ai/biome-text/documentation/tutorials/Performing_a_hyperparameter_optimization_for_a_text_classifier.html)\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/recognai/biome-text/blob/master/docs/docs/documentation/tutorials/Performing_a_hyperparameter_optimization_for_a_text_classifier.ipynb\"><img class=\"icon\" src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" width=24 /></a>\n",
    "[Run in Google Colab](https://colab.research.google.com/github/recognai/biome-text/blob/master/docs/docs/documentation/tutorials/Performing_a_hyperparameter_optimization_for_a_text_classifier.ipynb)\n",
    "\n",
    "<a target=\"_blank\" href=\"https://github.com/recognai/biome-text/blob/master/docs/docs/documentation/tutorials/Performing_a_hyperparameter_optimization_for_a_text_classifier.ipynb\"><img class=\"icon\" src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\" width=24 /></a>\n",
    "[View source on GitHub](https://github.com/recognai/biome-text/blob/master/docs/docs/documentation/tutorials/Performing_a_hyperparameter_optimization_for_a_text_classifier.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will optimize the hyperparameters of the short-text classifier from [this tutorial](./Training_a_text_classifier.html).\n",
    "We recommend to have a look at it first before going through the following tutorial.\n",
    "For the Hyper-Parameter Optimization (HPO) we rely on the awesome [ray tune library](https://docs.ray.io/en/latest/tune.html#tune-index) that is **not** a dependency of *biome.text* and has to be installed additionally.\n",
    "\n",
    "For a short introduction to HPO with Ray Tune you can have a look at this nice [talk](https://www.youtube.com/watch?v=VX7HvEoMrsA) by Richard Liaw. \n",
    "We will follow his terminology and use the name *trial* to refer to the training run of one set of hyperparameters. \n",
    "\n",
    "When running this tutorial in Google Colab, make sure to install *biome.text* and *ray tune* first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U git+https://github.com/recognai/biome-text.git ray[tune]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore warnings and don't forget to restart your runtime afterwards (*Runtime -> Restart runtime*).\n",
    "\n",
    "::: tip Note\n",
    "\n",
    "In this tutorial we will use a GPU by default.\n",
    "So when running this tutorial in Google Colab, make sure that you request one (*Edit -> Notebook settings*).\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data and create the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step we will download the training and validation data to our local machine. \n",
    "This will save us some time in the long run, since we will perform the hyperparameter search on our local machine and frequently access the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O https://biome-tutorials-data.s3-eu-west-1.amazonaws.com/text_classifier/business.cat.train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O https://biome-tutorials-data.s3-eu-west-1.amazonaws.com/text_classifier/business.cat.valid.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will store the absolute path of the data to use them later on when creating our `DataSource`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.abspath(\"business.cat.train.csv\")\n",
    "valid_path = os.path.abspath(\"business.cat.valid.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reuse the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be more efficient and speed things up, we will create the vocabulary beforehand and reuse it in every trial.\n",
    "For this we have to create a `Pipeline` first, create the vocabulary from our `DataSource` and save it to a folder.\n",
    "\n",
    "Let's start with defining the configuration of our pipeline (for details see [this tutorial](./Training_a_text_classifier.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from biome.text.data import DataSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = DataSource(train_path).to_dataframe().label.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_dict = {\n",
    "    \"name\": \"german_business_names\",\n",
    "    \"tokenizer\": {\n",
    "        \"text_cleaning\": {\n",
    "            \"rules\": [\"strip_spaces\"]\n",
    "        }\n",
    "    },\n",
    "    \"features\": {\n",
    "        \"word\": {\n",
    "            \"embedding_dim\": 64,\n",
    "            \"lowercase_tokens\": True,\n",
    "        },\n",
    "        \"char\": {\n",
    "            \"embedding_dim\": 32,\n",
    "            \"lowercase_characters\": True,\n",
    "            \"encoder\": {\n",
    "                \"type\": \"gru\",\n",
    "                \"num_layers\": 1,\n",
    "                \"hidden_size\": 32,\n",
    "                \"bidirectional\": True,\n",
    "            },\n",
    "            \"dropout\": 0.1,\n",
    "        },\n",
    "    },\n",
    "    \"head\": {\n",
    "        \"type\": \"TextClassification\",\n",
    "        \"labels\": list(labels.value_counts().index),\n",
    "        \"pooler\": {\n",
    "            \"type\": \"gru\",\n",
    "            \"num_layers\": 1,\n",
    "            \"hidden_size\": 32,\n",
    "            \"bidirectional\": True,\n",
    "        },\n",
    "        \"feedforward\": {\n",
    "            \"num_layers\": 1,\n",
    "            \"hidden_dims\": [32],\n",
    "            \"activations\": [\"relu\"],\n",
    "            \"dropout\": [0.0],\n",
    "        },\n",
    "    },       \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this configuration dictionary to create our pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from biome.text import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = Pipeline.from_config(pipeline_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to define the vocabulary configuration and create our vocabulary.\n",
    "\n",
    "\n",
    "::: tip Note\n",
    "\n",
    "If you want to optimize the vocabulary configuration in the hyperparameter search (for example, the `min_count` argument), you have to move the vocabulary creation to the `trainable` function below. That is, in each trial the vocabulary will be created anew.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from biome.text.configuration import VocabularyConfiguration, WordFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_config = VocabularyConfiguration(sources=[DataSource(train_path)], min_count={WordFeatures.namespace: 20})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.create_vocabulary(vocab_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to reuse the vocabulary in each trial, we have to save it to a folder and store its absolute path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_absolute_path = os.path.abspath(\"./vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.save_vocabulary(vocab_absolute_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the callback for early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will use a trial scheduler that adaptively allocates resources to promising hyperparameter configurations by terminating less promising candidates early.\n",
    "The early stopping mechanism requires the reporting of some metric during a trial.\n",
    "For this we use an `EpochCallback` that is executed after each epoch and pass it on to the `Pipeline.train()` method.\n",
    "\n",
    "Our `TuneReport` class simply reports some metrics back to tune, which in turn are used to define promising trials during the hyperparameter search:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.training import EpochCallback\n",
    "from ray import tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TuneReport(EpochCallback):\n",
    "    def __call__(self, trainer, metrics, epoch, is_master):\n",
    "        # The EpochCallback is also called before the training starts (at epoch = -1)\n",
    "        if epoch > -1:\n",
    "            tune.report(validation_loss=metrics[\"validation_loss\"], validation_accuracy=metrics[\"validation_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_report = TuneReport()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the HPO with *biome.text* we will use the [funtion-based Trainable API](https://docs.ray.io/en/latest/tune/api_docs/trainable.html#tune-function-api) of Ray Tune.\n",
    "Therefore, we have to define a `trainable` function that takes as input a configuration dictionary and executes a training run.\n",
    "\n",
    "We will use the configuration dictionary to create a `Pipeline` and a `TrainerConfiguration` in order to optimize the learning rate.\n",
    "In the `Pipeline.train()` method we will add our `tune_report` instance to the epoch callbacks, and completely silence the output of the training by setting `quiet=True`.\n",
    "This avoids cluttering the output of the hyperparameter search and makes it easier to follow the progress.\n",
    "\n",
    "::: tip Tip\n",
    "\n",
    "Keep in mind that the learning rate \"*is often the single most important hyper-parameter and one should always make sure that it has been tuned (up to approximately a factor of 2). ... If there is only time to optimize one hyper-parameter and one uses stochastic gradient descent, then this is the hyper-parameter that is worth tuning.\" ([Yoshua Bengio](https://arxiv.org/abs/1206.5533)).\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from biome.text.configuration import TrainerConfiguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainable(config):\n",
    "    pl = Pipeline.from_config(config[\"pipeline\"], vocab_absolute_path)\n",
    "    trainer_config = TrainerConfiguration(**config[\"trainer\"])\n",
    "    \n",
    "    train_ds = DataSource(train_path)\n",
    "    valid_ds = DataSource(valid_path)\n",
    "    \n",
    "    pl.train(\n",
    "        output=\"output\",\n",
    "        training=train_ds,\n",
    "        validation=valid_ds,\n",
    "        trainer=trainer_config,\n",
    "        epoch_callbacks=[tune_report],\n",
    "        quiet=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random search with a trial scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will perform a random search with the [Asynchronous Successive Halving Algorithm (ASHA)](https://blog.ml.cmu.edu/2018/12/12/massively-parallel-hyperparameter-optimization/) to schedule our trials.\n",
    "The Ray Tune developers advocate this scheduler as a good starting point for its aggressive termination of low-performing trials.\n",
    "\n",
    "To perform a random search (as well as a grid search) we simply have to replace the parameters we want to optimize with methods from the [Random Distributions API](https://docs.ray.io/en/latest/tune/api_docs/grid_random.html#random-distributions-api) and the [Grid Search API](https://docs.ray.io/en/latest/tune/api_docs/grid_random.html#grid-search-api), respectively.\n",
    "For a complete description of both APIs and how they interplay with each other, see the corresponding section in the [Ray Tune docs](https://docs.ray.io/en/latest/tune/api_docs/grid_random.html).\n",
    "\n",
    "In our case we will tune 4 parameters:\n",
    "- the embedding dimension of our `word` feature\n",
    "- the embedding dimension of our characters in our `char` feature\n",
    "- the output dimension of the `char` feature (the hidden size of our GRU)\n",
    "- and the learning rate\n",
    "\n",
    "For the first 3 parameters we will provide discrete values from which Tune will sample randomly, while for the learning rate we will provide a continuous logarithmic range.\n",
    "Since we want to directly compare the outcome of the optimization with the base configuration of the [underlying tutorial](./Training_a_text_classifier.html), we will fix the number of epochs to 3.\n",
    "\n",
    "::: tip Note\n",
    "\n",
    "By default we will use a GPU.\n",
    "If you do not have one available, just comment out the line `\"cuda_device\": 0` in the trainer section of the dictionary below.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "    \"pipeline\": {\n",
    "        \"name\": \"german_business_names\",\n",
    "        \"tokenizer\": {\n",
    "            \"text_cleaning\": {\n",
    "                \"rules\": [\"strip_spaces\"]\n",
    "            }\n",
    "        },\n",
    "        \"features\": {\n",
    "            \"word\": {\n",
    "                \"embedding_dim\": tune.choice([16, 32, 64]),\n",
    "                \"lowercase_tokens\": True,\n",
    "            },\n",
    "            \"char\": {\n",
    "                \"embedding_dim\": tune.choice([16, 32, 64]),\n",
    "                \"lowercase_characters\": True,\n",
    "                \"encoder\": {\n",
    "                    \"type\": \"gru\",\n",
    "                    \"num_layers\": 1,\n",
    "                    \"hidden_size\": tune.choice([16, 32, 64]),\n",
    "                    \"bidirectional\": True,\n",
    "                },\n",
    "                \"dropout\": 0.1,\n",
    "            },\n",
    "        },\n",
    "        \"head\": {\n",
    "            \"type\": \"TextClassification\",\n",
    "            \"labels\": list(labels.value_counts().index),\n",
    "            \"pooler\": {\n",
    "                \"type\": \"gru\",\n",
    "                \"num_layers\": 1,\n",
    "                \"hidden_size\": 32,\n",
    "                \"bidirectional\": True,\n",
    "            },\n",
    "            \"feedforward\": {\n",
    "                \"num_layers\": 1,\n",
    "                \"hidden_dims\": [32],\n",
    "                \"activations\": [\"relu\"],\n",
    "                \"dropout\": [0.0],\n",
    "            },\n",
    "        },       \n",
    "    }, \n",
    "    \"trainer\": {\n",
    "        \"optimizer\": {\n",
    "            \"type\": \"adam\",\n",
    "            \"lr\": tune.loguniform(0.001, 0.01),\n",
    "        },\n",
    "        \"num_epochs\": 3,\n",
    "        \"cuda_device\": 0,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, we will use the ASHA trial scheduler for our search.\n",
    "To create an instance of the `ASHAScheduler` we have to specify the decisive metric for terminating low-performing trials and the mode of this metric (is the objective to *minimize* the metric, `min`, or to *maximize* it, `max`).\n",
    "For a complete description of the configurations, see the [ASHAScheduler docs](https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#asha-tune-schedulers-ashascheduler)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asha = ASHAScheduler(metric=\"validation_loss\", mode=\"min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following the progress with tensorboard (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ray Tune automatically logs its results with [TensorBoard](https://www.tensorflow.org/tensorboard/).\n",
    "We can take advantage of this and launch a TensorBoard instance before starting the hyperparameter search to follow its progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir ./tune/trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the hyperparameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything ready to start our hyperparameter search with the `tune.run()` method.\n",
    "\n",
    "The number of trials our search will go through depends on the `num_samples` parameter.\n",
    "In our case, a random search, it equals the number of trials, whereas in the case of a grid search the total number of trials is `num_samples` times the grid configurations (see [the docs](https://docs.ray.io/en/latest/tune/api_docs/grid_random.html) for illustrative examples).\n",
    "\n",
    "The number of parallel running trials depends on your `resources_per_trial` configuration and your local resources.\n",
    "The default value is `{\"cpu\": 1, \"gpu\": 0}` and results in 8 parallel running trials on a machine with 8 CPUs.\n",
    "You can also use fractional values, for example to share a GPU between 2 trials, pass on `{\"gpu\": 0.5}`. \n",
    "\n",
    "The `local_dir` parameter defines the output directory of the HPO results and will also contain the training results of each trial (that is the model weights and metrics).\n",
    "\n",
    "::: tip Note\n",
    "\n",
    "Keep in mind: to run your HPO on GPUs, you have to specify them in the `TrainerConfiguration` in the `trainable` function, as well as in the `resources_per_trial` parameter when calling `tune.run()`.\n",
    "If you do not want to use a GPU, just set the value to 0 `{\"cpu\": 1, \"gpu\": 0}`.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = tune.run(\n",
    "    trainable, \n",
    "    config=configs, \n",
    "    scheduler=asha, \n",
    "    num_samples=1, \n",
    "    resources_per_trial={\"cpu\": 1, \"gpu\": 0.5},\n",
    "    local_dir=\"./tune\", \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the results (WIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from the TensorBoard view, we can also use the `analysis` object returned by `tune.run()` to check the results of our HPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.get_best_config(metric=\"validation_loss\", mode=\"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.dataframe(metric=\"validation_loss\", mode=\"min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- compare accuracy with basic tutorial\n",
    "- real live example: increase epochs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
