{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"top\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.recogn.ai/biome-text/documentation/tutorials/Training_a_text_classifier.html\"><img src=\"https://www.recogn.ai/biome-text/assets/img/biome-isotype.svg\" width=32 />View on recogn.ai</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/recognai/biome-text/blob/master/docs/docs/documentation/tutorials/Training_a_text_classifier.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/recognai/biome-text/blob/master/docs/docs/documentation/tutorials/Training_a_text_classifier.ipynb\"><img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\" width=32 />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>\n",
    "\n",
    "When running this tutorial in Google Colab, make sure to install *biome.text* first:\n",
    "```\n",
    "!pip install -U git+https://github.com/recognai/biome-text.git\n",
    "```\n",
    "Ignore warnings and don't forget to restart your runtime afterwards (*Runtime -> Restart runtime*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a short text classifier of German business names\n",
    "\n",
    "In this tutorial we will train a basic short-text classifier for predicting the sector of a business based only on its business name. \n",
    "For this we will use a training dataset with business names and business categories in German."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from biome.text.data import DataSource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data we will use for training. For this we create a `DataSource` instance providing a path to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Edv</td>\n",
       "      <td>Cse Gmbh Computer Edv-service Bürobedarf</td>\n",
       "      <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maler</td>\n",
       "      <td>Malerfachbetrieb U. Nee</td>\n",
       "      <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gebrauchtwagen</td>\n",
       "      <td>Sippl Automobilverkäufer Hausmann</td>\n",
       "      <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Handelsvermittler Und -vertreter</td>\n",
       "      <td>Strenge Handelsagentur Werth</td>\n",
       "      <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gebrauchtwagen</td>\n",
       "      <td>Dzengel Autohaus Gordemitz Rusch</td>\n",
       "      <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Apotheken</td>\n",
       "      <td>Schinkel-apotheke Bitzer</td>\n",
       "      <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tiefbau</td>\n",
       "      <td>Franz Möbius Mehrings-bau-hude Und Stigge</td>\n",
       "      <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Handelsvermittler Und -vertreter</td>\n",
       "      <td>Kontze Hdl.vertr. Lau</td>\n",
       "      <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Autowerkstätten</td>\n",
       "      <td>Keßler Kfz-handel</td>\n",
       "      <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Gebrauchtwagen</td>\n",
       "      <td>Diko Lack Und Schrift Betriebsteil Der Autocen...</td>\n",
       "      <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              label  \\\n",
       "0                               Edv   \n",
       "1                             Maler   \n",
       "2                    Gebrauchtwagen   \n",
       "3  Handelsvermittler Und -vertreter   \n",
       "4                    Gebrauchtwagen   \n",
       "5                         Apotheken   \n",
       "6                           Tiefbau   \n",
       "7  Handelsvermittler Und -vertreter   \n",
       "8                   Autowerkstätten   \n",
       "9                    Gebrauchtwagen   \n",
       "\n",
       "                                                text  \\\n",
       "0           Cse Gmbh Computer Edv-service Bürobedarf   \n",
       "1                            Malerfachbetrieb U. Nee   \n",
       "2                  Sippl Automobilverkäufer Hausmann   \n",
       "3                       Strenge Handelsagentur Werth   \n",
       "4                   Dzengel Autohaus Gordemitz Rusch   \n",
       "5                           Schinkel-apotheke Bitzer   \n",
       "6          Franz Möbius Mehrings-bau-hude Und Stigge   \n",
       "7                              Kontze Hdl.vertr. Lau   \n",
       "8                                  Keßler Kfz-handel   \n",
       "9  Diko Lack Und Schrift Betriebsteil Der Autocen...   \n",
       "\n",
       "                                                path  \n",
       "0  https://biome-tutorials-data.s3-eu-west-1.amaz...  \n",
       "1  https://biome-tutorials-data.s3-eu-west-1.amaz...  \n",
       "2  https://biome-tutorials-data.s3-eu-west-1.amaz...  \n",
       "3  https://biome-tutorials-data.s3-eu-west-1.amaz...  \n",
       "4  https://biome-tutorials-data.s3-eu-west-1.amaz...  \n",
       "5  https://biome-tutorials-data.s3-eu-west-1.amaz...  \n",
       "6  https://biome-tutorials-data.s3-eu-west-1.amaz...  \n",
       "7  https://biome-tutorials-data.s3-eu-west-1.amaz...  \n",
       "8  https://biome-tutorials-data.s3-eu-west-1.amaz...  \n",
       "9  https://biome-tutorials-data.s3-eu-west-1.amaz...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = DataSource(\"https://biome-tutorials-data.s3-eu-west-1.amazonaws.com/text_classifier/business.cat.train.csv\")\n",
    "train_ds.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we have two relevant columns *label* and *text*. The *path* column is added automatically by the `DataSource` class to keep track of the source file.\n",
    "\n",
    "Our classifier will be trained to predict the *label* given a *text*.\n",
    "\n",
    "The `DataSource` class stores the data in an underlying [Dask DataFrame](https://docs.dask.org/en/latest/dataframe.html) that you can easily access.\n",
    "For example, let's check the size of our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds.to_dataframe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or let's check the distribution of our labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unternehmensberatungen              632\n",
       "Friseure                            564\n",
       "Tiefbau                             508\n",
       "Dienstleistungen                    503\n",
       "Gebrauchtwagen                      449\n",
       "Elektriker                          430\n",
       "Restaurants                         422\n",
       "Architekturbüros                    417\n",
       "Vereine                             384\n",
       "Versicherungsvermittler             358\n",
       "Maler                               330\n",
       "Sanitärinstallationen               323\n",
       "Edv                                 318\n",
       "Werbeagenturen                      294\n",
       "Apotheken                           289\n",
       "Physiotherapie                      286\n",
       "Vermittlungen                       277\n",
       "Hotels                              274\n",
       "Autowerkstätten                     263\n",
       "Elektrotechnik                      261\n",
       "Allgemeinärzte                      216\n",
       "Handelsvermittler Und -vertreter    202\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = train_ds.to_dataframe().label.compute()\n",
    "labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: tip\n",
    "\n",
    "The [TaskHead](../../api/biome/text/modules/heads/task_head.html#taskhead) of our model will expect a *text* and a *label* column to be present in the dataframe. \n",
    "Since they are already present, there is no need for a [mapping](../../api/biome/text/data/datasource.html#datasource) in the `DataSource`.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure your `biome.text` Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from biome.text import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical [Pipeline](../../api/biome/text/pipeline.html#pipeline) consists of tokenizing the input, extracting features, applying a language encoding (optionally) and executing a task-specific head in the end.\n",
    "\n",
    "After training a pipeline, you can use it to make predictions or explore the underlying model via the [UI](../../documentation/user-guides/02.explore.html).\n",
    "\n",
    "As a first step we must define a configuration for our pipeline. \n",
    "In this tutorial we will create a configuration dictionary and use the `Pipeline.from_config()` method to create our pipeline, but there are [other ways](../../api/biome/text/pipeline.html#pipeline)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `biome.text` pipeline has the following main components:\n",
    "\n",
    "```yaml\n",
    "name: # a descriptive name of your pipeline\n",
    "\n",
    "tokenizer: # how to tokenize the input\n",
    "\n",
    "features: # input features of the model\n",
    "\n",
    "encoder: # the language encoder\n",
    "\n",
    "head: # your task configuration\n",
    "\n",
    "```\n",
    "\n",
    "See the [Configuration section](../../documentation/user-guides/05.configuration.html) for a detailed description of how these main components can be configured.\n",
    "\n",
    "Our complete configuration for this tutorial will be following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_dict = {\n",
    "    \"name\": \"german_business_names\",\n",
    "    \"tokenizer\": {\n",
    "        \"text_cleaning\": {\n",
    "            \"rules\": [\"strip_spaces\"]\n",
    "        }\n",
    "    },\n",
    "    \"features\": {\n",
    "        \"word\": {\n",
    "            \"embedding_dim\": 64,\n",
    "            \"lowercase_tokens\": True,\n",
    "        },\n",
    "        \"char\": {\n",
    "            \"embedding_dim\": 32,\n",
    "            \"lowercase_characters\": True,\n",
    "            \"encoder\": {\n",
    "                \"type\": \"gru\",\n",
    "                \"num_layers\": 1,\n",
    "                \"hidden_size\": 32,\n",
    "                \"bidirectional\": True,\n",
    "            },\n",
    "            \"dropout\": 0.1,\n",
    "        },\n",
    "    },\n",
    "    \"head\": {\n",
    "        \"type\": \"TextClassification\",\n",
    "        \"labels\": list(labels.value_counts().index),\n",
    "        \"pooler\": {\n",
    "            \"type\": \"gru\",\n",
    "            \"num_layers\": 1,\n",
    "            \"hidden_size\": 32,\n",
    "            \"bidirectional\": True,\n",
    "        },\n",
    "        \"feedforward\": {\n",
    "            \"num_layers\": 1,\n",
    "            \"hidden_dims\": [32],\n",
    "            \"activations\": [\"relu\"],\n",
    "            \"dropout\": [0.0],\n",
    "        },\n",
    "    },       \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this dictionary we can now create a `Pipeline`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = Pipeline.from_config(pipeline_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can start the training we need to create the vocabulary for our model.\n",
    "For this we define a `VocabularyConfiguration`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from biome.text.configuration import VocabularyConfiguration, WordFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our business name classifier we only want to include words with a general meaning to our word feature vocabulary (like \"Computer\" or \"Autohaus\", for example), and want to exclude specific names that will not help to generally classify the kind of business.\n",
    "This can be achieved by including only the most frequent words in our training set via the `min_count` argument. For a complete list of available arguments see the [VocabularyConfiguration API](../../api/biome/text/configuration.html#vocabularyconfiguration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_config = VocabularyConfiguration(sources=[train_ds], min_count={WordFeatures.namespace: 20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then pass this configuration to our `Pipeline` to create the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbreg": {
     "diff_ignore": [
      "/outputs/0/data"
     ]
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b76972ed7b4cf88f9fa31ecde825fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pl.create_vocabulary(vocab_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the vocabulary we can check the size of our entire model in terms of trainable parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60566"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.trainable_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next step we have to configure the *trainer*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from biome.text.configuration import TrainerConfiguration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default trainer has sensible defaults and should work alright for most of your cases.\n",
    "In this tutorial, however, we want to tune a bit the learning rate and limit the training time to three epochs only.\n",
    "For a complete list of available arguments see the [TrainerConfiguration API](../../api/biome/text/configuration.html#trainerconfiguration).\n",
    "\n",
    "::: tip\n",
    "\n",
    "In case you have a cuda device available, you also specify it here.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = TrainerConfiguration(\n",
    "    optimizer={\n",
    "        \"type\": \"adam\",\n",
    "        \"lr\": 0.01,\n",
    "    },\n",
    "    num_epochs=3,\n",
    "    # cuda_device=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your model\n",
    "\n",
    "Now we have everything ready to start the training of our model:\n",
    "- training data set\n",
    "- vocabulary\n",
    "- trainer\n",
    "\n",
    "Optionally we can provide a validation data set to estimate the generalization error.\n",
    "For this we will create another `DataSource` pointing to our validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ds = DataSource(\"https://biome-tutorials-data.s3-eu-west-1.amazonaws.com/text_classifier/business.cat.valid.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training output will be saved in a folder specified by the `output` argument. It contains the trained model weights and the metrics, as well as the vocabulary and a *log* folder for visualizing the training process with [tensorboard](https://www.tensorflow.org/tensorboard/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbreg": {
     "diff_ignore": [
      "/outputs/*"
     ]
    }
   },
   "outputs": [],
   "source": [
    "pl.train(\n",
    "    output=\"output\",\n",
    "    training=train_ds,\n",
    "    validation=valid_ds,\n",
    "    trainer=trainer_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: tip\n",
    "\n",
    "If for some reason the training gets interrupted, you can continue where you left off by setting the `restore` argument in the `Pipeline.train()` method to `True`. \n",
    "If you want to train your model for a few more epochs, you can also use the `restore` argument, but you have to modify the `epochs` argument in your `TrainerConfiguration` to reflect the total amount of epochs you aim for.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make your first predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we trained our model we can go on to make our first predictions.\n",
    "First we must load our trained model into a new `Pipeline`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "nbreg": {
     "diff_ignore": [
      "/outputs/0/text"
     ]
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 139935435637520 acquired on /tmp/tmptjrxxrdz/vocabulary/.lock\n",
      "INFO:filelock:Lock 139935435637520 released on /tmp/tmptjrxxrdz/vocabulary/.lock\n"
     ]
    }
   ],
   "source": [
    "pl_trained = Pipeline.from_pretrained(\"output/model.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then provide the input expected by our `TaskHead` of the model to the `Pipeline.predict()` method.\n",
    "In our case it is a `TextClassification` head that classifies a `text` input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "nbreg": {
     "diff_ignore": [
      "/outputs/0/data/text/plain"
     ]
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logits': array([-12.923394 ,  -7.1921663, -12.471584 ,  -4.4750395,   4.683301 ,\n",
       "         -8.345891 ,  -6.496138 ,  -8.7462845, -15.876246 , -13.891476 ,\n",
       "         -6.8748384, -14.603043 ,  -6.4658375,  -2.5150626,  -5.1593747,\n",
       "        -13.662921 , -19.010736 ,  -3.9156628,   0.8396884,  -6.456712 ,\n",
       "        -22.493284 ,  -9.04215  ], dtype=float32),\n",
       " 'probs': array([2.2070692e-08, 6.8054460e-06, 3.4676471e-08, 1.0301247e-04,\n",
       "        9.7792721e-01, 2.1468434e-06, 1.3650156e-05, 1.4385059e-06,\n",
       "        1.1518834e-09, 8.3826999e-09, 9.3469562e-06, 4.1148533e-09,\n",
       "        1.4070101e-05, 7.3130248e-04, 5.1962084e-05, 1.0535219e-08,\n",
       "        5.0132159e-11, 1.8022873e-04, 2.0943379e-02, 1.4199089e-05,\n",
       "        1.5405122e-12, 1.0700871e-06], dtype=float32),\n",
       " 'classes': {'Gebrauchtwagen': tensor(0.9779),\n",
       "  'Autowerkstätten': tensor(0.0209),\n",
       "  'Werbeagenturen': tensor(0.0007),\n",
       "  'Hotels': tensor(0.0002),\n",
       "  'Dienstleistungen': tensor(0.0001),\n",
       "  'Apotheken': tensor(5.1962e-05),\n",
       "  'Elektrotechnik': tensor(1.4199e-05),\n",
       "  'Edv': tensor(1.4070e-05),\n",
       "  'Restaurants': tensor(1.3650e-05),\n",
       "  'Maler': tensor(9.3470e-06),\n",
       "  'Friseure': tensor(6.8054e-06),\n",
       "  'Elektriker': tensor(2.1468e-06),\n",
       "  'Architekturbüros': tensor(1.4385e-06),\n",
       "  'Handelsvermittler Und -vertreter': tensor(1.0701e-06),\n",
       "  'Tiefbau': tensor(3.4676e-08),\n",
       "  'Unternehmensberatungen': tensor(2.2071e-08),\n",
       "  'Physiotherapie': tensor(1.0535e-08),\n",
       "  'Versicherungsvermittler': tensor(8.3827e-09),\n",
       "  'Sanitärinstallationen': tensor(4.1149e-09),\n",
       "  'Vereine': tensor(1.1519e-09),\n",
       "  'Vermittlungen': tensor(5.0132e-11),\n",
       "  'Allgemeinärzte': tensor(1.5405e-12)},\n",
       " 'max_class': 'Gebrauchtwagen',\n",
       " 'max_class_prob': tensor(0.9779),\n",
       " 'label': 'Gebrauchtwagen',\n",
       " 'prob': tensor(0.9779)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl_trained.predict(text=\"Autohaus biome.text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned dictionary contains the logits and probabilities of all labels (classes).\n",
    "The label with the highest probability is stored under the `label` key, together with its probability under the `prob` key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: tip\n",
    "\n",
    "When configuring the pipeline in the first place, we recommend to check that it is correctly setup by using the `predict` method.\n",
    "Since the pipeline is still not trained at that moment, the predictions will be arbitrary.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the model's predictions\n",
    "\n",
    "To check and understand the predictions of the model, we can use the **biome.text explore UI**.\n",
    "Just calling the [Pipeline.predict](../../api/biome/text/pipeline.html#explore) method will open the UI in the output of our cell.\n",
    "We will set the `explain` argument to true, which automatically visualizes the attribution of each token by means of [integrated gradients](https://arxiv.org/abs/1703.01365).\n",
    "\n",
    "The usage of the explore UI should be largely intuitive, but if you are interested in more details see this [user guide](../../documentation/user-guides/02.explore.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: warning\n",
    "            \n",
    "For the UI to work you need a running [Elasticsearch](https://www.elastic.co/elasticsearch/) instance.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbreg": {
     "diff_ignore": [
      "/outputs/*"
     ]
    }
   },
   "outputs": [],
   "source": [
    "pl_trained.explore(valid_ds, explain=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the F1 scores of each label, we can figure out which labels to prioritize when gathering new training data.\n",
    "For example, although \"Allgemeinärzte\" is the second rarest label in our training data, it still seems relatively easy to classify for our model due to the distinctive words \"Dr.\" and \"Allgemeinmedizin\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "nbreg": {
   "diff_ignore": [
    "/metadata/widgets",
    "/metadata/language_info",
    "/cells/*/execution_count"
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
