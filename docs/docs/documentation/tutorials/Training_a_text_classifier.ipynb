{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a short text classifier of German business names\n",
    "\n",
    "In this tutorial we will train a basic short-text classifier for predicting the sector of a business based only on its business name. For this we will use a training dataset with business names and business categories in German.\n",
    "\n",
    "The tutorial will guide you through the following steps:\n",
    "\n",
    "\n",
    "[[toc]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore and prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from biome.text.data import DataSource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data we will use for training. For this we create a `DataSource` instance providing a path to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Edv</td>\n",
       "      <td>Cse Gmbh Computer Edv-service Bürobedarf</td>\n",
       "      <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maler</td>\n",
       "      <td>Malerfachbetrieb U. Nee</td>\n",
       "      <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gebrauchtwagen</td>\n",
       "      <td>Sippl Automobilverkäufer Hausmann</td>\n",
       "      <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Handelsvermittler Und -vertreter</td>\n",
       "      <td>Strenge Handelsagentur Werth</td>\n",
       "      <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gebrauchtwagen</td>\n",
       "      <td>Dzengel Autohaus Gordemitz Rusch</td>\n",
       "      <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Apotheken</td>\n",
       "      <td>Schinkel-apotheke Bitzer</td>\n",
       "      <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tiefbau</td>\n",
       "      <td>Franz Möbius Mehrings-bau-hude Und Stigge</td>\n",
       "      <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Handelsvermittler Und -vertreter</td>\n",
       "      <td>Kontze Hdl.vertr. Lau</td>\n",
       "      <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Autowerkstätten</td>\n",
       "      <td>Keßler Kfz-handel</td>\n",
       "      <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Gebrauchtwagen</td>\n",
       "      <td>Diko Lack Und Schrift Betriebsteil Der Autocen...</td>\n",
       "      <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              label  \\\n",
       "0                               Edv   \n",
       "1                             Maler   \n",
       "2                    Gebrauchtwagen   \n",
       "3  Handelsvermittler Und -vertreter   \n",
       "4                    Gebrauchtwagen   \n",
       "5                         Apotheken   \n",
       "6                           Tiefbau   \n",
       "7  Handelsvermittler Und -vertreter   \n",
       "8                   Autowerkstätten   \n",
       "9                    Gebrauchtwagen   \n",
       "\n",
       "                                                text  \\\n",
       "0           Cse Gmbh Computer Edv-service Bürobedarf   \n",
       "1                            Malerfachbetrieb U. Nee   \n",
       "2                  Sippl Automobilverkäufer Hausmann   \n",
       "3                       Strenge Handelsagentur Werth   \n",
       "4                   Dzengel Autohaus Gordemitz Rusch   \n",
       "5                           Schinkel-apotheke Bitzer   \n",
       "6          Franz Möbius Mehrings-bau-hude Und Stigge   \n",
       "7                              Kontze Hdl.vertr. Lau   \n",
       "8                                  Keßler Kfz-handel   \n",
       "9  Diko Lack Und Schrift Betriebsteil Der Autocen...   \n",
       "\n",
       "                                                path  \n",
       "0  https://biome-tutorials-data.s3-eu-west-1.amaz...  \n",
       "1  https://biome-tutorials-data.s3-eu-west-1.amaz...  \n",
       "2  https://biome-tutorials-data.s3-eu-west-1.amaz...  \n",
       "3  https://biome-tutorials-data.s3-eu-west-1.amaz...  \n",
       "4  https://biome-tutorials-data.s3-eu-west-1.amaz...  \n",
       "5  https://biome-tutorials-data.s3-eu-west-1.amaz...  \n",
       "6  https://biome-tutorials-data.s3-eu-west-1.amaz...  \n",
       "7  https://biome-tutorials-data.s3-eu-west-1.amaz...  \n",
       "8  https://biome-tutorials-data.s3-eu-west-1.amaz...  \n",
       "9  https://biome-tutorials-data.s3-eu-west-1.amaz...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train = DataSource(\"https://biome-tutorials-data.s3-eu-west-1.amazonaws.com/text_classifier/business.cat.train.csv\")\n",
    "ds_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we have two relevant columns *label* and *text*. The *path* column is added automatically by the `DataSource` class to keep track of the source file.\n",
    "\n",
    "Our classifier will be trained to predict the *label* given a *text*.\n",
    "\n",
    "The `DataSource` class stores the data in an underlying [Dask DataFrame](https://docs.dask.org/en/latest/dataframe.html) that you can easily access.\n",
    "For example, let's check the size of our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds_train.to_dataframe())  # TODO: Maybe make `DataSource.to_dataframe()` a property and simply rename it to `DataSource.dataframe`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or let's check the distribution of our labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unternehmensberatungen              632\n",
       "Friseure                            564\n",
       "Tiefbau                             508\n",
       "Dienstleistungen                    503\n",
       "Gebrauchtwagen                      449\n",
       "Elektriker                          430\n",
       "Restaurants                         422\n",
       "Architekturbüros                    417\n",
       "Vereine                             384\n",
       "Versicherungsvermittler             358\n",
       "Maler                               330\n",
       "Sanitärinstallationen               323\n",
       "Edv                                 318\n",
       "Werbeagenturen                      294\n",
       "Apotheken                           289\n",
       "Physiotherapie                      286\n",
       "Vermittlungen                       277\n",
       "Hotels                              274\n",
       "Autowerkstätten                     263\n",
       "Elektrotechnik                      261\n",
       "Allgemeinärzte                      216\n",
       "Handelsvermittler Und -vertreter    202\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = ds_train.to_dataframe().compute()\n",
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: tip\n",
    "\n",
    "The [TaskHead](link/to/API) of our model will expect a *text* and a *label* column to be present in the dataframe. \n",
    "Since they are already present, there is no need for a [mapping](link/to/API) in the `DataSource`.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure your `biome.text` Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from biome.text import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical [Pipeline](link/to/API) consists of tokenizing the input, extracting features, applying a language encoding (optionally) and executing a task-specific head in the end.\n",
    "\n",
    "After training a pipeline, you can use it to make predictions or explore the underlying model via the [UI](link/to/API).\n",
    "\n",
    "As a first step we must define a configuration for our pipeline. \n",
    "In this tutorial we will create a configuration dictionary and use the `Pipeline.from_config()` method to create our pipeline, but there are [other ways](link/to/API)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `biome.text` pipeline has the following main components:\n",
    "\n",
    "```yaml\n",
    "name: # a descriptive name of your pipeline\n",
    "\n",
    "tokenizer: # how to tokenize the input\n",
    "\n",
    "features: # input features of the model\n",
    "\n",
    "encoder: # the language encoder\n",
    "\n",
    "head: # your task configuration\n",
    "\n",
    "```\n",
    "\n",
    "See the [Configuration section](link/to/config) for a detailed description of how these main components can be configured.\n",
    "\n",
    "Our complete configuration for this tutorial will be following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_dict = {\n",
    "    \"name\": \"german_business_names\",\n",
    "    \"tokenizer\": {\n",
    "        \"text_cleaning\": {\n",
    "            \"rules\": [\"strip_spaces\"]\n",
    "        }\n",
    "    },\n",
    "    \"features\": {\n",
    "        \"word\": {\n",
    "            \"embedding_dim\": 32,\n",
    "            \"lowercase_tokens\": True,\n",
    "        },\n",
    "        \"char\": {\n",
    "            \"embedding_dim\": 32,\n",
    "            \"lowercase_characters\": True,\n",
    "            \"encoder\": {\n",
    "                \"type\": \"gru\",\n",
    "                \"num_layers\": 1,\n",
    "                \"hidden_size\": 32,\n",
    "                \"bidirectional\": True,\n",
    "            },\n",
    "            \"dropout\": 0.0,\n",
    "        },\n",
    "    },\n",
    "    \"head\": {\n",
    "        \"type\": \"TextClassification\",\n",
    "        \"labels\": list(df.label.value_counts().index),\n",
    "        \"pooler\": {\n",
    "            \"type\": \"gru\",\n",
    "            \"num_layers\": 1,\n",
    "            \"hidden_size\": 32,\n",
    "            \"bidirectional\": True,\n",
    "        },\n",
    "        \"feedforward\": {\n",
    "            \"num_layers\": 1,\n",
    "            \"hidden_dims\": [32],\n",
    "            \"activations\": [\"relu\"],\n",
    "            \"dropout\": [0.0],\n",
    "        },\n",
    "    },       \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this dictionary we can now create a `Pipeline`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = Pipeline.from_config(pipeline_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can start the training we need to create the vocabulary for our model.\n",
    "For this we define a `VocabularyConfiguration`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from biome.text.configuration import VocabularyConfiguration, WordFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our business name classifier we only want to include words with a general meaning to our word feature vocabulary (like \"Computer\" or \"Autohaus\", for example), and want to exclude specific names that will not help to generally classify the kind of business.\n",
    "This can be achieved by including only the most frequent words in our training set via the `min_count` argument. For a complete list of available arguments see the [VocabularyConfiguration API](link/to/api)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_config = VocabularyConfiguration(sources=[ds_train], min_count={WordFeatures.namespace: 100})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then pass this configuration to our `Pipeline` to create the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbreg": {
     "diff_ignore": [
      "/outputs/0/data"
     ]
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dde88c0b1c3455ab7fddd13a27d6e5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pl.create_vocabulary(vocab_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the vocabulary we can check the size of our entire model in terms of trainable parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43926"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.trainable_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next step we have to configure the *trainer*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from biome.text.configuration import TrainerConfiguration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default trainer has sensible defaults and should work alright for most of your cases.\n",
    "In this tutorial, however, we want to tune a bit the learning rate and limit the training time to one epoch only.\n",
    "For a complete list of the available arguments see the [TrainerConfiguration API](link/to/api).\n",
    "\n",
    "In case you have a cuda device available, you also specify it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = TrainerConfiguration(\n",
    "    optimizer={\n",
    "        \"type\": \"adam\",\n",
    "        \"lr\": 0.01,\n",
    "    },\n",
    "    num_epochs=1,\n",
    "    # cuda_device=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your model\n",
    "\n",
    "Now we have everything ready to start the training of our model:\n",
    "- training data set\n",
    "- vocabulary\n",
    "- trainer\n",
    "\n",
    "Optionally we can provide a validation data set to estimate the generalization error.\n",
    "For this we will create another `DataSource` pointing to our validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_valid = DataSource(\"https://biome-tutorials-data.s3-eu-west-1.amazonaws.com/text_classifier/business.cat.valid.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training output will be saved in a folder specified by the `output` argument. It contains the trained model weights and the metrics, as well as the vocabulary and a *log* folder for visualizing the training process with [tensorboard](https://www.tensorflow.org/tensorboard/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "nbreg": {
     "diff_ignore": [
      "/outputs/*"
     ]
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.common.params:validation_dataset_reader = None\n",
      "INFO:allennlp.common.params:train_data_path = output/.datasources/training_business.cat.train.csv.yml\n",
      "INFO:biome.text._model:Reading training data from output/.datasources/training_business.cat.train.csv.yml\n",
      "INFO:allennlp.common.params:validation_data_path = output/.datasources/validation_business.cat.valid.csv.yml\n",
      "INFO:biome.text._model:Reading validation data from output/.datasources/validation_business.cat.valid.csv.yml\n",
      "INFO:allennlp.common.params:test_data_path = None\n",
      "INFO:allennlp.common.params:random_seed = 13370\n",
      "INFO:allennlp.common.params:numpy_seed = 1337\n",
      "INFO:allennlp.common.params:pytorch_seed = 133\n",
      "INFO:allennlp.common.checks:Pytorch version: 1.5.0\n",
      "INFO:biome.text._model:Serialization directory (output) already exists and is not empty.\n",
      "WARNING:root:vocabulary serialization directory output/vocabulary is not empty\n",
      "INFO:filelock:Lock 140281347510480 acquired on output/vocabulary/.lock\n",
      "INFO:filelock:Lock 140281347510480 released on output/vocabulary/.lock\n",
      "INFO:allennlp.common.params:trainer.no_grad = ()\n",
      "INFO:allennlp.common.params:trainer.type = gradient_descent\n",
      "INFO:allennlp.common.params:trainer.local_rank = 0\n",
      "INFO:allennlp.common.params:trainer.patience = 2\n",
      "INFO:allennlp.common.params:trainer.validation_metric = -loss\n",
      "INFO:allennlp.common.params:trainer.num_epochs = 1\n",
      "INFO:allennlp.common.params:trainer.cuda_device = -1\n",
      "INFO:allennlp.common.params:trainer.grad_norm = None\n",
      "INFO:allennlp.common.params:trainer.grad_clipping = None\n",
      "INFO:allennlp.common.params:trainer.distributed = None\n",
      "INFO:allennlp.common.params:trainer.world_size = 1\n",
      "INFO:allennlp.common.params:trainer.num_gradient_accumulation_steps = 1\n",
      "INFO:allennlp.common.params:trainer.opt_level = None\n",
      "INFO:allennlp.common.params:trainer.no_grad = None\n",
      "INFO:allennlp.common.params:trainer.learning_rate_scheduler = None\n",
      "INFO:allennlp.common.params:trainer.momentum_scheduler = None\n",
      "INFO:allennlp.common.params:trainer.moving_average = None\n",
      "INFO:allennlp.common.params:trainer.batch_callbacks = None\n",
      "INFO:allennlp.common.params:trainer.epoch_callbacks = None\n",
      "INFO:allennlp.common.util:The following parameters are Frozen (without gradient):\n",
      "INFO:allennlp.common.util:The following parameters are Tunable (with gradient):\n",
      "INFO:allennlp.common.util:_head.backbone.embedder.token_embedder_word.weight\n",
      "INFO:allennlp.common.util:_head.backbone.embedder.token_embedder_char._embedding._module.weight\n",
      "INFO:allennlp.common.util:_head.backbone.embedder.token_embedder_char._encoder._module._module.weight_ih_l0\n",
      "INFO:allennlp.common.util:_head.backbone.embedder.token_embedder_char._encoder._module._module.weight_hh_l0\n",
      "INFO:allennlp.common.util:_head.backbone.embedder.token_embedder_char._encoder._module._module.bias_ih_l0\n",
      "INFO:allennlp.common.util:_head.backbone.embedder.token_embedder_char._encoder._module._module.bias_hh_l0\n",
      "INFO:allennlp.common.util:_head.backbone.embedder.token_embedder_char._encoder._module._module.weight_ih_l0_reverse\n",
      "INFO:allennlp.common.util:_head.backbone.embedder.token_embedder_char._encoder._module._module.weight_hh_l0_reverse\n",
      "INFO:allennlp.common.util:_head.backbone.embedder.token_embedder_char._encoder._module._module.bias_ih_l0_reverse\n",
      "INFO:allennlp.common.util:_head.backbone.embedder.token_embedder_char._encoder._module._module.bias_hh_l0_reverse\n",
      "INFO:allennlp.common.util:_head.pooler._module.weight_ih_l0\n",
      "INFO:allennlp.common.util:_head.pooler._module.weight_hh_l0\n",
      "INFO:allennlp.common.util:_head.pooler._module.bias_ih_l0\n",
      "INFO:allennlp.common.util:_head.pooler._module.bias_hh_l0\n",
      "INFO:allennlp.common.util:_head.pooler._module.weight_ih_l0_reverse\n",
      "INFO:allennlp.common.util:_head.pooler._module.weight_hh_l0_reverse\n",
      "INFO:allennlp.common.util:_head.pooler._module.bias_ih_l0_reverse\n",
      "INFO:allennlp.common.util:_head.pooler._module.bias_hh_l0_reverse\n",
      "INFO:allennlp.common.util:_head.feedforward._linear_layers.0.weight\n",
      "INFO:allennlp.common.util:_head.feedforward._linear_layers.0.bias\n",
      "INFO:allennlp.common.util:_head._classification_layer.weight\n",
      "INFO:allennlp.common.util:_head._classification_layer.bias\n",
      "INFO:allennlp.common.params:trainer.optimizer.type = adam\n",
      "INFO:allennlp.common.params:trainer.optimizer.parameter_groups = None\n",
      "INFO:allennlp.common.params:trainer.optimizer.lr = 0.01\n",
      "INFO:allennlp.common.params:trainer.optimizer.betas = (0.9, 0.999)\n",
      "INFO:allennlp.common.params:trainer.optimizer.eps = 1e-08\n",
      "INFO:allennlp.common.params:trainer.optimizer.weight_decay = 0.0\n",
      "INFO:allennlp.common.params:trainer.optimizer.amsgrad = False\n",
      "INFO:allennlp.training.optimizers:Number of trainable parameters: 43926\n",
      "INFO:allennlp.common.params:trainer.checkpointer.type = default\n",
      "INFO:allennlp.common.params:trainer.checkpointer.keep_serialized_model_every_num_seconds = None\n",
      "INFO:allennlp.common.params:trainer.checkpointer.num_serialized_models_to_keep = 1\n",
      "INFO:allennlp.common.params:trainer.checkpointer.model_save_interval = None\n",
      "INFO:allennlp.common.params:trainer.tensorboard_writer.summary_interval = 100\n",
      "INFO:allennlp.common.params:trainer.tensorboard_writer.histogram_interval = None\n",
      "INFO:allennlp.common.params:trainer.tensorboard_writer.batch_size_interval = None\n",
      "INFO:allennlp.common.params:trainer.tensorboard_writer.should_log_parameter_statistics = True\n",
      "INFO:allennlp.common.params:trainer.tensorboard_writer.should_log_learning_rate = True\n",
      "INFO:allennlp.common.params:trainer.tensorboard_writer.get_batch_num_total = None\n",
      "INFO:allennlp.training.trainer:Beginning training.\n",
      "INFO:allennlp.training.trainer:Epoch 0/0\n",
      "INFO:allennlp.training.trainer:Peak CPU memory usage MB: 2092.428\n",
      "INFO:allennlp.training.trainer:GPU 0 memory usage MB: 26\n",
      "INFO:allennlp.training.trainer:Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c97b4f05f9d433f91eda326916c7add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:allennlp.training.util:Metrics with names beginning with \"_\" will not be logged to the tqdm progress bar.\n",
      "INFO:allennlp.training.trainer:Validating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a12e21ed138e46dd98125789a6e25259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.training.tensorboard_writer:                                                Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer:_precision/Physiotherapie                   |     0.796  |     0.889\n",
      "INFO:allennlp.training.tensorboard_writer:_fscore/Tiefbau                             |     0.690  |     0.959\n",
      "INFO:allennlp.training.tensorboard_writer:_fscore/Handelsvermittler_Und_-vertreter    |     0.600  |     0.750\n",
      "INFO:allennlp.training.tensorboard_writer:_precision/Dienstleistungen                 |     0.669  |     0.761\n",
      "INFO:allennlp.training.tensorboard_writer:reg_loss                                    |     0.000  |     0.000\n",
      "INFO:allennlp.training.tensorboard_writer:_recall/Versicherungsvermittler             |     0.774  |     0.942\n",
      "INFO:allennlp.training.tensorboard_writer:_precision/Elektrotechnik                   |     0.832  |     0.983\n",
      "INFO:allennlp.training.tensorboard_writer:_fscore/Autowerkstätten                     |     0.632  |     0.667\n",
      "INFO:allennlp.training.tensorboard_writer:_fscore/Hotels                              |     0.664  |     0.816\n",
      "INFO:allennlp.training.tensorboard_writer:_precision/Edv                              |     0.642  |     0.983\n",
      "INFO:allennlp.training.tensorboard_writer:_recall/Elektrotechnik                      |     0.628  |     0.838\n",
      "INFO:allennlp.training.tensorboard_writer:_recall/Unternehmensberatungen              |     0.796  |     0.951\n",
      "INFO:allennlp.training.tensorboard_writer:_precision/Vermittlungen                    |     0.750  |     0.917\n",
      "INFO:allennlp.training.tensorboard_writer:_precision/Unternehmensberatungen           |     0.796  |     0.814\n",
      "INFO:allennlp.training.tensorboard_writer:_recall/Gebrauchtwagen                      |     0.755  |     0.915\n",
      "INFO:allennlp.training.tensorboard_writer:_fscore/Friseure                            |     0.769  |     0.927\n",
      "INFO:allennlp.training.tensorboard_writer:cpu_memory_MB                               |  2092.428  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:_precision/Tiefbau                          |     0.587  |     0.943\n",
      "INFO:allennlp.training.tensorboard_writer:_fscore/Dienstleistungen                    |     0.692  |     0.847\n",
      "INFO:allennlp.training.tensorboard_writer:_recall/Apotheken                           |     0.685  |     0.789\n",
      "INFO:allennlp.training.tensorboard_writer:_precision/Elektriker                       |     0.839  |     0.889\n",
      "INFO:allennlp.training.tensorboard_writer:macro/fscore                                |     0.733  |     0.882\n",
      "INFO:allennlp.training.tensorboard_writer:_precision/Handelsvermittler_Und_-vertreter |     0.739  |     0.643\n",
      "INFO:allennlp.training.tensorboard_writer:_recall/Hotels                              |     0.558  |     0.739\n",
      "INFO:allennlp.training.tensorboard_writer:_recall/Sanitärinstallationen               |     0.740  |     0.903\n",
      "INFO:allennlp.training.tensorboard_writer:macro/precision                             |     0.766  |     0.900\n",
      "INFO:allennlp.training.tensorboard_writer:_recall/Autowerkstätten                     |     0.559  |     0.536\n",
      "INFO:allennlp.training.tensorboard_writer:_precision/Sanitärinstallationen            |     0.833  |     0.955\n",
      "INFO:allennlp.training.tensorboard_writer:_fscore/Elektrotechnik                      |     0.716  |     0.905\n",
      "INFO:allennlp.training.tensorboard_writer:_recall/Physiotherapie                      |     0.710  |     1.000\n",
      "INFO:allennlp.training.tensorboard_writer:micro/fscore                                |     0.734  |     0.889\n",
      "INFO:allennlp.training.tensorboard_writer:_precision/Restaurants                      |     0.393  |     0.722\n",
      "INFO:allennlp.training.tensorboard_writer:_recall/Edv                                 |     0.547  |     0.611\n",
      "INFO:allennlp.training.tensorboard_writer:_recall/Vereine                             |     0.844  |     0.885\n",
      "INFO:allennlp.training.tensorboard_writer:macro/recall                                |     0.715  |     0.879\n",
      "INFO:allennlp.training.tensorboard_writer:_recall/Werbeagenturen                      |     0.663  |     0.838\n",
      "INFO:allennlp.training.tensorboard_writer:_fscore/Vermittlungen                       |     0.749  |     0.943\n",
      "INFO:allennlp.training.tensorboard_writer:_fscore/Sanitärinstallationen               |     0.784  |     0.928\n",
      "INFO:allennlp.training.tensorboard_writer:_recall/Friseure                            |     0.800  |     0.943\n",
      "INFO:allennlp.training.tensorboard_writer:_precision/Gebrauchtwagen                   |     0.764  |     0.871\n",
      "INFO:allennlp.training.tensorboard_writer:_fscore/Unternehmensberatungen              |     0.796  |     0.877\n",
      "INFO:allennlp.training.tensorboard_writer:_precision/Apotheken                        |     0.884  |     0.900\n",
      "INFO:allennlp.training.tensorboard_writer:_precision/Friseure                         |     0.741  |     0.911\n",
      "INFO:allennlp.training.tensorboard_writer:_recall/Handelsvermittler_Und_-vertreter    |     0.505  |     0.900\n",
      "INFO:allennlp.training.tensorboard_writer:_precision/Vereine                          |     0.878  |     0.979\n",
      "INFO:allennlp.training.tensorboard_writer:_precision/Architekturbüros                 |     0.804  |     0.981\n",
      "INFO:allennlp.training.tensorboard_writer:_precision/Versicherungsvermittler          |     0.780  |     0.980\n",
      "INFO:allennlp.training.tensorboard_writer:_precision/Hotels                           |     0.818  |     0.911\n",
      "INFO:allennlp.training.tensorboard_writer:_recall/Dienstleistungen                    |     0.716  |     0.955\n",
      "INFO:allennlp.training.tensorboard_writer:_fscore/Edv                                 |     0.591  |     0.753\n",
      "INFO:allennlp.training.tensorboard_writer:micro/recall                                |     0.734  |     0.889\n",
      "INFO:allennlp.training.tensorboard_writer:_fscore/Apotheken                           |     0.772  |     0.841\n",
      "INFO:allennlp.training.tensorboard_writer:_fscore/Gebrauchtwagen                      |     0.759  |     0.893\n",
      "INFO:allennlp.training.tensorboard_writer:_recall/Maler                               |     0.764  |     0.964\n",
      "INFO:allennlp.training.tensorboard_writer:_precision/Allgemeinärzte                   |     0.917  |     0.982\n",
      "INFO:allennlp.training.tensorboard_writer:_recall/Vermittlungen                       |     0.747  |     0.971\n",
      "INFO:allennlp.training.tensorboard_writer:_fscore/Elektriker                          |     0.818  |     0.925\n",
      "INFO:allennlp.training.tensorboard_writer:_fscore/Restaurants                         |     0.489  |     0.758\n",
      "INFO:allennlp.training.tensorboard_writer:accuracy                                    |     0.734  |     0.889\n",
      "INFO:allennlp.training.tensorboard_writer:_precision/Autowerkstätten                  |     0.728  |     0.881\n",
      "INFO:allennlp.training.tensorboard_writer:_recall/Elektriker                          |     0.798  |     0.964\n",
      "INFO:allennlp.training.tensorboard_writer:_precision/Werbeagenturen                   |     0.812  |     0.919\n",
      "INFO:allennlp.training.tensorboard_writer:_recall/Allgemeinärzte                      |     0.769  |     0.948\n",
      "INFO:allennlp.training.tensorboard_writer:_fscore/Vereine                             |     0.861  |     0.929\n",
      "INFO:allennlp.training.tensorboard_writer:_recall/Restaurants                         |     0.649  |     0.798\n",
      "INFO:allennlp.training.tensorboard_writer:_precision/Maler                            |     0.840  |     0.988\n",
      "INFO:allennlp.training.tensorboard_writer:_fscore/Maler                               |     0.800  |     0.976\n",
      "INFO:allennlp.training.tensorboard_writer:_fscore/Physiotherapie                      |     0.750  |     0.941\n",
      "INFO:allennlp.training.tensorboard_writer:_fscore/Architekturbüros                    |     0.847  |     0.976\n",
      "INFO:allennlp.training.tensorboard_writer:_recall/Architekturbüros                    |     0.894  |     0.972\n",
      "INFO:allennlp.training.tensorboard_writer:gpu_0_memory_MB                             |    26.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:loss                                        |     0.925  |     0.384\n",
      "INFO:allennlp.training.tensorboard_writer:_fscore/Allgemeinärzte                      |     0.836  |     0.965\n",
      "INFO:allennlp.training.tensorboard_writer:_recall/Tiefbau                             |     0.837  |     0.975\n",
      "INFO:allennlp.training.tensorboard_writer:_fscore/Versicherungsvermittler             |     0.777  |     0.961\n",
      "INFO:allennlp.training.tensorboard_writer:_fscore/Werbeagenturen                      |     0.730  |     0.877\n",
      "INFO:allennlp.training.tensorboard_writer:micro/precision                             |     0.734  |     0.889\n",
      "INFO:allennlp.training.checkpointer:Best validation performance so far. Copying weights to 'output/best.th'.\n",
      "INFO:allennlp.training.trainer:Epoch duration: 0:00:55.696340\n",
      "INFO:allennlp.training.checkpointer:loading best weights\n",
      "INFO:allennlp.models.archival:archiving weights and vocabulary to output/model.tar.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pl.train(\n",
    "    output=\"output\",\n",
    "    training=ds_train,\n",
    "    validation=ds_valid,\n",
    "    trainer=trainer_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: tip\n",
    "\n",
    "If for some reason the training gets interrupted, you can continue where you left off by setting the `restore` argument in the `Pipeline.train()` method to `True`. \n",
    "If you want to train your model for a few more epochs, you can also use the `restore` argument, but you have to modify the `epochs` argument in your `TrainerConfiguration` to reflect the total amount of epochs you aim for.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make your first predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we trained our model we can go on to make our first predictions.\n",
    "First we must load our trained model into a new `Pipeline`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "nbreg": {
     "diff_ignore": [
      "/outputs/0/text"
     ]
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 140282581614672 acquired on /tmp/tmpa8czwzha/vocabulary/.lock\n",
      "INFO:filelock:Lock 140282581614672 released on /tmp/tmpa8czwzha/vocabulary/.lock\n"
     ]
    }
   ],
   "source": [
    "pl_trained = Pipeline.from_pretrained(\"output/model.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then provide the input expected by our `TaskHead` of the model to the `Pipeline.predict()` method.\n",
    "In our case it is a `TextClassification` head that classifies a `text` input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "nbreg": {
     "diff_ignore": [
      "/outputs/0/data/text/plain"
     ]
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logits': array([ -1.583786  ,  -6.9499893 ,   0.07082307,   0.8625684 ,\n",
       "          9.981598  ,  -4.9741173 ,  -3.002375  , -15.961031  ,\n",
       "        -11.91686   ,  -7.106097  ,  -2.3369193 ,  -6.279768  ,\n",
       "         -2.2998297 ,  -8.338451  ,  -1.1450654 , -13.137184  ,\n",
       "         -1.1679922 ,   0.814244  ,   4.70496   ,  -3.7449305 ,\n",
       "        -14.830839  ,  -4.7980075 ], dtype=float32),\n",
       " 'probs': array([9.4377356e-06, 4.4091546e-08, 4.9369104e-05, 1.0896972e-04,\n",
       "        9.9460405e-01, 3.1802821e-07, 2.2844549e-06, 5.3815784e-12,\n",
       "        3.0709402e-10, 3.7718824e-08, 4.4441222e-06, 8.6184414e-08,\n",
       "        4.6120508e-06, 1.0999018e-08, 1.4635306e-05, 9.0633917e-11,\n",
       "        1.4303575e-05, 1.0382911e-04, 5.0820094e-03, 1.0871634e-06,\n",
       "        1.6662715e-11, 3.7927040e-07], dtype=float32),\n",
       " 'classes': {'Gebrauchtwagen': tensor(0.9946),\n",
       "  'Autowerkstätten': tensor(0.0051),\n",
       "  'Dienstleistungen': tensor(0.0001),\n",
       "  'Hotels': tensor(0.0001),\n",
       "  'Tiefbau': tensor(4.9369e-05),\n",
       "  'Apotheken': tensor(1.4635e-05),\n",
       "  'Vermittlungen': tensor(1.4304e-05),\n",
       "  'Unternehmensberatungen': tensor(9.4377e-06),\n",
       "  'Edv': tensor(4.6121e-06),\n",
       "  'Maler': tensor(4.4441e-06),\n",
       "  'Restaurants': tensor(2.2845e-06),\n",
       "  'Elektrotechnik': tensor(1.0872e-06),\n",
       "  'Handelsvermittler Und -vertreter': tensor(3.7927e-07),\n",
       "  'Elektriker': tensor(3.1803e-07),\n",
       "  'Sanitärinstallationen': tensor(8.6184e-08),\n",
       "  'Friseure': tensor(4.4092e-08),\n",
       "  'Versicherungsvermittler': tensor(3.7719e-08),\n",
       "  'Werbeagenturen': tensor(1.0999e-08),\n",
       "  'Vereine': tensor(3.0709e-10),\n",
       "  'Physiotherapie': tensor(9.0634e-11),\n",
       "  'Allgemeinärzte': tensor(1.6663e-11),\n",
       "  'Architekturbüros': tensor(5.3816e-12)},\n",
       " 'max_class': 'Gebrauchtwagen',\n",
       " 'max_class_prob': tensor(0.9946),\n",
       " 'label': 'Gebrauchtwagen',\n",
       " 'prob': tensor(0.9946)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl_trained.predict(text=\"Autohaus biome.text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned dictionary contains the logits and probabilities of all labels (classes).\n",
    "The label with the highest probability is stored under the `label` key, together with its probability under the `prob` key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: tip\n",
    "\n",
    "When configuring the pipeline in the first place, we recommend to check that it is correctly setup by using the `predict` method.\n",
    "Since the pipeline is still not trained at that moment, the predictions will be arbitrary.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the model's predictions\n",
    "\n",
    "To check and understand the predictions of the model, you can use the **biome.text explore UI**.\n",
    "For the UI to work you need a running [Elasticsearch]() instance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "nbreg": {
     "diff_ignore": [
      "/outputs/*"
     ]
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:biome.text.ui.ui:Running biome UI on http://0.0.0.0:9999 with elasticsearch backend http://localhost:9200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe src=http://localhost:9999/b0fcb09c-a67c-11ea-a172-74d83e8f6ee3 width=100% height=840></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>annotation</th>\n",
       "      <th>metadata</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=4</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: bulk_save, 4 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                 text   label annotation metadata\n",
       "npartitions=4                                    \n",
       "               object  object     object   object\n",
       "                  ...     ...        ...      ...\n",
       "                  ...     ...        ...      ...\n",
       "                  ...     ...        ...      ...\n",
       "                  ...     ...        ...      ...\n",
       "Dask Name: bulk_save, 4 tasks"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl_trained.explore(ds_valid)\n",
    "raise NotImplementedError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "nbreg": {
   "diff_ignore": [
    "/metadata/widgets",
    "/metadata/language_info",
    "/cells/*/execution_count"
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
