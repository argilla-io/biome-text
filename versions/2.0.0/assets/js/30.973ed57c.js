(window.webpackJsonp=window.webpackJsonp||[]).push([[30],{403:function(e,t,a){"use strict";a.r(t);var r=a(26),s=Object(r.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h1",{attrs:{id:"biome-text-featurizer"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#biome-text-featurizer"}},[e._v("#")]),e._v(" biome.text.featurizer "),a("Badge",{attrs:{text:"Module"}})],1),e._v(" "),a("div"),e._v(" "),a("div"),e._v(" "),a("pre",{staticClass:"title"},[a("h2",{attrs:{id:"inputfeaturizer"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#inputfeaturizer"}},[e._v("#")]),e._v(" InputFeaturizer "),a("Badge",{attrs:{text:"Class"}})],1),e._v("\n")]),e._v(" "),a("pre",{staticClass:"language-python"},[a("code",[e._v("\n"),a("span",{staticClass:"token keyword"},[e._v("class")]),e._v(" "),a("span",{staticClass:"ident"},[e._v("InputFeaturizer")]),e._v(" (tokenizer: "),a("a",{attrs:{title:"biome.text.tokenizer.Tokenizer",href:"tokenizer.html#biome.text.tokenizer.Tokenizer"}},[e._v("Tokenizer")]),e._v(", indexer: Dict[str, allennlp.data.token_indexers.token_indexer.TokenIndexer])"),e._v("\n")]),e._v("\n")]),e._v(" "),a("p",[e._v("Transforms input text (words and/or characters) into indexes and embedding vectors.")]),e._v(" "),a("p",[e._v("This class defines two input features, words and chars for embeddings at word and character level respectively.")]),e._v(" "),a("p",[e._v("You can provide additional features by manually specify "),a("code",[e._v("indexer")]),e._v(" and "),a("code",[e._v("embedder")]),e._v(" configurations within each\ninput feature.")]),e._v(" "),a("h2",{attrs:{id:"attributes"}},[e._v("Attributes")]),e._v(" "),a("dl",[a("dt",[a("strong",[a("code",[e._v("tokenizer")])]),e._v(" : "),a("code",[e._v("Tokenizer")])]),e._v(" "),a("dd",[e._v("Tokenizes the input depending on its type (str, List[str], Dict[str, Any])")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("indexer")])]),e._v(" : "),a("code",[e._v("Dict[str, TokenIdexer]")])]),e._v(" "),a("dd",[e._v("Features dictionary for token indexing. Built from "),a("code",[e._v("FeaturesConfiguration")])])]),e._v(" "),a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"instance-variables"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#instance-variables"}},[e._v("#")]),e._v(" Instance variables")]),e._v("\n")]),e._v(" "),a("dl",[a("dt",{attrs:{id:"biome.text.featurizer.InputFeaturizer.has_word_features"}},[a("code",{staticClass:"name"},[e._v("var "),a("span",{staticClass:"ident"},[e._v("has_word_features")]),e._v(" : bool")])]),e._v(" "),a("dd",[a("p",[e._v("Checks if word features are already configured as part of the featurization")])])])])}),[],!1,null,null,null);t.default=s.exports}}]);