{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to biome-text's documentation! Welcome Table of content Get started API","title":"Biome-text WTF?"},{"location":"#welcome-to-biome-texts-documentation","text":"Welcome","title":"Welcome to biome-text's documentation!"},{"location":"#table-of-content","text":"Get started API","title":"Table of content"},{"location":"get_started/","text":"Get started","title":"Get started"},{"location":"get_started/#get-started","text":"","title":"Get started"},{"location":"api/about/","text":"biome.text.about package_version package_version(version: str) Generates a proper package version from git repository info","title":"about"},{"location":"api/about/#biometextabout","text":"","title":"biome.text.about"},{"location":"api/about/#package_version","text":"package_version(version: str) Generates a proper package version from git repository info","title":"package_version"},{"location":"api/constants/","text":"biome.text.constants","title":"constants"},{"location":"api/constants/#biometextconstants","text":"","title":"biome.text.constants"},{"location":"api/environment/","text":"biome.text.environment","title":"environment"},{"location":"api/environment/#biometextenvironment","text":"","title":"biome.text.environment"},{"location":"api/helpers/","text":"biome.text.helpers get_compatible_doc_type get_compatible_doc_type(client: Elasticsearch) Find a compatible name for doc type by checking the cluster info Parameters client The elasticsearch client Returns A compatible name for doc type in function of cluster version","title":"helpers"},{"location":"api/helpers/#biometexthelpers","text":"","title":"biome.text.helpers"},{"location":"api/helpers/#get_compatible_doc_type","text":"get_compatible_doc_type(client: Elasticsearch) Find a compatible name for doc type by checking the cluster info Parameters client The elasticsearch client","title":"get_compatible_doc_type"},{"location":"api/helpers/#returns","text":"A compatible name for doc type in function of cluster version","title":"Returns"},{"location":"api/commands/explore/explore/","text":"biome.text.commands.explore.explore","title":"explore"},{"location":"api/commands/explore/explore/#biometextcommandsexploreexplore","text":"","title":"biome.text.commands.explore.explore"},{"location":"api/commands/learn/learn/","text":"biome.text.commands.learn.learn The train subcommand can be used to train a model. It requires a configuration file and a directory in which to write the results. .. code-block:: bash $ python -m allennlp.run train --help usage: run [command] train [-h] -s SERIALIZATION_DIR param_path Train the specified model on the specified dataset. positional arguments: param_path path to parameter file describing the model to be trained optional arguments: -h, --help show this help message and exit -s SERIALIZATION_DIR, --serialization-dir SERIALIZATION_DIR directory in which to save the model and its logs learn_from_args learn_from_args(args: Namespace) Launches a pipeline learn action with input command line arguments","title":"learn"},{"location":"api/commands/learn/learn/#biometextcommandslearnlearn","text":"The train subcommand can be used to train a model. It requires a configuration file and a directory in which to write the results. .. code-block:: bash $ python -m allennlp.run train --help usage: run [command] train [-h] -s SERIALIZATION_DIR param_path Train the specified model on the specified dataset. positional arguments: param_path path to parameter file describing the model to be trained optional arguments: -h, --help show this help message and exit -s SERIALIZATION_DIR, --serialization-dir SERIALIZATION_DIR directory in which to save the model and its logs","title":"biome.text.commands.learn.learn"},{"location":"api/commands/learn/learn/#learn_from_args","text":"learn_from_args(args: Namespace) Launches a pipeline learn action with input command line arguments","title":"learn_from_args"},{"location":"api/commands/serve/serve/","text":"biome.text.commands.serve.serve make_app make_app(binary: str, output: str = None) This function allows to serve a model from a gunicorn server. For example: gunicorn 'biome.allennlp.commands.serve.serve:make_app(\"/path/to/model.tar.gz\")' Parameters binary Path to the model.tar.gz file output Path to the output folder, in which to store the predictions. Returns app A Flask app used by gunicorn server","title":"serve"},{"location":"api/commands/serve/serve/#biometextcommandsserveserve","text":"","title":"biome.text.commands.serve.serve"},{"location":"api/commands/serve/serve/#make_app","text":"make_app(binary: str, output: str = None) This function allows to serve a model from a gunicorn server. For example: gunicorn 'biome.allennlp.commands.serve.serve:make_app(\"/path/to/model.tar.gz\")'","title":"make_app"},{"location":"api/commands/serve/serve/#parameters","text":"binary Path to the model.tar.gz file output Path to the output folder, in which to store the predictions.","title":"Parameters"},{"location":"api/commands/serve/serve/#returns","text":"app A Flask app used by gunicorn server","title":"Returns"},{"location":"api/commands/ui/app/","text":"biome.text.commands.ui.app","title":"app"},{"location":"api/commands/ui/app/#biometextcommandsuiapp","text":"","title":"biome.text.commands.ui.app"},{"location":"api/commands/ui/ui/","text":"biome.text.commands.ui.ui","title":"ui"},{"location":"api/commands/ui/ui/#biometextcommandsuiui","text":"","title":"biome.text.commands.ui.ui"},{"location":"api/dataset_readers/datasource_reader/","text":"biome.text.dataset_readers.datasource_reader DataSourceReader DataSourceReader( self, tokenizer: Tokenizer = None, token_indexers: typing.Dict[str, allennlp.data.token_indexers.token_indexer.TokenIndexer] = None, segment_sentences: typing.Union[bool, allennlp.data.tokenizers.sentence_splitter.SentenceSplitter] = False, as_text_field: bool = True, skip_empty_tokens: bool = False, max_sequence_length: int = None, max_nr_of_sentences: int = None, text_transforms: TextTransforms = None) A DataSetReader as base for read instances from DataSource The subclasses must implements their own way to transform input data to Instance in the text_to_field method Parameters tokenizer By default we use a WordTokenizer with the SpacyWordSplitter token_indexers By default we use the following dict {'tokens': SingleIdTokenIndexer} segment_sentences If True, we will first segment the text into sentences using SpaCy and then tokenize words. as_text_field Flag indicating how to generate the TextField . If enabled, the output Field will be a TextField with text concatenation, else the result field will be a ListField of TextField s, one per input data value skip_empty_tokens Should i silently skip empty tokens? max_sequence_length If you want to truncate the text input to a maximum number of characters max_nr_of_sentences Use only the first max_nr_of_sentences when segmenting the text into sentences text_transforms By default we use the as 'rm_spaces' registered class, which just removes useless, leading and trailing spaces from the text before embedding it in a TextField . signature Describe de input signature for the pipeline predictions Returns A list of expected inputs with information about if input is optional or nor. For example, for the signature >>def text_to_instance(a:str,b:str, c:str=None) This method will return: >>{\"a\":{\"optional\": False},\"b\":{\"optional\": False},\"c\":{\"optional\": True}} build_textfield DataSourceReader.build_textfield( data: typing.Union[str, typing.Iterable, dict]) Embeds the record in a TextField or ListField depending on the _as_text_field parameter. Parameters data Record to be embedded. Returns field Either a TextField or a ListField containing the record. Returns None if data is not a str or a dict. text_to_instance DataSourceReader.text_to_instance(**inputs) Convert an input text data into a allennlp Instance","title":"datasource_reader"},{"location":"api/dataset_readers/datasource_reader/#biometextdataset_readersdatasource_reader","text":"","title":"biome.text.dataset_readers.datasource_reader"},{"location":"api/dataset_readers/datasource_reader/#datasourcereader","text":"DataSourceReader( self, tokenizer: Tokenizer = None, token_indexers: typing.Dict[str, allennlp.data.token_indexers.token_indexer.TokenIndexer] = None, segment_sentences: typing.Union[bool, allennlp.data.tokenizers.sentence_splitter.SentenceSplitter] = False, as_text_field: bool = True, skip_empty_tokens: bool = False, max_sequence_length: int = None, max_nr_of_sentences: int = None, text_transforms: TextTransforms = None) A DataSetReader as base for read instances from DataSource The subclasses must implements their own way to transform input data to Instance in the text_to_field method","title":"DataSourceReader"},{"location":"api/dataset_readers/datasource_reader/#parameters","text":"tokenizer By default we use a WordTokenizer with the SpacyWordSplitter token_indexers By default we use the following dict {'tokens': SingleIdTokenIndexer} segment_sentences If True, we will first segment the text into sentences using SpaCy and then tokenize words. as_text_field Flag indicating how to generate the TextField . If enabled, the output Field will be a TextField with text concatenation, else the result field will be a ListField of TextField s, one per input data value skip_empty_tokens Should i silently skip empty tokens? max_sequence_length If you want to truncate the text input to a maximum number of characters max_nr_of_sentences Use only the first max_nr_of_sentences when segmenting the text into sentences text_transforms By default we use the as 'rm_spaces' registered class, which just removes useless, leading and trailing spaces from the text before embedding it in a TextField .","title":"Parameters"},{"location":"api/dataset_readers/datasource_reader/#signature","text":"Describe de input signature for the pipeline predictions","title":"signature"},{"location":"api/dataset_readers/datasource_reader/#returns","text":"A list of expected inputs with information about if input is optional or nor. For example, for the signature >>def text_to_instance(a:str,b:str, c:str=None) This method will return: >>{\"a\":{\"optional\": False},\"b\":{\"optional\": False},\"c\":{\"optional\": True}}","title":"Returns"},{"location":"api/dataset_readers/datasource_reader/#build_textfield","text":"DataSourceReader.build_textfield( data: typing.Union[str, typing.Iterable, dict]) Embeds the record in a TextField or ListField depending on the _as_text_field parameter.","title":"build_textfield"},{"location":"api/dataset_readers/datasource_reader/#parameters_1","text":"data Record to be embedded.","title":"Parameters"},{"location":"api/dataset_readers/datasource_reader/#returns_1","text":"field Either a TextField or a ListField containing the record. Returns None if data is not a str or a dict.","title":"Returns"},{"location":"api/dataset_readers/datasource_reader/#text_to_instance","text":"DataSourceReader.text_to_instance(**inputs) Convert an input text data into a allennlp Instance","title":"text_to_instance"},{"location":"api/dataset_readers/mixins/","text":"biome.text.dataset_readers.mixins CacheableMixin CacheableMixin() This CacheableMixin allow in memory cache mechanism get CacheableMixin.get(key) Get a value from cache by key set CacheableMixin.set(key, data) Set an cache entry","title":"mixins"},{"location":"api/dataset_readers/mixins/#biometextdataset_readersmixins","text":"","title":"biome.text.dataset_readers.mixins"},{"location":"api/dataset_readers/mixins/#cacheablemixin","text":"CacheableMixin() This CacheableMixin allow in memory cache mechanism","title":"CacheableMixin"},{"location":"api/dataset_readers/mixins/#get","text":"CacheableMixin.get(key) Get a value from cache by key","title":"get"},{"location":"api/dataset_readers/mixins/#set","text":"CacheableMixin.set(key, data) Set an cache entry","title":"set"},{"location":"api/dataset_readers/sequence_classifier_dataset_reader/","text":"biome.text.dataset_readers.sequence_classifier_dataset_reader SequenceClassifierReader SequenceClassifierReader( self, tokenizer: Tokenizer = None, token_indexers: typing.Dict[str, allennlp.data.token_indexers.token_indexer.TokenIndexer] = None, segment_sentences: typing.Union[bool, allennlp.data.tokenizers.sentence_splitter.SentenceSplitter] = False, as_text_field: bool = True, skip_empty_tokens: bool = False, max_sequence_length: int = None, max_nr_of_sentences: int = None, text_transforms: TextTransforms = None) A DatasetReader for the SequenceClassifier model. text_to_instance SequenceClassifierReader.text_to_instance( tokens: typing.Union[str, typing.List[str], dict], label: typing.Union[str, NoneType] = None) Extracts the forward parameters from the example and transforms them to an Instance Parameters tokens The input tokens key,values (or the text string) label The label value Returns instance Returns None if cannot generate an new Instance.","title":"sequence_classifier_dataset_reader"},{"location":"api/dataset_readers/sequence_classifier_dataset_reader/#biometextdataset_readerssequence_classifier_dataset_reader","text":"","title":"biome.text.dataset_readers.sequence_classifier_dataset_reader"},{"location":"api/dataset_readers/sequence_classifier_dataset_reader/#sequenceclassifierreader","text":"SequenceClassifierReader( self, tokenizer: Tokenizer = None, token_indexers: typing.Dict[str, allennlp.data.token_indexers.token_indexer.TokenIndexer] = None, segment_sentences: typing.Union[bool, allennlp.data.tokenizers.sentence_splitter.SentenceSplitter] = False, as_text_field: bool = True, skip_empty_tokens: bool = False, max_sequence_length: int = None, max_nr_of_sentences: int = None, text_transforms: TextTransforms = None) A DatasetReader for the SequenceClassifier model.","title":"SequenceClassifierReader"},{"location":"api/dataset_readers/sequence_classifier_dataset_reader/#text_to_instance","text":"SequenceClassifierReader.text_to_instance( tokens: typing.Union[str, typing.List[str], dict], label: typing.Union[str, NoneType] = None) Extracts the forward parameters from the example and transforms them to an Instance","title":"text_to_instance"},{"location":"api/dataset_readers/sequence_classifier_dataset_reader/#parameters","text":"tokens The input tokens key,values (or the text string) label The label value","title":"Parameters"},{"location":"api/dataset_readers/sequence_classifier_dataset_reader/#returns","text":"instance Returns None if cannot generate an new Instance.","title":"Returns"},{"location":"api/dataset_readers/sequence_pair_classifier_dataset_reader/","text":"biome.text.dataset_readers.sequence_pair_classifier_dataset_reader SequencePairClassifierReader SequencePairClassifierReader( self, tokenizer: Tokenizer = None, token_indexers: typing.Dict[str, allennlp.data.token_indexers.token_indexer.TokenIndexer] = None, segment_sentences: typing.Union[bool, allennlp.data.tokenizers.sentence_splitter.SentenceSplitter] = False, as_text_field: bool = True, skip_empty_tokens: bool = False, max_sequence_length: int = None, max_nr_of_sentences: int = None, text_transforms: TextTransforms = None) A DatasetReader for the SequencePairClassifier model.","title":"sequence_pair_classifier_dataset_reader"},{"location":"api/dataset_readers/sequence_pair_classifier_dataset_reader/#biometextdataset_readerssequence_pair_classifier_dataset_reader","text":"","title":"biome.text.dataset_readers.sequence_pair_classifier_dataset_reader"},{"location":"api/dataset_readers/sequence_pair_classifier_dataset_reader/#sequencepairclassifierreader","text":"SequencePairClassifierReader( self, tokenizer: Tokenizer = None, token_indexers: typing.Dict[str, allennlp.data.token_indexers.token_indexer.TokenIndexer] = None, segment_sentences: typing.Union[bool, allennlp.data.tokenizers.sentence_splitter.SentenceSplitter] = False, as_text_field: bool = True, skip_empty_tokens: bool = False, max_sequence_length: int = None, max_nr_of_sentences: int = None, text_transforms: TextTransforms = None) A DatasetReader for the SequencePairClassifier model.","title":"SequencePairClassifierReader"},{"location":"api/dataset_readers/text_transforms/","text":"biome.text.dataset_readers.text_transforms TextTransforms TextTransforms(self, rules: typing.List[str] = None) This class defines some rules that can be applied to the text before it gets embedded in a TextField . Each rule is a simple python class method that receives and returns a str. It will be applied when an instance of this class is called. Parameters rules A list of class method names to be applied on calling the instance. Attributes DEFAULT_RULES The default rules if the rules parameter is not provided. default_implementation DEFAULT_RULES RmSpacesTransforms RmSpacesTransforms(self, rules: typing.List[str] = None) DEFAULT_RULES strip_spaces RmSpacesTransforms.strip_spaces(text: str) Strip leading and trailing spaces/new lines rm_useless_spaces RmSpacesTransforms.rm_useless_spaces(text: str) Remove multiple spaces in text Html2TextTransforms Html2TextTransforms(self, rules: typing.List[str] = None) DEFAULT_RULES fix_html Html2TextTransforms.fix_html(text: str) list of replacements in html code. I leave a link to the fastai version here as a reference: https://docs.fast.ai/text.transform.html#fix_html html_to_text Html2TextTransforms.html_to_text(text: str) Extract text from a html doc with BeautifulSoup4","title":"text_transforms"},{"location":"api/dataset_readers/text_transforms/#biometextdataset_readerstext_transforms","text":"","title":"biome.text.dataset_readers.text_transforms"},{"location":"api/dataset_readers/text_transforms/#texttransforms","text":"TextTransforms(self, rules: typing.List[str] = None) This class defines some rules that can be applied to the text before it gets embedded in a TextField . Each rule is a simple python class method that receives and returns a str. It will be applied when an instance of this class is called.","title":"TextTransforms"},{"location":"api/dataset_readers/text_transforms/#parameters","text":"rules A list of class method names to be applied on calling the instance.","title":"Parameters"},{"location":"api/dataset_readers/text_transforms/#attributes","text":"DEFAULT_RULES The default rules if the rules parameter is not provided.","title":"Attributes"},{"location":"api/dataset_readers/text_transforms/#default_implementation","text":"","title":"default_implementation"},{"location":"api/dataset_readers/text_transforms/#default_rules","text":"","title":"DEFAULT_RULES"},{"location":"api/dataset_readers/text_transforms/#rmspacestransforms","text":"RmSpacesTransforms(self, rules: typing.List[str] = None)","title":"RmSpacesTransforms"},{"location":"api/dataset_readers/text_transforms/#default_rules_1","text":"","title":"DEFAULT_RULES"},{"location":"api/dataset_readers/text_transforms/#strip_spaces","text":"RmSpacesTransforms.strip_spaces(text: str) Strip leading and trailing spaces/new lines","title":"strip_spaces"},{"location":"api/dataset_readers/text_transforms/#rm_useless_spaces","text":"RmSpacesTransforms.rm_useless_spaces(text: str) Remove multiple spaces in text","title":"rm_useless_spaces"},{"location":"api/dataset_readers/text_transforms/#html2texttransforms","text":"Html2TextTransforms(self, rules: typing.List[str] = None)","title":"Html2TextTransforms"},{"location":"api/dataset_readers/text_transforms/#default_rules_2","text":"","title":"DEFAULT_RULES"},{"location":"api/dataset_readers/text_transforms/#fix_html","text":"Html2TextTransforms.fix_html(text: str) list of replacements in html code. I leave a link to the fastai version here as a reference: https://docs.fast.ai/text.transform.html#fix_html","title":"fix_html"},{"location":"api/dataset_readers/text_transforms/#html_to_text","text":"Html2TextTransforms.html_to_text(text: str) Extract text from a html doc with BeautifulSoup4","title":"html_to_text"},{"location":"api/interpreters/integrated_gradient/","text":"biome.text.interpreters.integrated_gradient IntegratedGradient IntegratedGradient(self, predictor: Predictor) Interprets the prediction using Integrated Gradients (https://arxiv.org/abs/1703.01365)","title":"integrated_gradient"},{"location":"api/interpreters/integrated_gradient/#biometextinterpretersintegrated_gradient","text":"","title":"biome.text.interpreters.integrated_gradient"},{"location":"api/interpreters/integrated_gradient/#integratedgradient","text":"IntegratedGradient(self, predictor: Predictor) Interprets the prediction using Integrated Gradients (https://arxiv.org/abs/1703.01365)","title":"IntegratedGradient"},{"location":"api/models/archival/","text":"biome.text.models.archival to_local_archive to_local_archive(archive_file: str) Wraps archive download to support remote locations (s3, hdfs,...)","title":"archival"},{"location":"api/models/archival/#biometextmodelsarchival","text":"","title":"biome.text.models.archival"},{"location":"api/models/archival/#to_local_archive","text":"to_local_archive(archive_file: str) Wraps archive download to support remote locations (s3, hdfs,...)","title":"to_local_archive"},{"location":"api/models/biome_bimpm/","text":"biome.text.models.biome_bimpm BiomeBiMpm BiomeBiMpm( self, vocab: Vocabulary, text_field_embedder: TextFieldEmbedder, matcher_word: BiMpmMatching, encoder1: Seq2SeqEncoder, matcher_forward1: BiMpmMatching, matcher_backward1: BiMpmMatching, encoder2: Seq2SeqEncoder, matcher_forward2: BiMpmMatching, matcher_backward2: BiMpmMatching, aggregator: Seq2VecEncoder, classifier_feedforward: FeedForward, dropout: float = 0.1, initializer: InitializerApplicator = <allennlp.nn.initializers.InitializerApplicator object at 0x13b5e7e10>, regularizer: typing.Union[allennlp.nn.regularizers.regularizer_applicator.RegularizerApplicator, NoneType] = None, accuracy: typing.Union[allennlp.training.metrics.categorical_accuracy.CategoricalAccuracy, NoneType] = None ) This Model implements BiMPM model described in Bilateral Multi-Perspective Matching for Natural Language Sentences <https://arxiv.org/abs/1702.03814> by Zhiguo Wang et al., 2017. Also please refer to the TensorFlow implementation <https://github.com/zhiguowang/BiMPM/> and PyTorch implementation <https://github.com/galsang/BIMPM-pytorch> _. Parameters vocab : Vocabulary text_field_embedder : TextFieldEmbedder Used to embed the premise and hypothesis TextFields we get as input to the model. matcher_word : BiMpmMatching BiMPM matching on the output of word embeddings of premise and hypothesis. encoder1 : Seq2SeqEncoder First encoder layer for the premise and hypothesis matcher_forward1 : BiMPMMatching BiMPM matching for the forward output of first encoder layer matcher_backward1 : BiMPMMatching BiMPM matching for the backward output of first encoder layer encoder2 : Seq2SeqEncoder Second encoder layer for the premise and hypothesis matcher_forward2 : BiMPMMatching BiMPM matching for the forward output of second encoder layer matcher_backward2 : BiMPMMatching BiMPM matching for the backward output of second encoder layer aggregator : Seq2VecEncoder Aggregator of all BiMPM matching vectors classifier_feedforward : FeedForward Fully connected layers for classification. dropout : float , optional (default=0.1) Dropout percentage to use. initializer : InitializerApplicator , optional (default= InitializerApplicator() ) If provided, will be used to initialize the model parameters. regularizer : RegularizerApplicator , optional (default= None ) If provided, will be used to calculate the regularization penalty during training. accuracy The accuracy you want to use. By default, we choose a categorical top-1 accuracy. forward BiomeBiMpm.forward(record1: typing.Dict[str, torch.LongTensor], record2: typing.Dict[str, torch.LongTensor], label: Tensor = None) Parameters premise : Dict[str, torch.LongTensor] The premise from a TextField hypothesis : Dict[str, torch.LongTensor] The hypothesis from a TextField label : torch.LongTensor, optional (default = None) The label for the pair of the premise and the hypothesis metadata : List[Dict[str, Any]] , optional, (default = None) Additional information about the pair Returns An output dictionary consisting of: logits : torch.FloatTensor A tensor of shape (batch_size, num_labels) representing unnormalised log probabilities of the entailment label. loss : torch.FloatTensor, optional A scalar loss to be optimised.","title":"biome_bimpm"},{"location":"api/models/biome_bimpm/#biometextmodelsbiome_bimpm","text":"","title":"biome.text.models.biome_bimpm"},{"location":"api/models/biome_bimpm/#biomebimpm","text":"BiomeBiMpm( self, vocab: Vocabulary, text_field_embedder: TextFieldEmbedder, matcher_word: BiMpmMatching, encoder1: Seq2SeqEncoder, matcher_forward1: BiMpmMatching, matcher_backward1: BiMpmMatching, encoder2: Seq2SeqEncoder, matcher_forward2: BiMpmMatching, matcher_backward2: BiMpmMatching, aggregator: Seq2VecEncoder, classifier_feedforward: FeedForward, dropout: float = 0.1, initializer: InitializerApplicator = <allennlp.nn.initializers.InitializerApplicator object at 0x13b5e7e10>, regularizer: typing.Union[allennlp.nn.regularizers.regularizer_applicator.RegularizerApplicator, NoneType] = None, accuracy: typing.Union[allennlp.training.metrics.categorical_accuracy.CategoricalAccuracy, NoneType] = None ) This Model implements BiMPM model described in Bilateral Multi-Perspective Matching for Natural Language Sentences <https://arxiv.org/abs/1702.03814> by Zhiguo Wang et al., 2017. Also please refer to the TensorFlow implementation <https://github.com/zhiguowang/BiMPM/> and PyTorch implementation <https://github.com/galsang/BIMPM-pytorch> _.","title":"BiomeBiMpm"},{"location":"api/models/biome_bimpm/#parameters","text":"vocab : Vocabulary text_field_embedder : TextFieldEmbedder Used to embed the premise and hypothesis TextFields we get as input to the model. matcher_word : BiMpmMatching BiMPM matching on the output of word embeddings of premise and hypothesis. encoder1 : Seq2SeqEncoder First encoder layer for the premise and hypothesis matcher_forward1 : BiMPMMatching BiMPM matching for the forward output of first encoder layer matcher_backward1 : BiMPMMatching BiMPM matching for the backward output of first encoder layer encoder2 : Seq2SeqEncoder Second encoder layer for the premise and hypothesis matcher_forward2 : BiMPMMatching BiMPM matching for the forward output of second encoder layer matcher_backward2 : BiMPMMatching BiMPM matching for the backward output of second encoder layer aggregator : Seq2VecEncoder Aggregator of all BiMPM matching vectors classifier_feedforward : FeedForward Fully connected layers for classification. dropout : float , optional (default=0.1) Dropout percentage to use. initializer : InitializerApplicator , optional (default= InitializerApplicator() ) If provided, will be used to initialize the model parameters. regularizer : RegularizerApplicator , optional (default= None ) If provided, will be used to calculate the regularization penalty during training. accuracy The accuracy you want to use. By default, we choose a categorical top-1 accuracy.","title":"Parameters"},{"location":"api/models/biome_bimpm/#forward","text":"BiomeBiMpm.forward(record1: typing.Dict[str, torch.LongTensor], record2: typing.Dict[str, torch.LongTensor], label: Tensor = None)","title":"forward"},{"location":"api/models/biome_bimpm/#parameters_1","text":"premise : Dict[str, torch.LongTensor] The premise from a TextField hypothesis : Dict[str, torch.LongTensor] The hypothesis from a TextField label : torch.LongTensor, optional (default = None) The label for the pair of the premise and the hypothesis metadata : List[Dict[str, Any]] , optional, (default = None) Additional information about the pair Returns An output dictionary consisting of: logits : torch.FloatTensor A tensor of shape (batch_size, num_labels) representing unnormalised log probabilities of the entailment label. loss : torch.FloatTensor, optional A scalar loss to be optimised.","title":"Parameters"},{"location":"api/models/language_model/","text":"biome.text.models.language_model","title":"language_model"},{"location":"api/models/language_model/#biometextmodelslanguage_model","text":"","title":"biome.text.models.language_model"},{"location":"api/models/mixins/","text":"biome.text.models.mixins BiomeClassifierMixin BiomeClassifierMixin( self, vocab, accuracy: typing.Union[allennlp.training.metrics.categorical_accuracy.CategoricalAccuracy, NoneType] = None, **kwargs) A mixin class for biome classifiers. Inheriting from this class allows you to use Biome's awesome UIs. It standardizes the decode and get_metrics methods. Some stuff to be aware of: - make sure your forward's output_dict has a \"class_probability\" key - use the _biome_classifier_metrics dict in the forward method to record the metrics - the forward signature must be compatible with the text_to_instance method of your DataReader - the decode and get_metrics methods override the allennlp.models.model.Model methods Parameters vocab Used to initiate the F1 measures for each label. It is also passed on to the model. accuracy The accuracy you want to use. By default, we choose a categorical top-1 accuracy. kwargs Passed on to the model class init Examples An example of how to implement an AllenNLP model in biome-text to be able to use Biome's UIs: from allennlp.models.bert_for_classification import BertForClassification @Model.register(\"biome_bert_classifier\") class BiomeBertClassifier(BiomeClassifierMixin, BertForClassification): def init (self, vocab, bert_model, num_labels, index, label_namespace, trainable, initializer, regularizer, accuracy): super(). init (accuracy=accuracy, vocab=vocab, bert_model=bert_model, num_labels=num_labels, index=index, label_namespace=label_namespace, trainable=trainable, initializer=initializer, regularizer=regularizer) @overrides def forward(self, tokens, label = None): output_dict = super().forward(tokens=tokens, label=label) output_dict[\"class_probabilities\"] = output_dict.pop(\"probs\") if label is not None: for metric in self._biome_classifier_metrics.values(): metric(logits, label) return output_dict decode BiomeClassifierMixin.decode(output_dict: typing.Dict[str, torch.Tensor]) Does a simple position-wise argmax over each token, converts indices to string labels, and adds a \"tags\" key to the dictionary with the result. get_metrics BiomeClassifierMixin.get_metrics(reset: bool = False) Get the metrics of our classifier, see :func: ~allennlp_2.models.Model.get_metrics . Parameters reset Reset the metrics after obtaining them? Returns A dictionary with all metric names and values.","title":"mixins"},{"location":"api/models/mixins/#biometextmodelsmixins","text":"","title":"biome.text.models.mixins"},{"location":"api/models/mixins/#biomeclassifiermixin","text":"BiomeClassifierMixin( self, vocab, accuracy: typing.Union[allennlp.training.metrics.categorical_accuracy.CategoricalAccuracy, NoneType] = None, **kwargs) A mixin class for biome classifiers. Inheriting from this class allows you to use Biome's awesome UIs. It standardizes the decode and get_metrics methods. Some stuff to be aware of: - make sure your forward's output_dict has a \"class_probability\" key - use the _biome_classifier_metrics dict in the forward method to record the metrics - the forward signature must be compatible with the text_to_instance method of your DataReader - the decode and get_metrics methods override the allennlp.models.model.Model methods","title":"BiomeClassifierMixin"},{"location":"api/models/mixins/#parameters","text":"vocab Used to initiate the F1 measures for each label. It is also passed on to the model. accuracy The accuracy you want to use. By default, we choose a categorical top-1 accuracy. kwargs Passed on to the model class init","title":"Parameters"},{"location":"api/models/mixins/#examples","text":"An example of how to implement an AllenNLP model in biome-text to be able to use Biome's UIs: from allennlp.models.bert_for_classification import BertForClassification @Model.register(\"biome_bert_classifier\") class BiomeBertClassifier(BiomeClassifierMixin, BertForClassification): def init (self, vocab, bert_model, num_labels, index, label_namespace, trainable, initializer, regularizer, accuracy): super(). init (accuracy=accuracy, vocab=vocab, bert_model=bert_model, num_labels=num_labels, index=index, label_namespace=label_namespace, trainable=trainable, initializer=initializer, regularizer=regularizer) @overrides def forward(self, tokens, label = None): output_dict = super().forward(tokens=tokens, label=label) output_dict[\"class_probabilities\"] = output_dict.pop(\"probs\") if label is not None: for metric in self._biome_classifier_metrics.values(): metric(logits, label) return output_dict","title":"Examples"},{"location":"api/models/mixins/#decode","text":"BiomeClassifierMixin.decode(output_dict: typing.Dict[str, torch.Tensor]) Does a simple position-wise argmax over each token, converts indices to string labels, and adds a \"tags\" key to the dictionary with the result.","title":"decode"},{"location":"api/models/mixins/#get_metrics","text":"BiomeClassifierMixin.get_metrics(reset: bool = False) Get the metrics of our classifier, see :func: ~allennlp_2.models.Model.get_metrics .","title":"get_metrics"},{"location":"api/models/mixins/#parameters_1","text":"reset Reset the metrics after obtaining them?","title":"Parameters"},{"location":"api/models/mixins/#returns","text":"A dictionary with all metric names and values.","title":"Returns"},{"location":"api/models/multifield_bimpm/","text":"biome.text.models.multifield_bimpm MultifieldBiMpm MultifieldBiMpm( self, vocab: Vocabulary, text_field_embedder: TextFieldEmbedder, matcher_word: BiMpmMatching, encoder: Seq2SeqEncoder, matcher_forward: BiMpmMatching, aggregator: Seq2VecEncoder, classifier_feedforward: FeedForward, matcher_backward: BiMpmMatching = None, encoder2: Seq2SeqEncoder = None, matcher2_forward: BiMpmMatching = None, matcher2_backward: BiMpmMatching = None, dropout: float = 0.1, multifield: bool = True, initializer: InitializerApplicator = <allennlp.nn.initializers.InitializerApplicator object at 0x13982b3d0>, regularizer: typing.Union[allennlp.nn.regularizers.regularizer_applicator.RegularizerApplicator, NoneType] = None, accuracy: typing.Union[allennlp.training.metrics.categorical_accuracy.CategoricalAccuracy, NoneType] = None ) This Model is a version of AllenNLPs implementation of the BiMPM model described in Bilateral Multi-Perspective Matching for Natural Language Sentences <https://arxiv.org/abs/1702.03814> _ by Zhiguo Wang et al., 2017. This version adds the feature of being compatible with multiple inputs for the two records. The matching will be done for all possible combinations between the two records, that is: (r1_1, r2_1), (r1_1, r2_2), ..., (r1_2, r2_1), (r1_2, r2_2), ... This version also allows you to apply only one encoder, and to leave out the backward matching. Parameters vocab : Vocabulary text_field_embedder : TextFieldEmbedder Used to embed the record1 and record2 TextFields we get as input to the model. matcher_word : BiMpmMatching BiMPM matching on the output of word embeddings of record1 and record2. encoder : Seq2SeqEncoder Encoder layer for record1 and record2 matcher_forward : BiMPMMatching BiMPM matching for the forward output of the encoder layer aggregator : Seq2VecEncoder Aggregator of all BiMPM matching vectors classifier_feedforward : FeedForward Fully connected layers for classification. A linear output layer with the number of labels at the end will be added automatically!!! matcher_backward : BiMPMMatching , optional BiMPM matching for the backward output of the encoder layer encoder2 : Seq2SeqEncoder , optional Encoder layer for encoded record1 and encoded record2 matcher2_forward : BiMPMMatching , optional BiMPM matching for the forward output of the second encoder layer matcher2_backward : BiMPMMatching , optional BiMPM matching for the backward output of the second encoder layer dropout : float , optional (default=0.1) Dropout percentage to use. multifield : bool , optional (default=False) Are there multiple inputs for each record, that is do the inputs come from ListField s? initializer : InitializerApplicator , optional (default= InitializerApplicator() ) If provided, will be used to initialize the model parameters. regularizer : RegularizerApplicator , optional (default= None ) If provided, will be used to calculate the regularization penalty during training. accuracy The accuracy you want to use. By default, we choose a categorical top-1 accuracy. forward MultifieldBiMpm.forward(record1: typing.Dict[str, torch.LongTensor], record2: typing.Dict[str, torch.LongTensor], label: LongTensor = None) Parameters record1 The first input tokens. The dictionary is the output of a TextField.as_array() . It gives names to the tensors created by the TokenIndexer s. In its most basic form, using a SingleIdTokenIndexer , the dictionary is composed of: {\"tokens\": Tensor(batch_size, num_tokens)} . The keys of the dictionary are defined in the model.yml input. The dictionary is designed to be passed on directly to a TextFieldEmbedder , that has a TokenEmbedder for each key in the dictionary (except you set allow_unmatched_keys in the TextFieldEmbedder to False) and knows how to combine different word/character representations into a single vector per token in your input. record2 The second input tokens. label : torch.LongTensor, optional (default = None) A torch tensor representing the sequence of integer gold class label of shape (batch_size, num_classes) . Returns An output dictionary consisting of: logits : torch.FloatTensor A tensor of shape (batch_size, num_tokens, tag_vocab_size) representing unnormalised log probabilities of the tag classes. class_probabilities : torch.FloatTensor A tensor of shape (batch_size, num_tokens, tag_vocab_size) representing a distribution of the tag classes per word. loss : torch.FloatTensor, optional A scalar loss to be optimised.","title":"multifield_bimpm"},{"location":"api/models/multifield_bimpm/#biometextmodelsmultifield_bimpm","text":"","title":"biome.text.models.multifield_bimpm"},{"location":"api/models/multifield_bimpm/#multifieldbimpm","text":"MultifieldBiMpm( self, vocab: Vocabulary, text_field_embedder: TextFieldEmbedder, matcher_word: BiMpmMatching, encoder: Seq2SeqEncoder, matcher_forward: BiMpmMatching, aggregator: Seq2VecEncoder, classifier_feedforward: FeedForward, matcher_backward: BiMpmMatching = None, encoder2: Seq2SeqEncoder = None, matcher2_forward: BiMpmMatching = None, matcher2_backward: BiMpmMatching = None, dropout: float = 0.1, multifield: bool = True, initializer: InitializerApplicator = <allennlp.nn.initializers.InitializerApplicator object at 0x13982b3d0>, regularizer: typing.Union[allennlp.nn.regularizers.regularizer_applicator.RegularizerApplicator, NoneType] = None, accuracy: typing.Union[allennlp.training.metrics.categorical_accuracy.CategoricalAccuracy, NoneType] = None ) This Model is a version of AllenNLPs implementation of the BiMPM model described in Bilateral Multi-Perspective Matching for Natural Language Sentences <https://arxiv.org/abs/1702.03814> _ by Zhiguo Wang et al., 2017. This version adds the feature of being compatible with multiple inputs for the two records. The matching will be done for all possible combinations between the two records, that is: (r1_1, r2_1), (r1_1, r2_2), ..., (r1_2, r2_1), (r1_2, r2_2), ... This version also allows you to apply only one encoder, and to leave out the backward matching.","title":"MultifieldBiMpm"},{"location":"api/models/multifield_bimpm/#parameters","text":"vocab : Vocabulary text_field_embedder : TextFieldEmbedder Used to embed the record1 and record2 TextFields we get as input to the model. matcher_word : BiMpmMatching BiMPM matching on the output of word embeddings of record1 and record2. encoder : Seq2SeqEncoder Encoder layer for record1 and record2 matcher_forward : BiMPMMatching BiMPM matching for the forward output of the encoder layer aggregator : Seq2VecEncoder Aggregator of all BiMPM matching vectors classifier_feedforward : FeedForward Fully connected layers for classification. A linear output layer with the number of labels at the end will be added automatically!!! matcher_backward : BiMPMMatching , optional BiMPM matching for the backward output of the encoder layer encoder2 : Seq2SeqEncoder , optional Encoder layer for encoded record1 and encoded record2 matcher2_forward : BiMPMMatching , optional BiMPM matching for the forward output of the second encoder layer matcher2_backward : BiMPMMatching , optional BiMPM matching for the backward output of the second encoder layer dropout : float , optional (default=0.1) Dropout percentage to use. multifield : bool , optional (default=False) Are there multiple inputs for each record, that is do the inputs come from ListField s? initializer : InitializerApplicator , optional (default= InitializerApplicator() ) If provided, will be used to initialize the model parameters. regularizer : RegularizerApplicator , optional (default= None ) If provided, will be used to calculate the regularization penalty during training. accuracy The accuracy you want to use. By default, we choose a categorical top-1 accuracy.","title":"Parameters"},{"location":"api/models/multifield_bimpm/#forward","text":"MultifieldBiMpm.forward(record1: typing.Dict[str, torch.LongTensor], record2: typing.Dict[str, torch.LongTensor], label: LongTensor = None)","title":"forward"},{"location":"api/models/multifield_bimpm/#parameters_1","text":"record1 The first input tokens. The dictionary is the output of a TextField.as_array() . It gives names to the tensors created by the TokenIndexer s. In its most basic form, using a SingleIdTokenIndexer , the dictionary is composed of: {\"tokens\": Tensor(batch_size, num_tokens)} . The keys of the dictionary are defined in the model.yml input. The dictionary is designed to be passed on directly to a TextFieldEmbedder , that has a TokenEmbedder for each key in the dictionary (except you set allow_unmatched_keys in the TextFieldEmbedder to False) and knows how to combine different word/character representations into a single vector per token in your input. record2 The second input tokens. label : torch.LongTensor, optional (default = None) A torch tensor representing the sequence of integer gold class label of shape (batch_size, num_classes) .","title":"Parameters"},{"location":"api/models/multifield_bimpm/#returns","text":"An output dictionary consisting of: logits : torch.FloatTensor A tensor of shape (batch_size, num_tokens, tag_vocab_size) representing unnormalised log probabilities of the tag classes. class_probabilities : torch.FloatTensor A tensor of shape (batch_size, num_tokens, tag_vocab_size) representing a distribution of the tag classes per word. loss : torch.FloatTensor, optional A scalar loss to be optimised.","title":"Returns"},{"location":"api/models/sequence_classifier/","text":"biome.text.models.sequence_classifier SequenceClassifier SequenceClassifier( self, vocab: Vocabulary, text_field_embedder: TextFieldEmbedder, seq2vec_encoder: Seq2VecEncoder, seq2seq_encoder: typing.Union[allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder, NoneType] = None, multifield_seq2seq_encoder: typing.Union[allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder, NoneType] = None, multifield_seq2vec_encoder: typing.Union[allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder, NoneType] = None, feed_forward: typing.Union[allennlp.modules.feedforward.FeedForward, NoneType] = None, dropout: typing.Union[float, NoneType] = None, multifield_dropout: typing.Union[float, NoneType] = None, initializer: typing.Union[allennlp.nn.initializers.InitializerApplicator, NoneType] = None, regularizer: typing.Union[allennlp.nn.regularizers.regularizer_applicator.RegularizerApplicator, NoneType] = None, accuracy: typing.Union[allennlp.training.metrics.categorical_accuracy.CategoricalAccuracy, NoneType] = None, loss_weights: typing.Dict[str, float] = None) forward SequenceClassifier.forward(tokens: typing.Dict[str, torch.Tensor], label: Tensor = None) Parameters tokens The input tokens. The dictionary is the output of a TextField.as_array() . It gives names to the tensors created by the TokenIndexer s. In its most basic form, using a SingleIdTokenIndexer , the dictionary is composed of: {\"tokens\": Tensor(batch_size, num_tokens)} . The keys of the dictionary are defined in the model.yml input. The dictionary is designed to be passed on directly to a TextFieldEmbedder , that has a TokenEmbedder for each key in the dictionary (except you set allow_unmatched_keys in the TextFieldEmbedder to False) and knows how to combine different word/character representations into a single vector per token in your input. label A torch tensor representing the sequence of integer gold class label of shape (batch_size, num_classes) .","title":"sequence_classifier"},{"location":"api/models/sequence_classifier/#biometextmodelssequence_classifier","text":"","title":"biome.text.models.sequence_classifier"},{"location":"api/models/sequence_classifier/#sequenceclassifier","text":"SequenceClassifier( self, vocab: Vocabulary, text_field_embedder: TextFieldEmbedder, seq2vec_encoder: Seq2VecEncoder, seq2seq_encoder: typing.Union[allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder, NoneType] = None, multifield_seq2seq_encoder: typing.Union[allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder, NoneType] = None, multifield_seq2vec_encoder: typing.Union[allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder, NoneType] = None, feed_forward: typing.Union[allennlp.modules.feedforward.FeedForward, NoneType] = None, dropout: typing.Union[float, NoneType] = None, multifield_dropout: typing.Union[float, NoneType] = None, initializer: typing.Union[allennlp.nn.initializers.InitializerApplicator, NoneType] = None, regularizer: typing.Union[allennlp.nn.regularizers.regularizer_applicator.RegularizerApplicator, NoneType] = None, accuracy: typing.Union[allennlp.training.metrics.categorical_accuracy.CategoricalAccuracy, NoneType] = None, loss_weights: typing.Dict[str, float] = None)","title":"SequenceClassifier"},{"location":"api/models/sequence_classifier/#forward","text":"SequenceClassifier.forward(tokens: typing.Dict[str, torch.Tensor], label: Tensor = None)","title":"forward"},{"location":"api/models/sequence_classifier/#parameters","text":"tokens The input tokens. The dictionary is the output of a TextField.as_array() . It gives names to the tensors created by the TokenIndexer s. In its most basic form, using a SingleIdTokenIndexer , the dictionary is composed of: {\"tokens\": Tensor(batch_size, num_tokens)} . The keys of the dictionary are defined in the model.yml input. The dictionary is designed to be passed on directly to a TextFieldEmbedder , that has a TokenEmbedder for each key in the dictionary (except you set allow_unmatched_keys in the TextFieldEmbedder to False) and knows how to combine different word/character representations into a single vector per token in your input. label A torch tensor representing the sequence of integer gold class label of shape (batch_size, num_classes) .","title":"Parameters"},{"location":"api/models/sequence_classifier_base/","text":"biome.text.models.sequence_classifier_base SequenceClassifierBase SequenceClassifierBase( self, vocab: Vocabulary, text_field_embedder: TextFieldEmbedder, seq2vec_encoder: Seq2VecEncoder, seq2seq_encoder: typing.Union[allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder, NoneType] = None, multifield_seq2seq_encoder: typing.Union[allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder, NoneType] = None, multifield_seq2vec_encoder: typing.Union[allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder, NoneType] = None, feed_forward: typing.Union[allennlp.modules.feedforward.FeedForward, NoneType] = None, dropout: typing.Union[float, NoneType] = None, multifield_dropout: typing.Union[float, NoneType] = None, initializer: typing.Union[allennlp.nn.initializers.InitializerApplicator, NoneType] = None, regularizer: typing.Union[allennlp.nn.regularizers.regularizer_applicator.RegularizerApplicator, NoneType] = None, accuracy: typing.Union[allennlp.training.metrics.categorical_accuracy.CategoricalAccuracy, NoneType] = None, loss_weights: typing.Dict[str, float] = None) In the most simple form this BaseModelClassifier encodes a sequence with a Seq2VecEncoder , then predicts a label for the sequence. Parameters vocab A Vocabulary, required in order to compute sizes for input/output projections and passed on to the :class: ~allennlp.models.model.Model class. text_field_embedder Used to embed the input text into a TextField seq2seq_encoder Optional Seq2Seq encoder layer for the input text. seq2vec_encoder Required Seq2Vec encoder layer. If seq2seq_encoder is provided, this encoder will pool its output. Otherwise, this encoder will operate directly on the output of the text_field_embedder . dropout Dropout percentage to use on the output of the Seq2VecEncoder multifield_seq2seq_encoder Optional Seq2Seq encoder layer for the encoded fields. multifield_seq2vec_encoder If we use ListField s, this Seq2Vec encoder is required. If multifield_seq2seq_encoder is provided, this encoder will pool its output. Otherwise, this encoder will operate directly on the output of the seq2vec_encoder . multifield_dropout Dropout percentage to use on the output of the doc Seq2VecEncoder feed_forward A feed forward layer applied to the encoded inputs. initializer Used to initialize the model parameters. regularizer Used to regularize the model. Passed on to :class: ~allennlp.models.model.Model . accuracy The accuracy you want to use. By default, we choose a categorical top-1 accuracy. loss_weights A dict with the labels and the corresponding weights. These weights will be used in the CrossEntropyLoss function. n_inputs This value is used for calculate the output layer dimension. Default value is 1 num_classes Number of output classes output_classes The output token classes label_for_index SequenceClassifierBase.label_for_index(idx) Token label for label index extend_labels SequenceClassifierBase.extend_labels(labels: typing.List[str]) Extends the number of output labels forward_tokens SequenceClassifierBase.forward_tokens( tokens: typing.Dict[str, torch.Tensor]) Apply the whole forward chain but last layer (output) Parameters tokens The tokens tensor Returns A Tensor output_layer SequenceClassifierBase.output_layer(encoded_text: Tensor, label) Returns An output dictionary consisting of: logits : :class: ~torch.Tensor A tensor of shape (batch_size, num_classes) representing the logits of the classifier model. class_probabilities : :class: ~torch.Tensor A tensor of shape (batch_size, num_classes) representing the softmax probabilities of the classes. loss : :class: ~torch.Tensor , optional A scalar loss to be optimised. forward SequenceClassifierBase.forward(*inputs) Defines the forward pass of the model. In addition, to facilitate easy training, this method is designed to compute a loss function defined by a user. The input is comprised of everything required to perform a training update, including labels - you define the signature here! It is down to the user to ensure that inference can be performed without the presence of these labels. Hence, any inputs not available at inference time should only be used inside a conditional block. The intended sketch of this method is as follows:: def forward(self, input1, input2, targets=None): .... .... output1 = self.layer1(input1) output2 = self.layer2(input2) output_dict = {\"output1\": output1, \"output2\": output2} if targets is not None: # Function returning a scalar torch.Tensor, defined by the user. loss = self._compute_loss(output1, output2, targets) output_dict[\"loss\"] = loss return output_dict Parameters inputs: Tensors comprising everything needed to perform a training update, including labels, which should be optional (i.e have a default value of None ). At inference time, simply pass the relevant inputs, not including the labels. Returns output_dict: Dict[str, torch.Tensor] The outputs from the model. In order to train a model using the :class: ~allennlp.training.Trainer api, you must provide a \"loss\" key pointing to a scalar torch.Tensor representing the loss to be optimized.","title":"sequence_classifier_base"},{"location":"api/models/sequence_classifier_base/#biometextmodelssequence_classifier_base","text":"","title":"biome.text.models.sequence_classifier_base"},{"location":"api/models/sequence_classifier_base/#sequenceclassifierbase","text":"SequenceClassifierBase( self, vocab: Vocabulary, text_field_embedder: TextFieldEmbedder, seq2vec_encoder: Seq2VecEncoder, seq2seq_encoder: typing.Union[allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder, NoneType] = None, multifield_seq2seq_encoder: typing.Union[allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder, NoneType] = None, multifield_seq2vec_encoder: typing.Union[allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder, NoneType] = None, feed_forward: typing.Union[allennlp.modules.feedforward.FeedForward, NoneType] = None, dropout: typing.Union[float, NoneType] = None, multifield_dropout: typing.Union[float, NoneType] = None, initializer: typing.Union[allennlp.nn.initializers.InitializerApplicator, NoneType] = None, regularizer: typing.Union[allennlp.nn.regularizers.regularizer_applicator.RegularizerApplicator, NoneType] = None, accuracy: typing.Union[allennlp.training.metrics.categorical_accuracy.CategoricalAccuracy, NoneType] = None, loss_weights: typing.Dict[str, float] = None) In the most simple form this BaseModelClassifier encodes a sequence with a Seq2VecEncoder , then predicts a label for the sequence.","title":"SequenceClassifierBase"},{"location":"api/models/sequence_classifier_base/#parameters","text":"vocab A Vocabulary, required in order to compute sizes for input/output projections and passed on to the :class: ~allennlp.models.model.Model class. text_field_embedder Used to embed the input text into a TextField seq2seq_encoder Optional Seq2Seq encoder layer for the input text. seq2vec_encoder Required Seq2Vec encoder layer. If seq2seq_encoder is provided, this encoder will pool its output. Otherwise, this encoder will operate directly on the output of the text_field_embedder . dropout Dropout percentage to use on the output of the Seq2VecEncoder multifield_seq2seq_encoder Optional Seq2Seq encoder layer for the encoded fields. multifield_seq2vec_encoder If we use ListField s, this Seq2Vec encoder is required. If multifield_seq2seq_encoder is provided, this encoder will pool its output. Otherwise, this encoder will operate directly on the output of the seq2vec_encoder . multifield_dropout Dropout percentage to use on the output of the doc Seq2VecEncoder feed_forward A feed forward layer applied to the encoded inputs. initializer Used to initialize the model parameters. regularizer Used to regularize the model. Passed on to :class: ~allennlp.models.model.Model . accuracy The accuracy you want to use. By default, we choose a categorical top-1 accuracy. loss_weights A dict with the labels and the corresponding weights. These weights will be used in the CrossEntropyLoss function.","title":"Parameters"},{"location":"api/models/sequence_classifier_base/#n_inputs","text":"This value is used for calculate the output layer dimension. Default value is 1","title":"n_inputs"},{"location":"api/models/sequence_classifier_base/#num_classes","text":"Number of output classes","title":"num_classes"},{"location":"api/models/sequence_classifier_base/#output_classes","text":"The output token classes","title":"output_classes"},{"location":"api/models/sequence_classifier_base/#label_for_index","text":"SequenceClassifierBase.label_for_index(idx) Token label for label index","title":"label_for_index"},{"location":"api/models/sequence_classifier_base/#extend_labels","text":"SequenceClassifierBase.extend_labels(labels: typing.List[str]) Extends the number of output labels","title":"extend_labels"},{"location":"api/models/sequence_classifier_base/#forward_tokens","text":"SequenceClassifierBase.forward_tokens( tokens: typing.Dict[str, torch.Tensor]) Apply the whole forward chain but last layer (output)","title":"forward_tokens"},{"location":"api/models/sequence_classifier_base/#parameters_1","text":"tokens The tokens tensor","title":"Parameters"},{"location":"api/models/sequence_classifier_base/#returns","text":"A Tensor","title":"Returns"},{"location":"api/models/sequence_classifier_base/#output_layer","text":"SequenceClassifierBase.output_layer(encoded_text: Tensor, label)","title":"output_layer"},{"location":"api/models/sequence_classifier_base/#returns_1","text":"An output dictionary consisting of: logits : :class: ~torch.Tensor A tensor of shape (batch_size, num_classes) representing the logits of the classifier model. class_probabilities : :class: ~torch.Tensor A tensor of shape (batch_size, num_classes) representing the softmax probabilities of the classes. loss : :class: ~torch.Tensor , optional A scalar loss to be optimised.","title":"Returns"},{"location":"api/models/sequence_classifier_base/#forward","text":"SequenceClassifierBase.forward(*inputs) Defines the forward pass of the model. In addition, to facilitate easy training, this method is designed to compute a loss function defined by a user. The input is comprised of everything required to perform a training update, including labels - you define the signature here! It is down to the user to ensure that inference can be performed without the presence of these labels. Hence, any inputs not available at inference time should only be used inside a conditional block. The intended sketch of this method is as follows:: def forward(self, input1, input2, targets=None): .... .... output1 = self.layer1(input1) output2 = self.layer2(input2) output_dict = {\"output1\": output1, \"output2\": output2} if targets is not None: # Function returning a scalar torch.Tensor, defined by the user. loss = self._compute_loss(output1, output2, targets) output_dict[\"loss\"] = loss return output_dict","title":"forward"},{"location":"api/models/sequence_classifier_base/#parameters_2","text":"inputs: Tensors comprising everything needed to perform a training update, including labels, which should be optional (i.e have a default value of None ). At inference time, simply pass the relevant inputs, not including the labels.","title":"Parameters"},{"location":"api/models/sequence_classifier_base/#returns_2","text":"output_dict: Dict[str, torch.Tensor] The outputs from the model. In order to train a model using the :class: ~allennlp.training.Trainer api, you must provide a \"loss\" key pointing to a scalar torch.Tensor representing the loss to be optimized.","title":"Returns"},{"location":"api/models/sequence_pair_classifier/","text":"biome.text.models.sequence_pair_classifier SequencePairClassifier SequencePairClassifier( self, vocab: Vocabulary, text_field_embedder: TextFieldEmbedder, seq2vec_encoder: Seq2VecEncoder, seq2seq_encoder: typing.Union[allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder, NoneType] = None, multifield_seq2seq_encoder: typing.Union[allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder, NoneType] = None, multifield_seq2vec_encoder: typing.Union[allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder, NoneType] = None, feed_forward: typing.Union[allennlp.modules.feedforward.FeedForward, NoneType] = None, dropout: typing.Union[float, NoneType] = None, multifield_dropout: typing.Union[float, NoneType] = None, initializer: typing.Union[allennlp.nn.initializers.InitializerApplicator, NoneType] = None, regularizer: typing.Union[allennlp.nn.regularizers.regularizer_applicator.RegularizerApplicator, NoneType] = None, accuracy: typing.Union[allennlp.training.metrics.categorical_accuracy.CategoricalAccuracy, NoneType] = None, loss_weights: typing.Dict[str, float] = None) This SequencePairClassifier uses a siamese network architecture to perform a classification task between a pair of records or documents. The classifier can be configured to take into account the hierarchical structure of documents and multi-field records. A record/document can be (1) single-field (single sentence): composed of a sequence of tokens, or (2) multi-field (multi-sentence): a sequence of fields with each of the fields containing a sequence of tokens. In the case of multi-field a doc_seq2vec_encoder and optionally a doc_seq2seq_encoder should be configured, for encoding each of the fields into a single vector encoding the full record/doc must be configured. The sequences are encoded into two single vectors, the resulting vectors are concatenated and fed to a linear classification layer. forward SequencePairClassifier.forward(record1: typing.Dict[str, torch.Tensor], record2: typing.Dict[str, torch.Tensor], label: Tensor = None) Parameters record1 The first input tokens. The dictionary is the output of a TextField.as_array() . It gives names to the tensors created by the TokenIndexer s. In its most basic form, using a SingleIdTokenIndexer , the dictionary is composed of: {\"tokens\": Tensor(batch_size, num_tokens)} . The keys of the dictionary are defined in the model.yml input. The dictionary is designed to be passed on directly to a TextFieldEmbedder , that has a TokenEmbedder for each key in the dictionary (except you set allow_unmatched_keys in the TextFieldEmbedder to False) and knows how to combine different word/character representations into a single vector per token in your input. record2 The second input tokens. label : torch.LongTensor, optional (default = None) A torch tensor representing the sequence of integer gold class label of shape (batch_size, num_classes) . Returns An output dictionary consisting of: logits : torch.FloatTensor A tensor of shape (batch_size, num_tokens, tag_vocab_size) representing unnormalised log probabilities of the tag classes. class_probabilities : torch.FloatTensor A tensor of shape (batch_size, num_tokens, tag_vocab_size) representing a distribution of the tag classes per word. loss : torch.FloatTensor, optional A scalar loss to be optimised.","title":"sequence_pair_classifier"},{"location":"api/models/sequence_pair_classifier/#biometextmodelssequence_pair_classifier","text":"","title":"biome.text.models.sequence_pair_classifier"},{"location":"api/models/sequence_pair_classifier/#sequencepairclassifier","text":"SequencePairClassifier( self, vocab: Vocabulary, text_field_embedder: TextFieldEmbedder, seq2vec_encoder: Seq2VecEncoder, seq2seq_encoder: typing.Union[allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder, NoneType] = None, multifield_seq2seq_encoder: typing.Union[allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder, NoneType] = None, multifield_seq2vec_encoder: typing.Union[allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder, NoneType] = None, feed_forward: typing.Union[allennlp.modules.feedforward.FeedForward, NoneType] = None, dropout: typing.Union[float, NoneType] = None, multifield_dropout: typing.Union[float, NoneType] = None, initializer: typing.Union[allennlp.nn.initializers.InitializerApplicator, NoneType] = None, regularizer: typing.Union[allennlp.nn.regularizers.regularizer_applicator.RegularizerApplicator, NoneType] = None, accuracy: typing.Union[allennlp.training.metrics.categorical_accuracy.CategoricalAccuracy, NoneType] = None, loss_weights: typing.Dict[str, float] = None) This SequencePairClassifier uses a siamese network architecture to perform a classification task between a pair of records or documents. The classifier can be configured to take into account the hierarchical structure of documents and multi-field records. A record/document can be (1) single-field (single sentence): composed of a sequence of tokens, or (2) multi-field (multi-sentence): a sequence of fields with each of the fields containing a sequence of tokens. In the case of multi-field a doc_seq2vec_encoder and optionally a doc_seq2seq_encoder should be configured, for encoding each of the fields into a single vector encoding the full record/doc must be configured. The sequences are encoded into two single vectors, the resulting vectors are concatenated and fed to a linear classification layer.","title":"SequencePairClassifier"},{"location":"api/models/sequence_pair_classifier/#forward","text":"SequencePairClassifier.forward(record1: typing.Dict[str, torch.Tensor], record2: typing.Dict[str, torch.Tensor], label: Tensor = None)","title":"forward"},{"location":"api/models/sequence_pair_classifier/#parameters","text":"record1 The first input tokens. The dictionary is the output of a TextField.as_array() . It gives names to the tensors created by the TokenIndexer s. In its most basic form, using a SingleIdTokenIndexer , the dictionary is composed of: {\"tokens\": Tensor(batch_size, num_tokens)} . The keys of the dictionary are defined in the model.yml input. The dictionary is designed to be passed on directly to a TextFieldEmbedder , that has a TokenEmbedder for each key in the dictionary (except you set allow_unmatched_keys in the TextFieldEmbedder to False) and knows how to combine different word/character representations into a single vector per token in your input. record2 The second input tokens. label : torch.LongTensor, optional (default = None) A torch tensor representing the sequence of integer gold class label of shape (batch_size, num_classes) .","title":"Parameters"},{"location":"api/models/sequence_pair_classifier/#returns","text":"An output dictionary consisting of: logits : torch.FloatTensor A tensor of shape (batch_size, num_tokens, tag_vocab_size) representing unnormalised log probabilities of the tag classes. class_probabilities : torch.FloatTensor A tensor of shape (batch_size, num_tokens, tag_vocab_size) representing a distribution of the tag classes per word. loss : torch.FloatTensor, optional A scalar loss to be optimised.","title":"Returns"},{"location":"api/models/similarity_classifier/","text":"biome.text.models.similarity_classifier SimilarityClassifier SimilarityClassifier( self, vocab: Vocabulary, text_field_embedder: TextFieldEmbedder, seq2vec_encoder: Seq2VecEncoder, seq2seq_encoder: typing.Union[allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder, NoneType] = None, multifield_seq2seq_encoder: typing.Union[allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder, NoneType] = None, multifield_seq2vec_encoder: typing.Union[allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder, NoneType] = None, feed_forward: typing.Union[allennlp.modules.feedforward.FeedForward, NoneType] = None, dropout: typing.Union[float, NoneType] = None, multifield_dropout: typing.Union[float, NoneType] = None, initializer: typing.Union[allennlp.nn.initializers.InitializerApplicator, NoneType] = None, regularizer: typing.Union[allennlp.nn.regularizers.regularizer_applicator.RegularizerApplicator, NoneType] = None, margin: float = 0.5, verification_weight: float = 2.0) This SimilarityClassifier uses a siamese network architecture to perform a binary classification task: are two inputs similar or not? The two input sequences are encoded with two single vectors, the resulting vectors are concatenated and fed to a linear classification layer. Apart from the CrossEntropy loss, this model includes a CosineEmbedding loss (https://pytorch.org/docs/stable/nn.html#cosineembeddingloss) that will drive the network to create vector clusters for each \"class\" in the data. Make sure that the label \"same\" is indexed as 0, and the label \"different\" as 1!!! Make sure that the dropout of the last Seq2Vec or the last FeedForward layer is set to 0!!! (Deep Learning Face Representation by Joint Identification-Verification, https://arxiv.org/pdf/1406.4773.pdf) Parameters kwargs See the BaseModelClassifier for a description of the parameters. margin This parameter is passed on to the CosineEmbedding loss. It provides a margin, at which dissimilar vectors are not driven further apart. Can be between -1 (always drive apart) and 1 (never drive apart). verification_weight Defines the weight of the verification loss in the final loss sum: loss = CrossEntropy + w * CosineEmbedding forward SimilarityClassifier.forward(record1: typing.Dict[str, torch.Tensor], record2: typing.Dict[str, torch.Tensor], label: Tensor = None) The architecture is basically: Embedding -> Seq2Seq -> Seq2Vec -> Dropout -> (Optional: MultiField stuff) -> FeedForward -> Concatenation -> Classification layer Parameters record1 The first input tokens. The dictionary is the output of a TextField.as_array() . It gives names to the tensors created by the TokenIndexer s. In its most basic form, using a SingleIdTokenIndexer , the dictionary is composed of: {\"tokens\": Tensor(batch_size, num_tokens)} . The keys of the dictionary are defined in the model.yml input. The dictionary is designed to be passed on directly to a TextFieldEmbedder , that has a TokenEmbedder for each key in the dictionary (except you set allow_unmatched_keys in the TextFieldEmbedder to False) and knows how to combine different word/character representations into a single vector per token in your input. record2 The second input tokens. label : torch.LongTensor, optional (default = None) A torch tensor representing the sequence of integer gold class label of shape (batch_size, num_classes) . Returns An output dictionary consisting of: logits : torch.FloatTensor A tensor of shape (batch_size, num_tokens, tag_vocab_size) representing unnormalised log probabilities of the tag classes. class_probabilities : torch.FloatTensor A tensor of shape (batch_size, num_tokens, tag_vocab_size) representing a distribution of the tag classes per word. loss : torch.FloatTensor, optional A scalar loss to be optimised. output_layer SimilarityClassifier.output_layer( encoded_texts: typing.List[torch.Tensor], label) Returns An output dictionary consisting of: logits : :class: ~torch.Tensor A tensor of shape (batch_size, num_classes) representing the logits of the classifier model. class_probabilities : :class: ~torch.Tensor A tensor of shape (batch_size, num_classes) representing the softmax probabilities of the classes. loss : :class: ~torch.Tensor , optional A scalar loss to be optimised. ContrastiveLoss ContrastiveLoss(self) Computes a contrastive loss given a distance. We do not use it at the moment, i leave it here just in case. forward ContrastiveLoss.forward(distance, label, margin) Compute the loss. Important: Make sure label = 0 corresponds to the same case, label = 1 to the different case! Parameters distance Distance between the two input vectors label Label if the two input vectors belong to the same or different class. margin If the distance is larger than the margin, the distance of different class vectors does not contribute to the loss. Returns loss","title":"similarity_classifier"},{"location":"api/models/similarity_classifier/#biometextmodelssimilarity_classifier","text":"","title":"biome.text.models.similarity_classifier"},{"location":"api/models/similarity_classifier/#similarityclassifier","text":"SimilarityClassifier( self, vocab: Vocabulary, text_field_embedder: TextFieldEmbedder, seq2vec_encoder: Seq2VecEncoder, seq2seq_encoder: typing.Union[allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder, NoneType] = None, multifield_seq2seq_encoder: typing.Union[allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder, NoneType] = None, multifield_seq2vec_encoder: typing.Union[allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder, NoneType] = None, feed_forward: typing.Union[allennlp.modules.feedforward.FeedForward, NoneType] = None, dropout: typing.Union[float, NoneType] = None, multifield_dropout: typing.Union[float, NoneType] = None, initializer: typing.Union[allennlp.nn.initializers.InitializerApplicator, NoneType] = None, regularizer: typing.Union[allennlp.nn.regularizers.regularizer_applicator.RegularizerApplicator, NoneType] = None, margin: float = 0.5, verification_weight: float = 2.0) This SimilarityClassifier uses a siamese network architecture to perform a binary classification task: are two inputs similar or not? The two input sequences are encoded with two single vectors, the resulting vectors are concatenated and fed to a linear classification layer. Apart from the CrossEntropy loss, this model includes a CosineEmbedding loss (https://pytorch.org/docs/stable/nn.html#cosineembeddingloss) that will drive the network to create vector clusters for each \"class\" in the data. Make sure that the label \"same\" is indexed as 0, and the label \"different\" as 1!!! Make sure that the dropout of the last Seq2Vec or the last FeedForward layer is set to 0!!! (Deep Learning Face Representation by Joint Identification-Verification, https://arxiv.org/pdf/1406.4773.pdf)","title":"SimilarityClassifier"},{"location":"api/models/similarity_classifier/#parameters","text":"kwargs See the BaseModelClassifier for a description of the parameters. margin This parameter is passed on to the CosineEmbedding loss. It provides a margin, at which dissimilar vectors are not driven further apart. Can be between -1 (always drive apart) and 1 (never drive apart). verification_weight Defines the weight of the verification loss in the final loss sum: loss = CrossEntropy + w * CosineEmbedding","title":"Parameters"},{"location":"api/models/similarity_classifier/#forward","text":"SimilarityClassifier.forward(record1: typing.Dict[str, torch.Tensor], record2: typing.Dict[str, torch.Tensor], label: Tensor = None) The architecture is basically: Embedding -> Seq2Seq -> Seq2Vec -> Dropout -> (Optional: MultiField stuff) -> FeedForward -> Concatenation -> Classification layer","title":"forward"},{"location":"api/models/similarity_classifier/#parameters_1","text":"record1 The first input tokens. The dictionary is the output of a TextField.as_array() . It gives names to the tensors created by the TokenIndexer s. In its most basic form, using a SingleIdTokenIndexer , the dictionary is composed of: {\"tokens\": Tensor(batch_size, num_tokens)} . The keys of the dictionary are defined in the model.yml input. The dictionary is designed to be passed on directly to a TextFieldEmbedder , that has a TokenEmbedder for each key in the dictionary (except you set allow_unmatched_keys in the TextFieldEmbedder to False) and knows how to combine different word/character representations into a single vector per token in your input. record2 The second input tokens. label : torch.LongTensor, optional (default = None) A torch tensor representing the sequence of integer gold class label of shape (batch_size, num_classes) .","title":"Parameters"},{"location":"api/models/similarity_classifier/#returns","text":"An output dictionary consisting of: logits : torch.FloatTensor A tensor of shape (batch_size, num_tokens, tag_vocab_size) representing unnormalised log probabilities of the tag classes. class_probabilities : torch.FloatTensor A tensor of shape (batch_size, num_tokens, tag_vocab_size) representing a distribution of the tag classes per word. loss : torch.FloatTensor, optional A scalar loss to be optimised.","title":"Returns"},{"location":"api/models/similarity_classifier/#output_layer","text":"SimilarityClassifier.output_layer( encoded_texts: typing.List[torch.Tensor], label)","title":"output_layer"},{"location":"api/models/similarity_classifier/#returns_1","text":"An output dictionary consisting of: logits : :class: ~torch.Tensor A tensor of shape (batch_size, num_classes) representing the logits of the classifier model. class_probabilities : :class: ~torch.Tensor A tensor of shape (batch_size, num_classes) representing the softmax probabilities of the classes. loss : :class: ~torch.Tensor , optional A scalar loss to be optimised.","title":"Returns"},{"location":"api/models/similarity_classifier/#contrastiveloss","text":"ContrastiveLoss(self) Computes a contrastive loss given a distance. We do not use it at the moment, i leave it here just in case.","title":"ContrastiveLoss"},{"location":"api/models/similarity_classifier/#forward_1","text":"ContrastiveLoss.forward(distance, label, margin) Compute the loss. Important: Make sure label = 0 corresponds to the same case, label = 1 to the different case!","title":"forward"},{"location":"api/models/similarity_classifier/#parameters_2","text":"distance Distance between the two input vectors label Label if the two input vectors belong to the same or different class. margin If the distance is larger than the margin, the distance of different class vectors does not contribute to the loss.","title":"Parameters"},{"location":"api/models/similarity_classifier/#returns_2","text":"loss","title":"Returns"},{"location":"api/pipelines/biome_bimpm/","text":"biome.text.pipelines.biome_bimpm","title":"biome_bimpm"},{"location":"api/pipelines/biome_bimpm/#biometextpipelinesbiome_bimpm","text":"","title":"biome.text.pipelines.biome_bimpm"},{"location":"api/pipelines/defs/","text":"biome.text.pipelines.defs ElasticsearchConfig ElasticsearchConfig(self, es_host: str, es_index: str) Elasticsearch configuration data class ExploreConfig ExploreConfig(self, batch_size: int = 500, prediction_cache_size: int = 0, interpret: bool = False, force_delete: bool = True, **metadata) Explore configuration data class","title":"defs"},{"location":"api/pipelines/defs/#biometextpipelinesdefs","text":"","title":"biome.text.pipelines.defs"},{"location":"api/pipelines/defs/#elasticsearchconfig","text":"ElasticsearchConfig(self, es_host: str, es_index: str) Elasticsearch configuration data class","title":"ElasticsearchConfig"},{"location":"api/pipelines/defs/#exploreconfig","text":"ExploreConfig(self, batch_size: int = 500, prediction_cache_size: int = 0, interpret: bool = False, force_delete: bool = True, **metadata) Explore configuration data class","title":"ExploreConfig"},{"location":"api/pipelines/explore/","text":"biome.text.pipelines.explore pipeline_predictions pipeline_predictions(pipeline: Pipeline, source_path: str, config: ExploreConfig, es_config: ElasticsearchConfig) Read a data source and tries to apply a model predictions to the whole data source. The results will be persisted into an elasticsearch index for further data exploration register_biome_prediction register_biome_prediction(name: str, pipeline: Pipeline, es_config: ElasticsearchConfig, **kwargs) Creates a new metadata entry for the incoming prediction Parameters name A descriptive prediction name pipeline The pipeline used for the prediction batch es_config: The Elasticsearch configuration data kwargs Extra arguments passed as extra metadata info","title":"explore"},{"location":"api/pipelines/explore/#biometextpipelinesexplore","text":"","title":"biome.text.pipelines.explore"},{"location":"api/pipelines/explore/#pipeline_predictions","text":"pipeline_predictions(pipeline: Pipeline, source_path: str, config: ExploreConfig, es_config: ElasticsearchConfig) Read a data source and tries to apply a model predictions to the whole data source. The results will be persisted into an elasticsearch index for further data exploration","title":"pipeline_predictions"},{"location":"api/pipelines/explore/#register_biome_prediction","text":"register_biome_prediction(name: str, pipeline: Pipeline, es_config: ElasticsearchConfig, **kwargs) Creates a new metadata entry for the incoming prediction","title":"register_biome_prediction"},{"location":"api/pipelines/explore/#parameters","text":"name A descriptive prediction name pipeline The pipeline used for the prediction batch es_config: The Elasticsearch configuration data kwargs Extra arguments passed as extra metadata info","title":"Parameters"},{"location":"api/pipelines/multifield_bimpm/","text":"biome.text.pipelines.multifield_bimpm","title":"multifield_bimpm"},{"location":"api/pipelines/multifield_bimpm/#biometextpipelinesmultifield_bimpm","text":"","title":"biome.text.pipelines.multifield_bimpm"},{"location":"api/pipelines/pipeline/","text":"biome.text.pipelines.pipeline Architecture Type variable. Usage:: T = TypeVar('T') # Can be anything A = TypeVar('A', str, bytes) # Must be str or bytes Type variables exist primarily for the benefit of static type checkers. They serve as the parameters for generic types as well as for generic function definitions. See class Generic for more information on generic types. Generic functions work as follows: def repeat(x: T, n: int) -> List[T]: '''Return a list containing n references to x.''' return [x]*n def longest(x: A, y: A) -> A: '''Return the longest of two strings.''' return x if len(x) >= len(y) else y The latter example's signature is essentially the overloading of (str, str) -> str and (bytes, bytes) -> bytes. Also note that if the arguments are instances of some subclass of str, the return type is still plain str. At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError. Type variables defined with covariant=True or contravariant=True can be used to declare covariant or contravariant generic types. See PEP 484 for more details. By default generic types are invariant in all type variables. Type variables can be introspected. e.g.: T. name == 'T' T. constraints == () T. covariant == False T. contravariant = False A. constraints == (str, bytes) Note that only type variables defined in global scope can be pickled. Reader Type variable. Usage:: T = TypeVar('T') # Can be anything A = TypeVar('A', str, bytes) # Must be str or bytes Type variables exist primarily for the benefit of static type checkers. They serve as the parameters for generic types as well as for generic function definitions. See class Generic for more information on generic types. Generic functions work as follows: def repeat(x: T, n: int) -> List[T]: '''Return a list containing n references to x.''' return [x]*n def longest(x: A, y: A) -> A: '''Return the longest of two strings.''' return x if len(x) >= len(y) else y The latter example's signature is essentially the overloading of (str, str) -> str and (bytes, bytes) -> bytes. Also note that if the arguments are instances of some subclass of str, the return type is still plain str. At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError. Type variables defined with covariant=True or contravariant=True can be used to declare covariant or contravariant generic types. See PEP 484 for more details. By default generic types are invariant in all type variables. Type variables can be introspected. e.g.: T. name == 'T' T. constraints == () T. covariant == False T. contravariant = False A. constraints == (str, bytes) Note that only type variables defined in global scope can be pickled. Pipeline Pipeline(cls, *args, **kwds) This class combine the different allennlp components that make possible a `Pipeline , understanding as a model, not only the definition of the neural network architecture, but also the transformation of the input data to Instances and the evaluation of predictions on new data The base idea is that this class contains the model and the dataset reader (as a predictor does), and allow operations of learning, predict and save Parameters model` The class:~allennlp.models.Model architecture reader The class:allennlp.data.DatasetReader ARCHITECTURE_FIELD config A representation of reader and model in a properties defined way as allennlp does Returns The configuration dictionary model The model (AKA allennlp.models.Model ) Returns The configured ``allennlp.models.Model`` name Get the pipeline name Returns The fully qualified pipeline class name PIPELINE_FIELD PREDICTION_FILE_NAME reader The data reader (AKA DatasetReader ) Returns The configured ``DatasetReader`` signature Describe de input signature for the pipeline Returns A dict of expected inputs TYPE_FIELD init_prediction_logger Pipeline.init_prediction_logger(output_dir: str, max_bytes: int = 20000000, backup_count: int = 20) Initialize the prediction logger. If initialized we will log all predictions to a file called predictions.json in the output_folder . Parameters output_dir Path to the folder in which we create the predictions.json file. max_bytes Passed on to logging.handlers.RotatingFileHandler backup_count Passed on to logging.handlers.RotatingFileHandler reader_class Pipeline.reader_class() Must be implemented by subclasses Returns The class of ``DataSourceReader`` used in the model instance model_class Pipeline.model_class() Must be implemented by subclasses Returns The class of ``allennlp.models.Model`` used in the model instance load Pipeline.load(binary_path: str, **kwargs) Load a model pipeline form a binary path. Parameters binary_path Path to the binary file kwargs Passed on to the biome.text.models.load_archive method Returns pipeline predictions_to_labeled_instances Pipeline.predictions_to_labeled_instances( instance: Instance, outputs: typing.Dict[str, numpy.ndarray]) This function takes a model's outputs for an Instance, and it labels that instance according to the output. For example, in classification this function labels the instance according to the class with the highest probability. This function is used to to compute gradients of what the model predicted. The return type is a list because in some tasks there are multiple predictions in the output (e.g., in NER a model predicts multiple spans). In this case, each instance in the returned list of Instances contains an individual entity prediction as the label. get_gradients Pipeline.get_gradients( instances: typing.List[allennlp.data.instance.Instance]) Gets the gradients of the loss with respect to the model inputs. Parameters instances: List[Instance] Returns Tuple[Dict[str, Any], Dict[str, Any]] The first item is a Dict of gradient entries for each input. The keys have the form {grad_input_1: ..., grad_input_2: ... } up to the number of inputs given. The second item is the model's output. Notes Takes a JsonDict representing the inputs of the model and converts them to :class: ~allennlp.data.instance.Instance s, sends these through the model :func: forward function after registering hooks on the embedding layer of the model. Calls :func: backward on the loss and then removes the hooks. json_to_labeled_instances Pipeline.json_to_labeled_instances(inputs: typing.Dict[str, typing.Any]) Converts incoming json to a :class: ~allennlp.data.instance.Instance , runs the model on the newly created instance, and adds labels to the :class: ~allennlp.data.instance.Instance s given by the model's output. Returns List[instance] A list of :class: ~allennlp.data.instance.Instance predict_json Pipeline.predict_json(inputs: typing.Dict[str, typing.Any]) Predict an input with the pipeline's model. Parameters inputs The input features/tokens in form of a json dict Returns output The model's prediction in form of a dict. Returns None if the input could not be transformed to an instance. init_prediction_cache Pipeline.init_prediction_cache(max_size) Initialize a prediction cache using the functools.lru_cache decorator Parameters max_size Save up to max_size most recent items. empty_pipeline Pipeline.empty_pipeline(labels: typing.List[str]) Creates a dummy pipeline with labels for model layers from_config Pipeline.from_config(path: str, labels: typing.List[str] = None) Read a Pipeline subclass instance by reading a configuration file Parameters path The configuration file path labels: Optional. If passed, set a list of output labels for empty pipeline model Returns An instance of ``Pipeline`` with no architecture, since the internal ``allennlp.models.Model`` needs a Vocabulary for the initialization learn Pipeline.learn(trainer: str, train: str, output: str, validation: str = None, test: typing.Union[str, NoneType] = None, vocab: typing.Union[str, NoneType] = None, verbose: bool = False) Launch a learning process for loaded model configuration. Once the learn process finish, the model is ready for make predictions Parameters trainer The trainer file path train The train datasource file path validation The validation datasource file path output The learn output path vocab: Vocab The already generated vocabulary path test: str The test datasource configuration verbose Turn on verbose logs extend_labels Pipeline.extend_labels(labels: typing.List[str]) Allow extend prediction labels to pipeline get_output_labels Pipeline.get_output_labels() Output model labels","title":"pipeline"},{"location":"api/pipelines/pipeline/#biometextpipelinespipeline","text":"","title":"biome.text.pipelines.pipeline"},{"location":"api/pipelines/pipeline/#architecture","text":"Type variable. Usage:: T = TypeVar('T') # Can be anything A = TypeVar('A', str, bytes) # Must be str or bytes Type variables exist primarily for the benefit of static type checkers. They serve as the parameters for generic types as well as for generic function definitions. See class Generic for more information on generic types. Generic functions work as follows: def repeat(x: T, n: int) -> List[T]: '''Return a list containing n references to x.''' return [x]*n def longest(x: A, y: A) -> A: '''Return the longest of two strings.''' return x if len(x) >= len(y) else y The latter example's signature is essentially the overloading of (str, str) -> str and (bytes, bytes) -> bytes. Also note that if the arguments are instances of some subclass of str, the return type is still plain str. At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError. Type variables defined with covariant=True or contravariant=True can be used to declare covariant or contravariant generic types. See PEP 484 for more details. By default generic types are invariant in all type variables. Type variables can be introspected. e.g.: T. name == 'T' T. constraints == () T. covariant == False T. contravariant = False A. constraints == (str, bytes) Note that only type variables defined in global scope can be pickled.","title":"Architecture"},{"location":"api/pipelines/pipeline/#reader","text":"Type variable. Usage:: T = TypeVar('T') # Can be anything A = TypeVar('A', str, bytes) # Must be str or bytes Type variables exist primarily for the benefit of static type checkers. They serve as the parameters for generic types as well as for generic function definitions. See class Generic for more information on generic types. Generic functions work as follows: def repeat(x: T, n: int) -> List[T]: '''Return a list containing n references to x.''' return [x]*n def longest(x: A, y: A) -> A: '''Return the longest of two strings.''' return x if len(x) >= len(y) else y The latter example's signature is essentially the overloading of (str, str) -> str and (bytes, bytes) -> bytes. Also note that if the arguments are instances of some subclass of str, the return type is still plain str. At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError. Type variables defined with covariant=True or contravariant=True can be used to declare covariant or contravariant generic types. See PEP 484 for more details. By default generic types are invariant in all type variables. Type variables can be introspected. e.g.: T. name == 'T' T. constraints == () T. covariant == False T. contravariant = False A. constraints == (str, bytes) Note that only type variables defined in global scope can be pickled.","title":"Reader"},{"location":"api/pipelines/pipeline/#pipeline","text":"Pipeline(cls, *args, **kwds) This class combine the different allennlp components that make possible a `Pipeline , understanding as a model, not only the definition of the neural network architecture, but also the transformation of the input data to Instances and the evaluation of predictions on new data The base idea is that this class contains the model and the dataset reader (as a predictor does), and allow operations of learning, predict and save","title":"Pipeline"},{"location":"api/pipelines/pipeline/#parameters","text":"model` The class:~allennlp.models.Model architecture reader The class:allennlp.data.DatasetReader","title":"Parameters"},{"location":"api/pipelines/pipeline/#architecture_field","text":"","title":"ARCHITECTURE_FIELD"},{"location":"api/pipelines/pipeline/#config","text":"A representation of reader and model in a properties defined way as allennlp does","title":"config"},{"location":"api/pipelines/pipeline/#returns","text":"The configuration dictionary","title":"Returns"},{"location":"api/pipelines/pipeline/#model","text":"The model (AKA allennlp.models.Model )","title":"model"},{"location":"api/pipelines/pipeline/#returns_1","text":"The configured ``allennlp.models.Model``","title":"Returns"},{"location":"api/pipelines/pipeline/#name","text":"Get the pipeline name","title":"name"},{"location":"api/pipelines/pipeline/#returns_2","text":"The fully qualified pipeline class name","title":"Returns"},{"location":"api/pipelines/pipeline/#pipeline_field","text":"","title":"PIPELINE_FIELD"},{"location":"api/pipelines/pipeline/#prediction_file_name","text":"","title":"PREDICTION_FILE_NAME"},{"location":"api/pipelines/pipeline/#reader_1","text":"The data reader (AKA DatasetReader )","title":"reader"},{"location":"api/pipelines/pipeline/#returns_3","text":"The configured ``DatasetReader``","title":"Returns"},{"location":"api/pipelines/pipeline/#signature","text":"Describe de input signature for the pipeline","title":"signature"},{"location":"api/pipelines/pipeline/#returns_4","text":"A dict of expected inputs","title":"Returns"},{"location":"api/pipelines/pipeline/#type_field","text":"","title":"TYPE_FIELD"},{"location":"api/pipelines/pipeline/#init_prediction_logger","text":"Pipeline.init_prediction_logger(output_dir: str, max_bytes: int = 20000000, backup_count: int = 20) Initialize the prediction logger. If initialized we will log all predictions to a file called predictions.json in the output_folder .","title":"init_prediction_logger"},{"location":"api/pipelines/pipeline/#parameters_1","text":"output_dir Path to the folder in which we create the predictions.json file. max_bytes Passed on to logging.handlers.RotatingFileHandler backup_count Passed on to logging.handlers.RotatingFileHandler","title":"Parameters"},{"location":"api/pipelines/pipeline/#reader_class","text":"Pipeline.reader_class() Must be implemented by subclasses","title":"reader_class"},{"location":"api/pipelines/pipeline/#returns_5","text":"The class of ``DataSourceReader`` used in the model instance","title":"Returns"},{"location":"api/pipelines/pipeline/#model_class","text":"Pipeline.model_class() Must be implemented by subclasses","title":"model_class"},{"location":"api/pipelines/pipeline/#returns_6","text":"The class of ``allennlp.models.Model`` used in the model instance","title":"Returns"},{"location":"api/pipelines/pipeline/#load","text":"Pipeline.load(binary_path: str, **kwargs) Load a model pipeline form a binary path.","title":"load"},{"location":"api/pipelines/pipeline/#parameters_2","text":"binary_path Path to the binary file kwargs Passed on to the biome.text.models.load_archive method","title":"Parameters"},{"location":"api/pipelines/pipeline/#returns_7","text":"pipeline","title":"Returns"},{"location":"api/pipelines/pipeline/#predictions_to_labeled_instances","text":"Pipeline.predictions_to_labeled_instances( instance: Instance, outputs: typing.Dict[str, numpy.ndarray]) This function takes a model's outputs for an Instance, and it labels that instance according to the output. For example, in classification this function labels the instance according to the class with the highest probability. This function is used to to compute gradients of what the model predicted. The return type is a list because in some tasks there are multiple predictions in the output (e.g., in NER a model predicts multiple spans). In this case, each instance in the returned list of Instances contains an individual entity prediction as the label.","title":"predictions_to_labeled_instances"},{"location":"api/pipelines/pipeline/#get_gradients","text":"Pipeline.get_gradients( instances: typing.List[allennlp.data.instance.Instance]) Gets the gradients of the loss with respect to the model inputs.","title":"get_gradients"},{"location":"api/pipelines/pipeline/#parameters_3","text":"instances: List[Instance]","title":"Parameters"},{"location":"api/pipelines/pipeline/#returns_8","text":"Tuple[Dict[str, Any], Dict[str, Any]] The first item is a Dict of gradient entries for each input. The keys have the form {grad_input_1: ..., grad_input_2: ... } up to the number of inputs given. The second item is the model's output.","title":"Returns"},{"location":"api/pipelines/pipeline/#notes","text":"Takes a JsonDict representing the inputs of the model and converts them to :class: ~allennlp.data.instance.Instance s, sends these through the model :func: forward function after registering hooks on the embedding layer of the model. Calls :func: backward on the loss and then removes the hooks.","title":"Notes"},{"location":"api/pipelines/pipeline/#json_to_labeled_instances","text":"Pipeline.json_to_labeled_instances(inputs: typing.Dict[str, typing.Any]) Converts incoming json to a :class: ~allennlp.data.instance.Instance , runs the model on the newly created instance, and adds labels to the :class: ~allennlp.data.instance.Instance s given by the model's output. Returns List[instance] A list of :class: ~allennlp.data.instance.Instance","title":"json_to_labeled_instances"},{"location":"api/pipelines/pipeline/#predict_json","text":"Pipeline.predict_json(inputs: typing.Dict[str, typing.Any]) Predict an input with the pipeline's model.","title":"predict_json"},{"location":"api/pipelines/pipeline/#parameters_4","text":"inputs The input features/tokens in form of a json dict","title":"Parameters"},{"location":"api/pipelines/pipeline/#returns_9","text":"output The model's prediction in form of a dict. Returns None if the input could not be transformed to an instance.","title":"Returns"},{"location":"api/pipelines/pipeline/#init_prediction_cache","text":"Pipeline.init_prediction_cache(max_size) Initialize a prediction cache using the functools.lru_cache decorator","title":"init_prediction_cache"},{"location":"api/pipelines/pipeline/#parameters_5","text":"max_size Save up to max_size most recent items.","title":"Parameters"},{"location":"api/pipelines/pipeline/#empty_pipeline","text":"Pipeline.empty_pipeline(labels: typing.List[str]) Creates a dummy pipeline with labels for model layers","title":"empty_pipeline"},{"location":"api/pipelines/pipeline/#from_config","text":"Pipeline.from_config(path: str, labels: typing.List[str] = None) Read a Pipeline subclass instance by reading a configuration file","title":"from_config"},{"location":"api/pipelines/pipeline/#parameters_6","text":"path The configuration file path labels: Optional. If passed, set a list of output labels for empty pipeline model","title":"Parameters"},{"location":"api/pipelines/pipeline/#returns_10","text":"An instance of ``Pipeline`` with no architecture, since the internal ``allennlp.models.Model`` needs a Vocabulary for the initialization","title":"Returns"},{"location":"api/pipelines/pipeline/#learn","text":"Pipeline.learn(trainer: str, train: str, output: str, validation: str = None, test: typing.Union[str, NoneType] = None, vocab: typing.Union[str, NoneType] = None, verbose: bool = False) Launch a learning process for loaded model configuration. Once the learn process finish, the model is ready for make predictions","title":"learn"},{"location":"api/pipelines/pipeline/#parameters_7","text":"trainer The trainer file path train The train datasource file path validation The validation datasource file path output The learn output path vocab: Vocab The already generated vocabulary path test: str The test datasource configuration verbose Turn on verbose logs","title":"Parameters"},{"location":"api/pipelines/pipeline/#extend_labels","text":"Pipeline.extend_labels(labels: typing.List[str]) Allow extend prediction labels to pipeline","title":"extend_labels"},{"location":"api/pipelines/pipeline/#get_output_labels","text":"Pipeline.get_output_labels() Output model labels","title":"get_output_labels"},{"location":"api/pipelines/sequence_classifier/","text":"biome.text.pipelines.sequence_classifier SequenceClassifierPipeline SequenceClassifierPipeline(cls, *args, **kwds) predict SequenceClassifierPipeline.predict( features: typing.Union[dict, str, typing.List[str]]) This methods just define the api use for the model Parameters features The data features used for prediction Returns The prediction result","title":"sequence_classifier"},{"location":"api/pipelines/sequence_classifier/#biometextpipelinessequence_classifier","text":"","title":"biome.text.pipelines.sequence_classifier"},{"location":"api/pipelines/sequence_classifier/#sequenceclassifierpipeline","text":"SequenceClassifierPipeline(cls, *args, **kwds)","title":"SequenceClassifierPipeline"},{"location":"api/pipelines/sequence_classifier/#predict","text":"SequenceClassifierPipeline.predict( features: typing.Union[dict, str, typing.List[str]]) This methods just define the api use for the model Parameters features The data features used for prediction","title":"predict"},{"location":"api/pipelines/sequence_classifier/#returns","text":"The prediction result","title":"Returns"},{"location":"api/pipelines/sequence_pair_classifier/","text":"biome.text.pipelines.sequence_pair_classifier","title":"sequence_pair_classifier"},{"location":"api/pipelines/sequence_pair_classifier/#biometextpipelinessequence_pair_classifier","text":"","title":"biome.text.pipelines.sequence_pair_classifier"},{"location":"api/pipelines/similarity_classifier/","text":"biome.text.pipelines.similarity_classifier","title":"similarity_classifier"},{"location":"api/pipelines/similarity_classifier/#biometextpipelinessimilarity_classifier","text":"","title":"biome.text.pipelines.similarity_classifier"},{"location":"api/pipelines/learn/allennlp/default_callback_trainer/","text":"biome.text.pipelines.learn.allennlp.default_callback_trainer This module includes the default biome callback trainer and some extra functions/classes for this purpose DefaultCallbackTrainer DefaultCallbackTrainer( self, model: Model, training_data: typing.Iterable[allennlp.data.instance.Instance], iterator: DataIterator, optimizer: Optimizer, num_epochs: int = 20, shuffle: bool = True, serialization_dir: typing.Union[str, NoneType] = None, cuda_device: typing.Union[int, typing.List] = -1, callbacks: typing.List[allennlp.training.callbacks.callback.Callback] = None ) An callback trainer with some extra callbacks already configured","title":"default_callback_trainer"},{"location":"api/pipelines/learn/allennlp/default_callback_trainer/#biometextpipelineslearnallennlpdefault_callback_trainer","text":"This module includes the default biome callback trainer and some extra functions/classes for this purpose","title":"biome.text.pipelines.learn.allennlp.default_callback_trainer"},{"location":"api/pipelines/learn/allennlp/default_callback_trainer/#defaultcallbacktrainer","text":"DefaultCallbackTrainer( self, model: Model, training_data: typing.Iterable[allennlp.data.instance.Instance], iterator: DataIterator, optimizer: Optimizer, num_epochs: int = 20, shuffle: bool = True, serialization_dir: typing.Union[str, NoneType] = None, cuda_device: typing.Union[int, typing.List] = -1, callbacks: typing.List[allennlp.training.callbacks.callback.Callback] = None ) An callback trainer with some extra callbacks already configured","title":"DefaultCallbackTrainer"},{"location":"api/pipelines/learn/allennlp/defs/","text":"biome.text.pipelines.learn.allennlp.defs BiomeConfig BiomeConfig(self, model_path: str = None, trainer_path: str = None, vocab_path: str = None, train_path: str = None, validation_path: str = None, test_path: str = None) This class contains biome config parameters usually necessary to run the biome commands. It also allows a transformation of these parameters to AllenNLP parameters. Parameters model_path Path to the model yaml file trainer_path Path to the trainer yaml file vocab_path Path to the vocab yaml file train_path Path to the data source yaml file of the training set validation_path Path to the data source yaml file of the validation set test_path Path to the data source yaml file of the test set CUDA_DEVICE_FIELD DATASET_READER_FIELD DEFAULT_CALLBACK_TRAINER EVALUATE_ON_TEST_FIELD MODEL_FIELD TEST_DATA_FIELD TRAIN_DATA_FIELD TRAINER_FIELD TYPE_FIELD VALIDATION_DATA_FIELD yaml_to_dict BiomeConfig.yaml_to_dict(path: str) Reads a yaml file and returns a dict. Parameters path Path to the yaml file Returns dict If no path is specified, returns an empty dict get_cuda_device BiomeConfig.get_cuda_device() Gets the cuda device from an environment variable. This is necessary to activate a GPU if available Returns cuda_device The integer number of the CUDA device to_allennlp_params BiomeConfig.to_allennlp_params() Transforms the cfg to AllenNLP parameters by basically joining all biome configurations. Returns params A dict in the right format containing the AllenNLP parameters","title":"defs"},{"location":"api/pipelines/learn/allennlp/defs/#biometextpipelineslearnallennlpdefs","text":"","title":"biome.text.pipelines.learn.allennlp.defs"},{"location":"api/pipelines/learn/allennlp/defs/#biomeconfig","text":"BiomeConfig(self, model_path: str = None, trainer_path: str = None, vocab_path: str = None, train_path: str = None, validation_path: str = None, test_path: str = None) This class contains biome config parameters usually necessary to run the biome commands. It also allows a transformation of these parameters to AllenNLP parameters.","title":"BiomeConfig"},{"location":"api/pipelines/learn/allennlp/defs/#parameters","text":"model_path Path to the model yaml file trainer_path Path to the trainer yaml file vocab_path Path to the vocab yaml file train_path Path to the data source yaml file of the training set validation_path Path to the data source yaml file of the validation set test_path Path to the data source yaml file of the test set","title":"Parameters"},{"location":"api/pipelines/learn/allennlp/defs/#cuda_device_field","text":"","title":"CUDA_DEVICE_FIELD"},{"location":"api/pipelines/learn/allennlp/defs/#dataset_reader_field","text":"","title":"DATASET_READER_FIELD"},{"location":"api/pipelines/learn/allennlp/defs/#default_callback_trainer","text":"","title":"DEFAULT_CALLBACK_TRAINER"},{"location":"api/pipelines/learn/allennlp/defs/#evaluate_on_test_field","text":"","title":"EVALUATE_ON_TEST_FIELD"},{"location":"api/pipelines/learn/allennlp/defs/#model_field","text":"","title":"MODEL_FIELD"},{"location":"api/pipelines/learn/allennlp/defs/#test_data_field","text":"","title":"TEST_DATA_FIELD"},{"location":"api/pipelines/learn/allennlp/defs/#train_data_field","text":"","title":"TRAIN_DATA_FIELD"},{"location":"api/pipelines/learn/allennlp/defs/#trainer_field","text":"","title":"TRAINER_FIELD"},{"location":"api/pipelines/learn/allennlp/defs/#type_field","text":"","title":"TYPE_FIELD"},{"location":"api/pipelines/learn/allennlp/defs/#validation_data_field","text":"","title":"VALIDATION_DATA_FIELD"},{"location":"api/pipelines/learn/allennlp/defs/#yaml_to_dict","text":"BiomeConfig.yaml_to_dict(path: str) Reads a yaml file and returns a dict.","title":"yaml_to_dict"},{"location":"api/pipelines/learn/allennlp/defs/#parameters_1","text":"path Path to the yaml file","title":"Parameters"},{"location":"api/pipelines/learn/allennlp/defs/#returns","text":"dict If no path is specified, returns an empty dict","title":"Returns"},{"location":"api/pipelines/learn/allennlp/defs/#get_cuda_device","text":"BiomeConfig.get_cuda_device() Gets the cuda device from an environment variable. This is necessary to activate a GPU if available","title":"get_cuda_device"},{"location":"api/pipelines/learn/allennlp/defs/#returns_1","text":"cuda_device The integer number of the CUDA device","title":"Returns"},{"location":"api/pipelines/learn/allennlp/defs/#to_allennlp_params","text":"BiomeConfig.to_allennlp_params() Transforms the cfg to AllenNLP parameters by basically joining all biome configurations.","title":"to_allennlp_params"},{"location":"api/pipelines/learn/allennlp/defs/#returns_2","text":"params A dict in the right format containing the AllenNLP parameters","title":"Returns"},{"location":"api/pipelines/learn/allennlp/learn/","text":"learn learn(output: str, model_spec: typing.Union[str, NoneType] = None, model_binary: typing.Union[str, NoneType] = None, vocab: typing.Union[str, NoneType] = None, trainer_path: str = '', train_cfg: str = '', validation_cfg: typing.Union[str, NoneType] = None, test_cfg: typing.Union[str, NoneType] = None, verbose: bool = False)","title":"learn"},{"location":"api/pipelines/learn/allennlp/learn/#learn","text":"learn(output: str, model_spec: typing.Union[str, NoneType] = None, model_binary: typing.Union[str, NoneType] = None, vocab: typing.Union[str, NoneType] = None, trainer_path: str = '', train_cfg: str = '', validation_cfg: typing.Union[str, NoneType] = None, test_cfg: typing.Union[str, NoneType] = None, verbose: bool = False)","title":"learn"},{"location":"api/pipelines/learn/allennlp/callbacks/evaluate/","text":"biome.text.pipelines.learn.allennlp.callbacks.evaluate EvaluateCallback EvaluateCallback(self, serialization_dir: str) This callback allows to a callback trainer evaluate the model against a test dataset Attributes serialization_dir:str The experiment folder evaluate_dataset EvaluateCallback.evaluate_dataset(trainer: CallbackTrainer) This method launches an test dataset (if defined) evaluation when the training ends and adds the test metrics to trainer metrics before they are processed (thanks to priority argument) Parameters trainer:CallbackTrainer The main callback trainer","title":"evaluate"},{"location":"api/pipelines/learn/allennlp/callbacks/evaluate/#biometextpipelineslearnallennlpcallbacksevaluate","text":"","title":"biome.text.pipelines.learn.allennlp.callbacks.evaluate"},{"location":"api/pipelines/learn/allennlp/callbacks/evaluate/#evaluatecallback","text":"EvaluateCallback(self, serialization_dir: str) This callback allows to a callback trainer evaluate the model against a test dataset","title":"EvaluateCallback"},{"location":"api/pipelines/learn/allennlp/callbacks/evaluate/#attributes","text":"serialization_dir:str The experiment folder","title":"Attributes"},{"location":"api/pipelines/learn/allennlp/callbacks/evaluate/#evaluate_dataset","text":"EvaluateCallback.evaluate_dataset(trainer: CallbackTrainer) This method launches an test dataset (if defined) evaluation when the training ends and adds the test metrics to trainer metrics before they are processed (thanks to priority argument)","title":"evaluate_dataset"},{"location":"api/pipelines/learn/allennlp/callbacks/evaluate/#parameters","text":"trainer:CallbackTrainer The main callback trainer","title":"Parameters"},{"location":"api/pipelines/learn/allennlp/callbacks/logging/","text":"biome.text.pipelines.learn.allennlp.callbacks.logging LoggingCallback LoggingCallback() This callbacks allows controls the logging messages during the training process","title":"logging"},{"location":"api/pipelines/learn/allennlp/callbacks/logging/#biometextpipelineslearnallennlpcallbackslogging","text":"","title":"biome.text.pipelines.learn.allennlp.callbacks.logging"},{"location":"api/pipelines/learn/allennlp/callbacks/logging/#loggingcallback","text":"LoggingCallback() This callbacks allows controls the logging messages during the training process","title":"LoggingCallback"},{"location":"api/predictors/default_predictor/","text":"biome.text.predictors.default_predictor","title":"default_predictor"},{"location":"api/predictors/default_predictor/#biometextpredictorsdefault_predictor","text":"","title":"biome.text.predictors.default_predictor"},{"location":"api/predictors/utils/","text":"biome.text.predictors.utils get_predictor_from_archive get_predictor_from_archive( archive: Archive, predictor_name: typing.Union[str, NoneType] = None) Loads a model predictor from a model.tar.gz file","title":"utils"},{"location":"api/predictors/utils/#biometextpredictorsutils","text":"","title":"biome.text.predictors.utils"},{"location":"api/predictors/utils/#get_predictor_from_archive","text":"get_predictor_from_archive( archive: Archive, predictor_name: typing.Union[str, NoneType] = None) Loads a model predictor from a model.tar.gz file","title":"get_predictor_from_archive"}]}