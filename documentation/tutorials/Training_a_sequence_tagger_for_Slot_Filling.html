<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Training a sequence tagger for Slot Filling | biome.text</title>
    <meta name="generator" content="VuePress 1.5.2">
    <link rel="shortcut icon" href="/biome-text/favicon.ico">
    <meta name="description" content="biome.text documentation">
    <link rel="preload" href="/biome-text/assets/css/0.styles.0a5653df.css" as="style"><link rel="preload" href="/biome-text/assets/js/app.56e7876f.js" as="script"><link rel="preload" href="/biome-text/assets/js/4.3b53f028.js" as="script"><link rel="preload" href="/biome-text/assets/js/3.e0fd2864.js" as="script"><link rel="preload" href="/biome-text/assets/js/59.c1fa1482.js" as="script"><link rel="prefetch" href="/biome-text/assets/js/10.2274a82a.js"><link rel="prefetch" href="/biome-text/assets/js/11.c15df7eb.js"><link rel="prefetch" href="/biome-text/assets/js/12.588c43a2.js"><link rel="prefetch" href="/biome-text/assets/js/13.5dde9001.js"><link rel="prefetch" href="/biome-text/assets/js/14.f22e9c59.js"><link rel="prefetch" href="/biome-text/assets/js/15.ab314e8a.js"><link rel="prefetch" href="/biome-text/assets/js/16.10c4ed92.js"><link rel="prefetch" href="/biome-text/assets/js/17.52724e0e.js"><link rel="prefetch" href="/biome-text/assets/js/18.a0e93da1.js"><link rel="prefetch" href="/biome-text/assets/js/19.27ba94d3.js"><link rel="prefetch" href="/biome-text/assets/js/20.10322e13.js"><link rel="prefetch" href="/biome-text/assets/js/21.991ce376.js"><link rel="prefetch" href="/biome-text/assets/js/22.b90829a9.js"><link rel="prefetch" href="/biome-text/assets/js/23.5a0616a3.js"><link rel="prefetch" href="/biome-text/assets/js/24.f01504a5.js"><link rel="prefetch" href="/biome-text/assets/js/25.84de5f05.js"><link rel="prefetch" href="/biome-text/assets/js/26.ef560c26.js"><link rel="prefetch" href="/biome-text/assets/js/27.5f8773f4.js"><link rel="prefetch" href="/biome-text/assets/js/28.b77b990e.js"><link rel="prefetch" href="/biome-text/assets/js/29.3422f09a.js"><link rel="prefetch" href="/biome-text/assets/js/30.798ac574.js"><link rel="prefetch" href="/biome-text/assets/js/31.d333e9ec.js"><link rel="prefetch" href="/biome-text/assets/js/32.8714b8da.js"><link rel="prefetch" href="/biome-text/assets/js/33.2eac4c8a.js"><link rel="prefetch" href="/biome-text/assets/js/34.48b325cb.js"><link rel="prefetch" href="/biome-text/assets/js/35.bb902c54.js"><link rel="prefetch" href="/biome-text/assets/js/36.33a99d52.js"><link rel="prefetch" href="/biome-text/assets/js/37.79d900ff.js"><link rel="prefetch" href="/biome-text/assets/js/38.cc0d66c9.js"><link rel="prefetch" href="/biome-text/assets/js/39.2e5224f1.js"><link rel="prefetch" href="/biome-text/assets/js/40.fb1dc8af.js"><link rel="prefetch" href="/biome-text/assets/js/41.03f2dfe8.js"><link rel="prefetch" href="/biome-text/assets/js/42.1cf67523.js"><link rel="prefetch" href="/biome-text/assets/js/43.ee851914.js"><link rel="prefetch" href="/biome-text/assets/js/44.4e541e89.js"><link rel="prefetch" href="/biome-text/assets/js/45.4c24618c.js"><link rel="prefetch" href="/biome-text/assets/js/46.ac4569ae.js"><link rel="prefetch" href="/biome-text/assets/js/47.49bb0a6d.js"><link rel="prefetch" href="/biome-text/assets/js/48.51b13e64.js"><link rel="prefetch" href="/biome-text/assets/js/49.795512c4.js"><link rel="prefetch" href="/biome-text/assets/js/5.e106e8ba.js"><link rel="prefetch" href="/biome-text/assets/js/50.aa6e315c.js"><link rel="prefetch" href="/biome-text/assets/js/51.c1c107fe.js"><link rel="prefetch" href="/biome-text/assets/js/52.9d6af1f5.js"><link rel="prefetch" href="/biome-text/assets/js/53.65af62b1.js"><link rel="prefetch" href="/biome-text/assets/js/54.70d0a06c.js"><link rel="prefetch" href="/biome-text/assets/js/55.48a92e6c.js"><link rel="prefetch" href="/biome-text/assets/js/56.f597b6a6.js"><link rel="prefetch" href="/biome-text/assets/js/57.2841a5c5.js"><link rel="prefetch" href="/biome-text/assets/js/58.02284ade.js"><link rel="prefetch" href="/biome-text/assets/js/6.60d2fc91.js"><link rel="prefetch" href="/biome-text/assets/js/60.bdb0314d.js"><link rel="prefetch" href="/biome-text/assets/js/61.d30fb828.js"><link rel="prefetch" href="/biome-text/assets/js/62.6d27f6b9.js"><link rel="prefetch" href="/biome-text/assets/js/63.0d2b213f.js"><link rel="prefetch" href="/biome-text/assets/js/64.9e859387.js"><link rel="prefetch" href="/biome-text/assets/js/65.3e0e554c.js"><link rel="prefetch" href="/biome-text/assets/js/7.c2654027.js"><link rel="prefetch" href="/biome-text/assets/js/8.77d63dba.js"><link rel="prefetch" href="/biome-text/assets/js/9.be41dd41.js"><link rel="prefetch" href="/biome-text/assets/js/vendors~docsearch.1e1e39cd.js">
    <link rel="stylesheet" href="/biome-text/assets/css/0.styles.0a5653df.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container" data-v-44f1b6a5><header class="navbar" data-v-44f1b6a5><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/biome-text/" class="home-link router-link-active"><!----> <span class="site-name">biome<span>.text</span></span></a> <div class="links"><form id="search-form" role="search" class="algolia-search-wrapper search-box"><input id="algolia-search-input" class="search-query"></form> <nav class="nav-links can-hide"><div class="nav-item"><a href="/biome-text/api/" class="nav-link">
  API
</a></div><div class="nav-item"><a href="/biome-text/documentation/" class="nav-link router-link-active">
  Documentation
</a></div><div class="nav-item"><a href="https://github.com/recognai/biome-text" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span class="external__icon"><vp-icon color="#4A4A4A" name="blank" size="12px"></vp-icon></span></a></div><div class="nav-item"><a href="https://recogn.ai" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Recognai
  <span class="external__icon"><vp-icon color="#4A4A4A" name="blank" size="12px"></vp-icon></span></a></div> <!----></nav></div></header> <div class="sidebar-mask" data-v-44f1b6a5></div> <aside class="sidebar" data-v-44f1b6a5><div class="sidebar__link"><a href="/biome-text/"><img src="/biome-text/assets/img/biome.svg" class="sidebar__img"></a></div> <nav class="nav-links"><div class="nav-item"><a href="/biome-text/api/" class="nav-link">
  API
</a></div><div class="nav-item"><a href="/biome-text/documentation/" class="nav-link router-link-active">
  Documentation
</a></div><div class="nav-item"><a href="https://github.com/recognai/biome-text" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span class="external__icon"><vp-icon color="#4A4A4A" name="blank" size="12px"></vp-icon></span></a></div><div class="nav-item"><a href="https://recogn.ai" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Recognai
  <span class="external__icon"><vp-icon color="#4A4A4A" name="blank" size="12px"></vp-icon></span></a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group depth-0"><p class="sidebar-heading"><span>Get started</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/biome-text/documentation/" aria-current="page" class="sidebar-link">Installation</a></li><li><a href="/biome-text/documentation/concepts.html" class="sidebar-link">Concepts</a></li></ul></section></li><li><section class="sidebar-group depth-0"><p class="sidebar-heading open"><span>Tutorials</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/biome-text/documentation/tutorials/Training_a_sequence_tagger_for_Slot_Filling.html" aria-current="page" class="active sidebar-link">Training a sequence tagger for Slot Filling</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/biome-text/documentation/tutorials/Training_a_sequence_tagger_for_Slot_Filling.html#explore-the-data" class="sidebar-link">Explore the data</a></li><li class="sidebar-sub-header"><a href="/biome-text/documentation/tutorials/Training_a_sequence_tagger_for_Slot_Filling.html#configure-your-biome-text-pipeline" class="sidebar-link">Configure your biome.text Pipeline</a></li><li class="sidebar-sub-header"><a href="/biome-text/documentation/tutorials/Training_a_sequence_tagger_for_Slot_Filling.html#create-a-vocabulary" class="sidebar-link">Create a vocabulary</a></li><li class="sidebar-sub-header"><a href="/biome-text/documentation/tutorials/Training_a_sequence_tagger_for_Slot_Filling.html#configure-the-trainer" class="sidebar-link">Configure the trainer</a></li><li class="sidebar-sub-header"><a href="/biome-text/documentation/tutorials/Training_a_sequence_tagger_for_Slot_Filling.html#train-your-model" class="sidebar-link">Train your model</a></li><li class="sidebar-sub-header"><a href="/biome-text/documentation/tutorials/Training_a_sequence_tagger_for_Slot_Filling.html#make-your-first-predictions" class="sidebar-link">Make your first predictions</a></li></ul></li><li><a href="/biome-text/documentation/tutorials/Training_a_text_classifier.html" class="sidebar-link">Training a short text classifier of German business names</a></li></ul></section></li><li><section class="sidebar-group depth-0"><p class="sidebar-heading"><span>User Guides</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/biome-text/documentation/user-guides/01.training.html" class="sidebar-link">Model training from scratch</a></li><li><a href="/biome-text/documentation/user-guides/02.explore.html" class="sidebar-link">Model exploration and explanation</a></li><li><a href="/biome-text/documentation/user-guides/03.fine.tuning.html" class="sidebar-link">Model pre-training and fine-tuning</a></li><li><a href="/biome-text/documentation/user-guides/04.serving.html" class="sidebar-link">Model deployment</a></li><li><a href="/biome-text/documentation/user-guides/05.configuration.html" class="sidebar-link">Configuration</a></li><li><a href="/biome-text/documentation/user-guides/06.cli.html" class="sidebar-link">CLI</a></li></ul></section></li><li><section class="sidebar-group depth-0"><p class="sidebar-heading"><span>Community</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/biome-text/documentation/community/get_help.html" class="sidebar-link">Getting help</a></li></ul></section></li></ul> </aside> <main class="page" data-v-44f1b6a5> <div class="theme-default-content content__default"><h1 id="training-a-sequence-tagger-for-slot-filling"><a href="#training-a-sequence-tagger-for-slot-filling" class="header-anchor">#</a> Training a sequence tagger for Slot Filling</h1> <table><td><a target="_blank" href="https://www.recogn.ai/biome-text/documentation/tutorials/Training_a_sequence_tagger_for_Slot_Filling.html"><img src="https://www.recogn.ai/biome-text/assets/img/biome-isotype.svg" width="32">View on recogn.ai</a></td> <td><a target="_blank" href="https://colab.research.google.com/github/recognai/biome-text/blob/master/docs/docs/documentation/tutorials/Training_a_sequence_tagger_for_Slot_Filling.ipynb"><img src="https://www.tensorflow.org/images/colab_logo_32px.png">Run in Google Colab</a></td> <td><a target="_blank" href="https://github.com/recognai/biome-text/blob/master/docs/docs/documentation/tutorials/Training_a_sequence_tagger_for_Slot_Filling.ipynb"><img src="https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png" width="32">View source on GitHub</a></td></table> <p>In this tutorial we will train a sequence tagger for filling slots in spoken requests.
The goal is to look for specific pieces of information in the request and tag the corresponding tokens accordingly.
The requests will include several intents, from getting weather information to adding a song to a playlist, each requiring its own set of slots.
Therefore, slot filling often goes hand in hand with intent classification.
In this tutorial, however, we will only focus on the slot filling task.</p> <p>Slot filling is closely related to <a href="https://en.wikipedia.org/wiki/Named-entity_recognition" target="_blank" rel="noopener noreferrer">Named-entity recognition (NER)<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> and the model of this tutorial can also be used to train a NER system.</p> <p>In this tutorial we will use the <a href="https://github.com/snipsco/nlu-benchmark/tree/master/2017-06-custom-intent-engines" target="_blank" rel="noopener noreferrer">SNIPS data set<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> adapted by <a href="https://github.com/sz128/slot_filling_and_intent_detection_of_SLU/tree/master/data/snips" target="_blank" rel="noopener noreferrer">Su Zhu<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> and our simple <a href="https://biome-tutorials-data.s3-eu-west-1.amazonaws.com/token_classifier/data_prep.ipynb" target="_blank" rel="noopener noreferrer">data preparation notebook<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <p>When running this tutorial in Google Colab, make sure to install <em>biome.text</em> first:</p> <div class="language-python extra-class"><pre class="language-python"><code>!pip install <span class="token operator">-</span>U git<span class="token operator">+</span>https<span class="token punctuation">:</span><span class="token operator">//</span>github<span class="token punctuation">.</span>com<span class="token operator">/</span>recognai<span class="token operator">/</span>biome<span class="token operator">-</span>text<span class="token punctuation">.</span>git
</code></pre></div><p>Ignore warnings and don't forget to restart your runtime afterwards (<em>Runtime -&gt; Restart runtime</em>).</p> <h2 id="explore-the-data"><a href="#explore-the-data" class="header-anchor">#</a> Explore the data</h2> <p>Let's take a look at the data before starting with the configuration of our pipeline.
For this we create a <code>DataSource</code> instance providing a path to our data.</p> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">from</span> biome<span class="token punctuation">.</span>text<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataSource
</code></pre></div><div class="language-python extra-class"><pre class="language-python"><code>train_ds <span class="token operator">=</span> DataSource<span class="token punctuation">(</span>source<span class="token operator">=</span><span class="token string">&quot;https://biome-tutorials-data.s3-eu-west-1.amazonaws.com/token_classifier/train.json&quot;</span><span class="token punctuation">)</span>
train_ds<span class="token punctuation">.</span>head<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre></div><div><style scoped="scoped">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;!--beforebegin--&gt;&lt;div class=&quot;language- extra-class&quot;&gt;&lt;!--afterbegin--&gt;&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;!--beforeend--&gt;&lt;/div&gt;&lt;!--afterend--&gt;</style> <table border="1" class="dataframe"><thead><tr style="text-align:right;"><th></th> <th>text</th> <th>labels</th> <th>intent</th> <th>path</th></tr></thead> <tbody><tr><th>0</th> <td>[Find, the, schedule, for, Across, the, Line, ...</td> <td>[O, O, B-object_type, O, B-movie_name, I-movie...</td> <td>SearchScreeningEvent</td> <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td></tr> <tr><th>1</th> <td>[play, Party, Ben, on, Slacker]</td> <td>[O, B-artist, I-artist, O, B-service]</td> <td>PlayMusic</td> <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td></tr> <tr><th>2</th> <td>[play, a, 1988, soundtrack]</td> <td>[O, O, B-year, B-music_item]</td> <td>PlayMusic</td> <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td></tr> <tr><th>3</th> <td>[Can, you, play, The, Change, Is, Made, on, Ne...</td> <td>[O, O, O, B-track, I-track, I-track, I-track, ...</td> <td>PlayMusic</td> <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td></tr> <tr><th>4</th> <td>[what, is, the, forecast, for, colder, in, Ans...</td> <td>[O, O, O, O, O, B-condition_temperature, O, B-...</td> <td>GetWeather</td> <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td></tr> <tr><th>5</th> <td>[What's, the, weather, in, Totowa, WY, one, mi...</td> <td>[O, O, O, O, B-city, B-state, B-timeRange, I-t...</td> <td>GetWeather</td> <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td></tr> <tr><th>6</th> <td>[Play, a, tune, from, Space, Mandino, .]</td> <td>[O, O, B-music_item, O, B-artist, I-artist, O]</td> <td>PlayMusic</td> <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td></tr> <tr><th>7</th> <td>[give, five, out, of, 6, stars, to, current, e...</td> <td>[O, B-rating_value, O, O, B-best_rating, B-rat...</td> <td>RateBook</td> <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td></tr> <tr><th>8</th> <td>[Play, some, chanson, style, music.]</td> <td>[O, O, B-genre, O, O]</td> <td>PlayMusic</td> <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td></tr> <tr><th>9</th> <td>[I, would, give, French, Poets, and, Novelists...</td> <td>[O, O, O, B-object_name, I-object_name, I-obje...</td> <td>RateBook</td> <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td></tr></tbody></table></div> <p>As we can see we have two relevant columns for our task: <em>text</em> and <em>labels</em>.
The <em>intent</em> column will be ignored in this tutorial.
The <em>path</em> column is added automatically by the <a href="/biome-text/api/biome/text/data/datasource.html#datasource">DataSource</a> class to keep track of the source file.</p> <p>The input already comes pre-tokenized and each token in the <em>text</em> column has a label/tag in the <em>labels</em> column, this means that both list always have the same length.
The labels are given in the <a href="https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)" target="_blank" rel="noopener noreferrer">BIO tagging scheme<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>, which is widely used in Slot Filling/NER systems.</p> <p>When specifying the <a href="/biome-text/api/biome/text/modules/heads/token_classification.html#tokenclassification">TokenClassification</a> head (see <a href="#Configure-your-biome.text-Pipeline">below</a>), the tokenization step in the pipeline is automatically disabled and the input is expected to be a list of tokens.</p> <p>The <a href="/biome-text/api/biome/text/data/datasource.html#datasource">DataSource</a> class stores the data in an underlying <a href="https://docs.dask.org/en/latest/dataframe.html" target="_blank" rel="noopener noreferrer">Dask DataFrame<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> that you can easily access.
For example, let's check the size of our training data:</p> <div class="language-python extra-class"><pre class="language-python"><code><span class="token builtin">len</span><span class="token punctuation">(</span>train_ds<span class="token punctuation">.</span>to_dataframe<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre></div><div class="language- extra-class"><pre><code>13084
</code></pre></div><p>Or let's check how many different labels/tags we have:</p> <div class="language-python extra-class"><pre class="language-python"><code>df <span class="token operator">=</span> train_ds<span class="token punctuation">.</span>to_dataframe<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>compute<span class="token punctuation">(</span><span class="token punctuation">)</span>
labels_total <span class="token operator">=</span> df<span class="token punctuation">.</span>labels<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token builtin">len</span><span class="token punctuation">(</span><span class="token builtin">set</span><span class="token punctuation">(</span>labels_total<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre></div><div class="language- extra-class"><pre><code>72
</code></pre></div><p>and how they are distributed:</p> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd
pd<span class="token punctuation">.</span>Series<span class="token punctuation">(</span>labels_total<span class="token punctuation">)</span><span class="token punctuation">.</span>value_counts<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre></div><div class="language- extra-class"><pre><code>O                               59610
I-object_name                    7400
I-playlist                       3230
B-object_type                    3023
B-object_name                    2778
                                ...  
I-cuisine                          28
I-facility                         14
I-object_select                     3
I-object_part_of_series_type        3
I-playlist_owner                    1
Length: 72, dtype: int64
</code></pre></div><div class="custom-block tip"><p class="custom-block-title">Tip</p> <p>The <a href="/biome-text/api/biome/text/modules/heads/task_head.html#taskhead">TaskHead</a> of our model (the <a href="/biome-text/api/biome/text/modules/heads/token_classification.html#tokenclassification">TokenClassification</a>) will expect a <em>text</em> and a <em>labels</em> column to be present in the dataframe.
Since they are already present, there is no need for a <code>mapping</code> in the <a href="/biome-text/api/biome/text/data/datasource.html#datasource">DataSource</a>.</p></div> <h2 id="configure-your-biome-text-pipeline"><a href="#configure-your-biome-text-pipeline" class="header-anchor">#</a> Configure your <em>biome.text</em> Pipeline</h2> <p>A typical <a href="/biome-text/api/biome/text/pipeline.html#pipeline">Pipeline</a> consists of tokenizing the input, extracting features, applying a language encoding (optionally) and executing a task-specific head in the end.</p> <p>After training a pipeline, you can use it to make predictions or explore the underlying model via the <a href="/biome-text/documentation/user-guides/02.explore.html">explore UI</a>.</p> <p>As a first step we must define a configuration for our pipeline.
In this tutorial we will create a configuration dictionary and use the <code>Pipeline.from_config()</code> method to create our pipeline, but there are <a href="/biome-text/api/biome/text/pipeline.html#pipeline">other ways</a>.</p> <p>A <em>biome.text</em> pipeline has the following main components:</p> <div class="language-yaml extra-class"><pre class="language-yaml"><code><span class="token key atrule">name</span><span class="token punctuation">:</span> <span class="token comment"># a descriptive name of your pipeline</span>

<span class="token key atrule">tokenizer</span><span class="token punctuation">:</span> <span class="token comment"># how to tokenize the input</span>

<span class="token key atrule">features</span><span class="token punctuation">:</span> <span class="token comment"># input features of the model</span>

<span class="token key atrule">encoder</span><span class="token punctuation">:</span> <span class="token comment"># the language encoder</span>

<span class="token key atrule">head</span><span class="token punctuation">:</span> <span class="token comment"># your task configuration</span>

</code></pre></div><p>See the <a href="/biome-text/documentation/user-guides/05.configuration.html">Configuration section</a> for a detailed description of how these main components can be configured.</p> <p>Our complete configuration for this tutorial will be following:</p> <div class="language-python extra-class"><pre class="language-python"><code>pipeline_dict <span class="token operator">=</span> <span class="token punctuation">{</span>
    <span class="token string">&quot;name&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;slot_filling&quot;</span><span class="token punctuation">,</span>
    <span class="token string">&quot;features&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">{</span>
        <span class="token string">&quot;word&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">{</span>
            <span class="token string">&quot;embedding_dim&quot;</span><span class="token punctuation">:</span> <span class="token number">300</span><span class="token punctuation">,</span>
            <span class="token string">&quot;weights_file&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip&quot;</span><span class="token punctuation">,</span>
        <span class="token punctuation">}</span><span class="token punctuation">,</span>
        <span class="token string">&quot;char&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">{</span>
            <span class="token string">&quot;embedding_dim&quot;</span><span class="token punctuation">:</span> <span class="token number">32</span><span class="token punctuation">,</span>
            <span class="token string">&quot;encoder&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">{</span>
                <span class="token string">&quot;type&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;gru&quot;</span><span class="token punctuation">,</span>
                <span class="token string">&quot;bidirectional&quot;</span><span class="token punctuation">:</span> <span class="token boolean">True</span><span class="token punctuation">,</span>
                <span class="token string">&quot;num_layers&quot;</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span>
                <span class="token string">&quot;hidden_size&quot;</span><span class="token punctuation">:</span> <span class="token number">32</span><span class="token punctuation">,</span>
            <span class="token punctuation">}</span><span class="token punctuation">,</span>
            <span class="token string">&quot;dropout&quot;</span><span class="token punctuation">:</span> <span class="token number">0.1</span><span class="token punctuation">,</span>
        <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token string">&quot;encoder&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">{</span>
        <span class="token string">&quot;type&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;gru&quot;</span><span class="token punctuation">,</span>
        <span class="token string">&quot;hidden_size&quot;</span><span class="token punctuation">:</span> <span class="token number">128</span><span class="token punctuation">,</span>
        <span class="token string">&quot;num_layers&quot;</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span>
        <span class="token string">&quot;bidirectional&quot;</span><span class="token punctuation">:</span> <span class="token boolean">True</span><span class="token punctuation">,</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token string">&quot;head&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">{</span>
        <span class="token string">&quot;type&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;TokenClassification&quot;</span><span class="token punctuation">,</span>
        <span class="token string">&quot;labels&quot;</span><span class="token punctuation">:</span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">set</span><span class="token punctuation">(</span>labels_total<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token string">&quot;label_encoding&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;BIO&quot;</span><span class="token punctuation">,</span>
        <span class="token string">&quot;feedforward&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">{</span>
            <span class="token string">&quot;num_layers&quot;</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span>
            <span class="token string">&quot;hidden_dims&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">128</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
            <span class="token string">&quot;activations&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;relu&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
            <span class="token string">&quot;dropout&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">}</span><span class="token punctuation">,</span>
        <span class="token string">&quot;dropout&quot;</span><span class="token punctuation">:</span> <span class="token number">0.1</span><span class="token punctuation">,</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
<span class="token punctuation">}</span>
</code></pre></div><p>As you can see in the <code>features</code> section, we will use pretrained word embeddings from <a href="https://fasttext.cc/docs/en/english-vectors.html" target="_blank" rel="noopener noreferrer">fasttext<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> for our <code>word</code> features.
Keep in mind that your <code>embedding_dim</code> parameter must be equal to the dimensions of those pretrained embeddings.</p> <p>With the dictionary above we can now create a <code>Pipeline</code>.</p> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">from</span> biome<span class="token punctuation">.</span>text <span class="token keyword">import</span> Pipeline
</code></pre></div><div class="language-python extra-class"><pre class="language-python"><code>pl <span class="token operator">=</span> Pipeline<span class="token punctuation">.</span>from_config<span class="token punctuation">(</span>pipeline_dict<span class="token punctuation">)</span>
</code></pre></div><h2 id="create-a-vocabulary"><a href="#create-a-vocabulary" class="header-anchor">#</a> Create a vocabulary</h2> <p>Before we can start the training we need to create the vocabulary for our model.
For this we define a <code>VocabularyConfiguration</code>.</p> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">from</span> biome<span class="token punctuation">.</span>text <span class="token keyword">import</span> VocabularyConfiguration
</code></pre></div><p>Since we use pretrained word embeddings we will also consider the validation data when creating the vocabulary.</p> <div class="language-python extra-class"><pre class="language-python"><code>valid_ds <span class="token operator">=</span> DataSource<span class="token punctuation">(</span>source<span class="token operator">=</span><span class="token string">&quot;https://biome-tutorials-data.s3-eu-west-1.amazonaws.com/token_classifier/valid.json&quot;</span><span class="token punctuation">)</span>
</code></pre></div><p>We also get rid of the rarest words by adding the <code>min_count</code> argument and set it to 2 for the word feature vocabulary.
For a complete list of available arguments see the <a href="/biome-text/api/biome/text/configuration.html#vocabularyconfiguration">VocabularyConfiguration API</a>.</p> <div class="language-python extra-class"><pre class="language-python"><code>vocab_config <span class="token operator">=</span> VocabularyConfiguration<span class="token punctuation">(</span>
    sources<span class="token operator">=</span><span class="token punctuation">[</span>train_ds<span class="token punctuation">,</span> valid_ds<span class="token punctuation">]</span><span class="token punctuation">,</span>
    min_count<span class="token operator">=</span><span class="token punctuation">{</span><span class="token string">&quot;word&quot;</span><span class="token punctuation">:</span> <span class="token number">2</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
</code></pre></div><p>We then pass this configuration to our <code>Pipeline</code> to create the vocabulary.
Apart from the loading bar of building the vocabulary, there will be two more loading bars corresponding to the <code>weights_file</code> provided in the word feature:</p> <ul><li>the progress of downloading the file (this file will be cached)</li> <li>the progress loading the weights from the file</li></ul> <div class="language-python extra-class"><pre class="language-python"><code>pl<span class="token punctuation">.</span>create_vocabulary<span class="token punctuation">(</span>vocab_config<span class="token punctuation">)</span>
</code></pre></div><div class="language- extra-class"><pre><code>HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))






HBox(children=(FloatProgress(value=0.0, max=999994.0), HTML(value='')))
</code></pre></div><p>After creating the vovocab_configbulary we can check the size of our entire model in terms of trainable parameters:</p> <div class="language-python extra-class"><pre class="language-python"><code>pl<span class="token punctuation">.</span>trainable_parameters
</code></pre></div><div class="language- extra-class"><pre><code>1989112
</code></pre></div><h2 id="configure-the-trainer"><a href="#configure-the-trainer" class="header-anchor">#</a> Configure the trainer</h2> <p>As a next step we have to configure the <em>trainer</em>.</p> <p>For this tutorial we will use the default configuration that has sensible values and works alright for our experiment.
You can have a look at the <a href="/biome-text/api/biome/text/configuration.html#trainerconfiguration">TrainerConfiguration API</a> for a complete list of available configurations.</p> <div class="custom-block tip"><p class="custom-block-title">Tip</p> <p>In case you have a cuda device available, you also specify it here.</p></div> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">from</span> biome<span class="token punctuation">.</span>text <span class="token keyword">import</span> TrainerConfiguration
</code></pre></div><div class="language-python extra-class"><pre class="language-python"><code>trainer_config <span class="token operator">=</span> TrainerConfiguration<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre></div><h2 id="train-your-model"><a href="#train-your-model" class="header-anchor">#</a> Train your model</h2> <p>Now we have everything ready to start the training of our model:</p> <ul><li>training data set</li> <li>vocabulary</li> <li>trainer</li></ul> <p>Apart from the validation data source to estimate the generalization error, we will also pass in a test data set in case we want to do some Hyperparameter optimization and compare different encoder architectures in the end.
For this we will create another <code>DataSource</code> pointing to our test data.</p> <div class="language-python extra-class"><pre class="language-python"><code>test_ds <span class="token operator">=</span> DataSource<span class="token punctuation">(</span>source<span class="token operator">=</span><span class="token string">&quot;https://biome-tutorials-data.s3-eu-west-1.amazonaws.com/token_classifier/test.json&quot;</span><span class="token punctuation">)</span>
</code></pre></div><p>The training output will be saved in a folder specified by the <code>output</code> argument of the <code>train</code> method.
It will contain the trained model weights and the metrics, as well as the vocabulary and a <em>log</em> folder for visualizing the training process with <a href="https://www.tensorflow.org/tensorboard/" target="_blank" rel="noopener noreferrer">tensorboard<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <p>When the training has finished it will automatically make a pass over the test data with the best weights to gather the test metrics.</p> <div class="language-python extra-class"><pre class="language-python"><code>pl<span class="token punctuation">.</span>train<span class="token punctuation">(</span>
    output<span class="token operator">=</span><span class="token string">&quot;output&quot;</span><span class="token punctuation">,</span>
    training<span class="token operator">=</span>train_ds<span class="token punctuation">,</span>
    validation<span class="token operator">=</span>valid_ds<span class="token punctuation">,</span>
    test<span class="token operator">=</span>test_ds<span class="token punctuation">,</span>
    trainer<span class="token operator">=</span>trainer_config<span class="token punctuation">,</span>
<span class="token punctuation">)</span>
</code></pre></div><p>The model above achieves an overall F1 score of around <strong>0.95</strong>, which is not bad when compared to <a href="https://nlpprogress.com/english/intent_detection_slot_filling.html" target="_blank" rel="noopener noreferrer">published results<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> of the same data set.
You could continue the experiment changing the encoder to an LSTM network, try out a transformer architecture or fine tune the trainer.
But for now we will go on and make our first predictions with this trained model.</p> <h2 id="make-your-first-predictions"><a href="#make-your-first-predictions" class="header-anchor">#</a> Make your first predictions</h2> <p>Now that we trained our model we can go on to make our first predictions.
First we must load our trained model into a new <code>Pipeline</code>:</p> <div class="language-python extra-class"><pre class="language-python"><code>pl_trained <span class="token operator">=</span> Pipeline<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">&quot;output/model.tar.gz&quot;</span><span class="token punctuation">)</span>
</code></pre></div><p>We then provide the input expected by our <code>TaskHead</code> of the model to the <code>Pipeline.predict()</code> method.
In our case it is a <code>TokenClassification</code> head that classifies a <code>text</code> input. <strong>Remember that the input has to be pre-tokenized!</strong></p> <div class="language-python extra-class"><pre class="language-python"><code>text <span class="token operator">=</span> <span class="token string">&quot;can you play biome text by backstreet recognais on Spotify&quot;</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>
prediction <span class="token operator">=</span> pl_trained<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>text<span class="token punctuation">)</span>
<span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">zip</span><span class="token punctuation">(</span>text<span class="token punctuation">,</span> prediction<span class="token punctuation">[</span><span class="token string">&quot;tags&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre></div><div class="language- extra-class"><pre><code>[('can', 'O'),
 ('you', 'O'),
 ('play', 'O'),
 ('biome', 'B-track'),
 ('text', 'I-track'),
 ('by', 'O'),
 ('backstreet', 'B-artist'),
 ('recognais', 'I-artist'),
 ('on', 'O'),
 ('Spotify', 'B-service')]
</code></pre></div><p>Apart from the most likely <em>tags</em>, the <code>prediction</code> dictionary contains the <em>logits</em> and <em>probs</em> of each of the label for each of the input token.</p></div> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><span class="page-nav__button prev"><a href="/biome-text/documentation/concepts.html" class="prev"><span class="page-nav__button__icon"><vp-icon color="#4A4A4A" name="chev-left" size="18px"></vp-icon></span> <span class="page-nav__button__text">
          Concepts
        </span></a></span> <span class="page-nav__button next"><a href="/biome-text/documentation/tutorials/Training_a_text_classifier.html"><span class="page-nav__button__text">
          Training a short text classifier of German business names
        </span> <span class="page-nav__button__icon"><vp-icon color="#4A4A4A" name="chev-right" size="18px"></vp-icon></span></a></span></p></div> <footer class="footer" data-v-44f1b6a5><div data-v-44f1b6a5>
          Maintained by
          <a href="https://www.recogn.ai/" target="_blank" data-v-44f1b6a5><img width="70px" src="/biome-text/assets/img/recognai.png" class="footer__img" data-v-44f1b6a5></a></div></footer> </main></div><div class="global-ui"><!----></div></div>
    <script src="/biome-text/assets/js/app.56e7876f.js" defer></script><script src="/biome-text/assets/js/4.3b53f028.js" defer></script><script src="/biome-text/assets/js/3.e0fd2864.js" defer></script><script src="/biome-text/assets/js/59.c1fa1482.js" defer></script>
  </body>
</html>
