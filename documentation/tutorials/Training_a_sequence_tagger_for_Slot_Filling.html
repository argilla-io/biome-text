<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Training a sequence tagger for Slot Filling | biome.text</title>
    <meta name="generator" content="VuePress 1.5.2">
    <link rel="shortcut icon" href="/biome-text/favicon.ico">
    <meta name="description" content="biome.text documentation">
    <link rel="preload" href="/biome-text/assets/css/0.styles.3c97221e.css" as="style"><link rel="preload" href="/biome-text/assets/js/app.e12e8d61.js" as="script"><link rel="preload" href="/biome-text/assets/js/4.27b9dd11.js" as="script"><link rel="preload" href="/biome-text/assets/js/3.f7207660.js" as="script"><link rel="preload" href="/biome-text/assets/js/61.45c76460.js" as="script"><link rel="prefetch" href="/biome-text/assets/js/10.02cd9e9b.js"><link rel="prefetch" href="/biome-text/assets/js/11.4d18739e.js"><link rel="prefetch" href="/biome-text/assets/js/12.df713edf.js"><link rel="prefetch" href="/biome-text/assets/js/13.ac0c49db.js"><link rel="prefetch" href="/biome-text/assets/js/14.83976745.js"><link rel="prefetch" href="/biome-text/assets/js/15.5e6aac58.js"><link rel="prefetch" href="/biome-text/assets/js/16.cd794fbe.js"><link rel="prefetch" href="/biome-text/assets/js/17.8dc1ddeb.js"><link rel="prefetch" href="/biome-text/assets/js/18.eea62f08.js"><link rel="prefetch" href="/biome-text/assets/js/19.32a1716d.js"><link rel="prefetch" href="/biome-text/assets/js/20.d580a379.js"><link rel="prefetch" href="/biome-text/assets/js/21.d05cdab4.js"><link rel="prefetch" href="/biome-text/assets/js/22.4004813d.js"><link rel="prefetch" href="/biome-text/assets/js/23.d9482dc6.js"><link rel="prefetch" href="/biome-text/assets/js/24.fc883fda.js"><link rel="prefetch" href="/biome-text/assets/js/25.b9474e98.js"><link rel="prefetch" href="/biome-text/assets/js/26.c861b13f.js"><link rel="prefetch" href="/biome-text/assets/js/27.8eb228d1.js"><link rel="prefetch" href="/biome-text/assets/js/28.74d70fd1.js"><link rel="prefetch" href="/biome-text/assets/js/29.bbafb6cf.js"><link rel="prefetch" href="/biome-text/assets/js/30.0b7e6a2a.js"><link rel="prefetch" href="/biome-text/assets/js/31.6d04e912.js"><link rel="prefetch" href="/biome-text/assets/js/32.615b3b0e.js"><link rel="prefetch" href="/biome-text/assets/js/33.083441d3.js"><link rel="prefetch" href="/biome-text/assets/js/34.13353a0e.js"><link rel="prefetch" href="/biome-text/assets/js/35.ba60f43d.js"><link rel="prefetch" href="/biome-text/assets/js/36.3d9278c8.js"><link rel="prefetch" href="/biome-text/assets/js/37.ad8ba9b1.js"><link rel="prefetch" href="/biome-text/assets/js/38.caf96e78.js"><link rel="prefetch" href="/biome-text/assets/js/39.84b8d401.js"><link rel="prefetch" href="/biome-text/assets/js/40.a6a8e46b.js"><link rel="prefetch" href="/biome-text/assets/js/41.3828a7aa.js"><link rel="prefetch" href="/biome-text/assets/js/42.561b2990.js"><link rel="prefetch" href="/biome-text/assets/js/43.28ac1f59.js"><link rel="prefetch" href="/biome-text/assets/js/44.4c84e4b4.js"><link rel="prefetch" href="/biome-text/assets/js/45.de972bd6.js"><link rel="prefetch" href="/biome-text/assets/js/46.d6391e5f.js"><link rel="prefetch" href="/biome-text/assets/js/47.395e4a1d.js"><link rel="prefetch" href="/biome-text/assets/js/48.3c4c2c56.js"><link rel="prefetch" href="/biome-text/assets/js/49.64039324.js"><link rel="prefetch" href="/biome-text/assets/js/5.7525e915.js"><link rel="prefetch" href="/biome-text/assets/js/50.b1e7e473.js"><link rel="prefetch" href="/biome-text/assets/js/51.d023973f.js"><link rel="prefetch" href="/biome-text/assets/js/52.010c86ad.js"><link rel="prefetch" href="/biome-text/assets/js/53.48ccf23c.js"><link rel="prefetch" href="/biome-text/assets/js/54.944ad0be.js"><link rel="prefetch" href="/biome-text/assets/js/55.e61f1f85.js"><link rel="prefetch" href="/biome-text/assets/js/56.c0e3f0d1.js"><link rel="prefetch" href="/biome-text/assets/js/57.8950ecbd.js"><link rel="prefetch" href="/biome-text/assets/js/58.2307e0f6.js"><link rel="prefetch" href="/biome-text/assets/js/59.ce33bf9f.js"><link rel="prefetch" href="/biome-text/assets/js/6.26eee5a8.js"><link rel="prefetch" href="/biome-text/assets/js/60.5e01fd10.js"><link rel="prefetch" href="/biome-text/assets/js/62.026f4520.js"><link rel="prefetch" href="/biome-text/assets/js/63.a45fb54e.js"><link rel="prefetch" href="/biome-text/assets/js/7.97753d4d.js"><link rel="prefetch" href="/biome-text/assets/js/8.73045528.js"><link rel="prefetch" href="/biome-text/assets/js/9.b591a112.js"><link rel="prefetch" href="/biome-text/assets/js/vendors~docsearch.9f78e3e8.js">
    <link rel="stylesheet" href="/biome-text/assets/css/0.styles.3c97221e.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container" data-v-44f1b6a5><header class="navbar" data-v-44f1b6a5><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/biome-text/" class="home-link router-link-active"><!----> <span class="site-name">biome<span>.text</span></span></a> <div class="links"><form id="search-form" role="search" class="algolia-search-wrapper search-box"><input id="algolia-search-input" class="search-query"></form> <nav class="nav-links can-hide"><div class="nav-item"><a href="/biome-text/api/" class="nav-link">
  API
</a></div><div class="nav-item"><a href="/biome-text/documentation/" class="nav-link router-link-active">
  Documentation
</a></div><div class="nav-item"><a href="https://github.com/recognai/biome-text" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span class="external__icon"><vp-icon color="#4A4A4A" name="blank" size="12px"></vp-icon></span></a></div><div class="nav-item"><a href="https://recogn.ai" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Recognai
  <span class="external__icon"><vp-icon color="#4A4A4A" name="blank" size="12px"></vp-icon></span></a></div> <!----></nav></div></header> <div class="sidebar-mask" data-v-44f1b6a5></div> <aside class="sidebar" data-v-44f1b6a5><div class="sidebar__link"><a href="/biome-text/"><img src="/biome-text/assets/img/biome.svg" class="sidebar__img"></a></div> <nav class="nav-links"><div class="nav-item"><a href="/biome-text/api/" class="nav-link">
  API
</a></div><div class="nav-item"><a href="/biome-text/documentation/" class="nav-link router-link-active">
  Documentation
</a></div><div class="nav-item"><a href="https://github.com/recognai/biome-text" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span class="external__icon"><vp-icon color="#4A4A4A" name="blank" size="12px"></vp-icon></span></a></div><div class="nav-item"><a href="https://recogn.ai" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Recognai
  <span class="external__icon"><vp-icon color="#4A4A4A" name="blank" size="12px"></vp-icon></span></a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group depth-0"><p class="sidebar-heading"><span>Get started</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/biome-text/documentation/" aria-current="page" class="sidebar-link">Installation</a></li><li><a href="/biome-text/documentation/basics.html" class="sidebar-link">The basics</a></li></ul></section></li><li><section class="sidebar-group depth-0"><p class="sidebar-heading open"><span>Tutorials</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/biome-text/documentation/tutorials/Hyperparameter_optimization_with_Ray_Tune.html" class="sidebar-link">Hyperparameter optimization with Ray Tune</a></li><li><a href="/biome-text/documentation/tutorials/Training_a_sequence_tagger_for_Slot_Filling.html" aria-current="page" class="active sidebar-link">Training a sequence tagger for Slot Filling</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/biome-text/documentation/tutorials/Training_a_sequence_tagger_for_Slot_Filling.html#explore-the-data" class="sidebar-link">Explore the data</a></li><li class="sidebar-sub-header"><a href="/biome-text/documentation/tutorials/Training_a_sequence_tagger_for_Slot_Filling.html#configure-your-biome-text-pipeline" class="sidebar-link">Configure your biome.text Pipeline</a></li><li class="sidebar-sub-header"><a href="/biome-text/documentation/tutorials/Training_a_sequence_tagger_for_Slot_Filling.html#create-a-vocabulary" class="sidebar-link">Create a vocabulary</a></li><li class="sidebar-sub-header"><a href="/biome-text/documentation/tutorials/Training_a_sequence_tagger_for_Slot_Filling.html#train-your-model" class="sidebar-link">Train your model</a></li><li class="sidebar-sub-header"><a href="/biome-text/documentation/tutorials/Training_a_sequence_tagger_for_Slot_Filling.html#make-your-first-predictions" class="sidebar-link">Make your first predictions</a></li></ul></li><li><a href="/biome-text/documentation/tutorials/Training_a_text_classifier.html" class="sidebar-link">Training a short text classifier of German business names</a></li></ul></section></li><li><section class="sidebar-group depth-0"><p class="sidebar-heading"><span>User Guides</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/biome-text/documentation/user-guides/1-nlp-tasks.html" class="sidebar-link">NLP Tasks</a></li><li><a href="/biome-text/documentation/user-guides/2-configuration.html" class="sidebar-link">Configuration</a></li></ul></section></li><li><section class="sidebar-group depth-0"><p class="sidebar-heading"><span>Community</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/biome-text/documentation/community/get_help.html" class="sidebar-link">Getting help</a></li></ul></section></li></ul> </aside> <main class="page" data-v-44f1b6a5> <div class="theme-default-content content__default"><h1 id="training-a-sequence-tagger-for-slot-filling"><a href="#training-a-sequence-tagger-for-slot-filling" class="header-anchor">#</a> Training a sequence tagger for Slot Filling</h1> <p><a target="_blank" href="https://www.recogn.ai/biome-text/documentation/tutorials/Training_a_sequence_tagger_for_Slot_Filling.html"><img src="https://www.recogn.ai/biome-text/assets/img/biome-isotype.svg" width="24" class="icon"></a> <a href="https://www.recogn.ai/biome-text/documentation/tutorials/Training_a_sequence_tagger_for_Slot_Filling.html" target="_blank" rel="noopener noreferrer">View on recogn.ai<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p><a target="_blank" href="https://colab.research.google.com/github/recognai/biome-text/blob/master/docs/docs/documentation/tutorials/Training_a_sequence_tagger_for_Slot_Filling.ipynb"><img src="https://www.tensorflow.org/images/colab_logo_32px.png" width="24" class="icon"></a> <a href="https://colab.research.google.com/github/recognai/biome-text/blob/master/docs/docs/documentation/tutorials/Training_a_sequence_tagger_for_Slot_Filling.ipynb" target="_blank" rel="noopener noreferrer">Run in Google Colab<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p><a target="_blank" href="https://github.com/recognai/biome-text/blob/master/docs/docs/documentation/tutorials/Training_a_sequence_tagger_for_Slot_Filling.ipynb"><img src="https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png" width="24" class="icon"></a> <a href="https://github.com/recognai/biome-text/blob/master/docs/docs/documentation/tutorials/Training_a_sequence_tagger_for_Slot_Filling.ipynb" target="_blank" rel="noopener noreferrer">View source on GitHub<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p>In this tutorial we will train a sequence tagger for filling slots in spoken requests.
The goal is to look for specific pieces of information in the request and tag the corresponding tokens accordingly.
The requests will include several intents, from getting weather information to adding a song to a playlist, each requiring its own set of slots.
Therefore, slot filling often goes hand in hand with intent classification.
In this tutorial, however, we will only focus on the slot filling task.</p> <p>Slot filling is closely related to <a href="https://en.wikipedia.org/wiki/Named-entity_recognition" target="_blank" rel="noopener noreferrer">Named-entity recognition (NER)<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> and the model of this tutorial can also be used to train a NER system.</p> <p>In this tutorial we will use the <a href="https://github.com/snipsco/nlu-benchmark/tree/master/2017-06-custom-intent-engines" target="_blank" rel="noopener noreferrer">SNIPS data set<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> adapted by <a href="https://github.com/sz128/slot_filling_and_intent_detection_of_SLU/tree/master/data/snips" target="_blank" rel="noopener noreferrer">Su Zhu<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> and our simple <a href="https://biome-tutorials-data.s3-eu-west-1.amazonaws.com/token_classifier/data_prep.ipynb" target="_blank" rel="noopener noreferrer">data preparation notebook<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <p>When running this tutorial in Google Colab, make sure to install <em>biome.text</em> first:</p> <div class="language-python extra-class"><pre class="language-python"><code>!pip install <span class="token operator">-</span>U git<span class="token operator">+</span>https<span class="token punctuation">:</span><span class="token operator">//</span>github<span class="token punctuation">.</span>com<span class="token operator">/</span>recognai<span class="token operator">/</span>biome<span class="token operator">-</span>text<span class="token punctuation">.</span>git
</code></pre></div><p>Ignore warnings and don't forget to restart your runtime afterwards (<em>Runtime -&gt; Restart runtime</em>).</p> <h2 id="explore-the-data"><a href="#explore-the-data" class="header-anchor">#</a> Explore the data</h2> <p>Let's take a look at the data before starting with the configuration of our pipeline.
For this we create a <code>DataSource</code> instance providing a path to our data.</p> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">from</span> biome<span class="token punctuation">.</span>text<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataSource
</code></pre></div><div class="language-python extra-class"><pre class="language-python"><code>train_ds <span class="token operator">=</span> DataSource<span class="token punctuation">(</span>source<span class="token operator">=</span><span class="token string">&quot;https://biome-tutorials-data.s3-eu-west-1.amazonaws.com/token_classifier/train.json&quot;</span><span class="token punctuation">)</span>
train_ds<span class="token punctuation">.</span>head<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre></div><div><style scoped="scoped">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;!--beforebegin--&gt;&lt;div class=&quot;language- extra-class&quot;&gt;&lt;!--afterbegin--&gt;&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;!--beforeend--&gt;&lt;/div&gt;&lt;!--afterend--&gt;</style> <table border="1" class="dataframe"><thead><tr style="text-align:right;"><th></th> <th>text</th> <th>labels</th> <th>intent</th> <th>path</th></tr></thead> <tbody><tr><th>0</th> <td>[Find, the, schedule, for, Across, the, Line, ...</td> <td>[O, O, B-object_type, O, B-movie_name, I-movie...</td> <td>SearchScreeningEvent</td> <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td></tr> <tr><th>1</th> <td>[play, Party, Ben, on, Slacker]</td> <td>[O, B-artist, I-artist, O, B-service]</td> <td>PlayMusic</td> <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td></tr> <tr><th>2</th> <td>[play, a, 1988, soundtrack]</td> <td>[O, O, B-year, B-music_item]</td> <td>PlayMusic</td> <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td></tr> <tr><th>3</th> <td>[Can, you, play, The, Change, Is, Made, on, Ne...</td> <td>[O, O, O, B-track, I-track, I-track, I-track, ...</td> <td>PlayMusic</td> <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td></tr> <tr><th>4</th> <td>[what, is, the, forecast, for, colder, in, Ans...</td> <td>[O, O, O, O, O, B-condition_temperature, O, B-...</td> <td>GetWeather</td> <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td></tr> <tr><th>5</th> <td>[What's, the, weather, in, Totowa, WY, one, mi...</td> <td>[O, O, O, O, B-city, B-state, B-timeRange, I-t...</td> <td>GetWeather</td> <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td></tr> <tr><th>6</th> <td>[Play, a, tune, from, Space, Mandino, .]</td> <td>[O, O, B-music_item, O, B-artist, I-artist, O]</td> <td>PlayMusic</td> <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td></tr> <tr><th>7</th> <td>[give, five, out, of, 6, stars, to, current, e...</td> <td>[O, B-rating_value, O, O, B-best_rating, B-rat...</td> <td>RateBook</td> <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td></tr> <tr><th>8</th> <td>[Play, some, chanson, style, music.]</td> <td>[O, O, B-genre, O, O]</td> <td>PlayMusic</td> <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td></tr> <tr><th>9</th> <td>[I, would, give, French, Poets, and, Novelists...</td> <td>[O, O, O, B-object_name, I-object_name, I-obje...</td> <td>RateBook</td> <td>https://biome-tutorials-data.s3-eu-west-1.amaz...</td></tr></tbody></table></div> <p>As we can see we have two relevant columns for our task: <em>text</em> and <em>labels</em>.
The <em>intent</em> column will be ignored in this tutorial.
The <em>path</em> column is added automatically by the <a href="/biome-text/api/biome/text/data/datasource.html#datasource">DataSource</a> class to keep track of the source file.</p> <p>The input already comes pre-tokenized and each token in the <em>text</em> column has a label/tag in the <em>labels</em> column, this means that both list always have the same length.
The labels are given in the <a href="https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)" target="_blank" rel="noopener noreferrer">BIO tagging scheme<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>, which is widely used in Slot Filling/NER systems.</p> <p>When specifying the <a href="/biome-text/api/biome/text/modules/heads/token_classification.html#tokenclassification">TokenClassification</a> head (see <a href="#Configure-your-biome.text-Pipeline">below</a>), the tokenization step in the pipeline is automatically disabled and the input is expected to be a list of tokens.</p> <p>The <a href="/biome-text/api/biome/text/data/datasource.html#datasource">DataSource</a> class stores the data in an underlying <a href="https://docs.dask.org/en/latest/dataframe.html" target="_blank" rel="noopener noreferrer">Dask DataFrame<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> that you can easily access.
For example, let's check the size of our training data:</p> <div class="language-python extra-class"><pre class="language-python"><code><span class="token builtin">len</span><span class="token punctuation">(</span>train_ds<span class="token punctuation">.</span>to_dataframe<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre></div><div class="language- extra-class"><pre><code>13084
</code></pre></div><p>Or let's check how many different labels/tags we have:</p> <div class="language-python extra-class"><pre class="language-python"><code>df <span class="token operator">=</span> train_ds<span class="token punctuation">.</span>to_dataframe<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>compute<span class="token punctuation">(</span><span class="token punctuation">)</span>
labels_total <span class="token operator">=</span> df<span class="token punctuation">.</span>labels<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token builtin">len</span><span class="token punctuation">(</span><span class="token builtin">set</span><span class="token punctuation">(</span>labels_total<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre></div><div class="language- extra-class"><pre><code>72
</code></pre></div><p>and how they are distributed:</p> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd
pd<span class="token punctuation">.</span>Series<span class="token punctuation">(</span>labels_total<span class="token punctuation">)</span><span class="token punctuation">.</span>value_counts<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre></div><div class="language- extra-class"><pre><code>O                               59610
I-object_name                    7400
I-playlist                       3230
B-object_type                    3023
B-object_name                    2778
                                ...  
I-cuisine                          28
I-facility                         14
I-object_part_of_series_type        3
I-object_select                     3
I-playlist_owner                    1
Length: 72, dtype: int64
</code></pre></div><div class="custom-block tip"><p class="custom-block-title">Tip</p> <p>The <a href="/biome-text/api/biome/text/modules/heads/task_head.html#taskhead">TaskHead</a> of our model (the <a href="/biome-text/api/biome/text/modules/heads/token_classification.html#tokenclassification">TokenClassification</a>) will expect a <em>text</em> and a <em>labels</em> column to be present in the dataframe.
Since they are already present, there is no need for a <code>mapping</code> in the <a href="/biome-text/api/biome/text/data/datasource.html#datasource">DataSource</a>.</p></div> <h2 id="configure-your-biome-text-pipeline"><a href="#configure-your-biome-text-pipeline" class="header-anchor">#</a> Configure your <em>biome.text</em> Pipeline</h2> <p>A typical <a href="/biome-text/api/biome/text/pipeline.html#pipeline">Pipeline</a> consists of tokenizing the input, extracting features, applying a language encoding (optionally) and executing a task-specific head in the end.
After training a pipeline, you can use it to make predictions or explore the underlying model via the <a href="/biome-text/documentation/user-guides/02.explore.html">explore UI</a>.</p> <p>A <em>biome.text</em> pipeline has the following main components:</p> <div class="language-yaml extra-class"><pre class="language-yaml"><code><span class="token key atrule">name</span><span class="token punctuation">:</span> <span class="token comment"># a descriptive name of your pipeline</span>

<span class="token key atrule">tokenizer</span><span class="token punctuation">:</span> <span class="token comment"># how to tokenize the input</span>

<span class="token key atrule">features</span><span class="token punctuation">:</span> <span class="token comment"># input features of the model</span>

<span class="token key atrule">encoder</span><span class="token punctuation">:</span> <span class="token comment"># the language encoder</span>

<span class="token key atrule">head</span><span class="token punctuation">:</span> <span class="token comment"># your task configuration</span>

</code></pre></div><p>See the <a href="/biome-text/documentation/user-guides/05.configuration.html">Configuration section</a> for a detailed description of how these main components can be configured.</p> <p>In this tutorial we will create a <a href="/biome-text/api/biome/text/configuration.html#pipelineconfiguration">PipelineConfiguration</a> programmatically, and use it to initialize the <a href="/biome-text/api/biome/text/pipeline.html#pipeline">Pipeline</a>.
You can also create your pipelines by providing a <a href="/biome-text/api/biome/text/pipeline.html#from-config">python dictionary</a> (see the text classification <a href="/biome-text/documentation/tutorials/Training_a_text_classifier.html">tutorial</a>), a YAML <a href="/biome-text/api/biome/text/pipeline.html#from-yaml">configuration file</a> or a <a href="/biome-text/api/biome/text/pipeline.html#from-pretrained">pretrained model</a>.</p> <p>A pipeline configuration is composed of several other <a href="/biome-text/api/biome/text/configuration.html#biome-text-configuration">configuration classes</a>, each one corresponding to one of the main components.</p> <h3 id="features"><a href="#features" class="header-anchor">#</a> Features</h3> <p>Let us first configure the features of our pipeline.
For our <code>word</code> feature we will use pretrained embeddings from <a href="https://fasttext.cc/docs/en/english-vectors.html" target="_blank" rel="noopener noreferrer">fasttext<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>, and our <code>char</code> feature will use the last hidden state of a <a href="https://en.wikipedia.org/wiki/Gated_recurrent_unit" target="_blank" rel="noopener noreferrer">GRU<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> encoder to represent a word based on its characters.
Keep in mind that the <code>embedding_dim</code> parameter for the <code>word</code> feature must be equal to the dimensions of the pretrained embeddings!</p> <div class="custom-block tip"><p class="custom-block-title">Tip</p> <p>If you do not provide any feature configurations, we will choose a very basic <code>word</code> feature by default.</p></div> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">from</span> biome<span class="token punctuation">.</span>text<span class="token punctuation">.</span>configuration <span class="token keyword">import</span> FeaturesConfiguration<span class="token punctuation">,</span> WordFeatures<span class="token punctuation">,</span> CharFeatures

word_feature <span class="token operator">=</span> WordFeatures<span class="token punctuation">(</span>
    embedding_dim<span class="token operator">=</span><span class="token number">300</span><span class="token punctuation">,</span>
    weights_file<span class="token operator">=</span><span class="token string">&quot;https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip&quot;</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>

char_feature <span class="token operator">=</span> CharFeatures<span class="token punctuation">(</span>
    embedding_dim<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>
    encoder<span class="token operator">=</span><span class="token punctuation">{</span>
        <span class="token string">&quot;type&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;gru&quot;</span><span class="token punctuation">,</span>
        <span class="token string">&quot;bidirectional&quot;</span><span class="token punctuation">:</span> <span class="token boolean">True</span><span class="token punctuation">,</span>
        <span class="token string">&quot;num_layers&quot;</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span>
        <span class="token string">&quot;hidden_size&quot;</span><span class="token punctuation">:</span> <span class="token number">32</span><span class="token punctuation">,</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    dropout<span class="token operator">=</span><span class="token number">0.1</span>
<span class="token punctuation">)</span>

features_config <span class="token operator">=</span> FeaturesConfiguration<span class="token punctuation">(</span>
    word<span class="token operator">=</span>word_feature<span class="token punctuation">,</span> 
    char<span class="token operator">=</span>char_feature
<span class="token punctuation">)</span>
</code></pre></div><h3 id="encoder"><a href="#encoder" class="header-anchor">#</a> Encoder</h3> <p>Next we will configure our encoder that takes as input a sequence of embedded word vectors and returns a sequence of encoded word vectors.
For this encoding we will use another larger GRU:</p> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">from</span> biome<span class="token punctuation">.</span>text<span class="token punctuation">.</span>modules<span class="token punctuation">.</span>configuration <span class="token keyword">import</span> Seq2SeqEncoderConfiguration

encoder_config <span class="token operator">=</span> Seq2SeqEncoderConfiguration<span class="token punctuation">(</span>
    <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">&quot;gru&quot;</span><span class="token punctuation">,</span>
    bidirectional<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    num_layers<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
    hidden_size<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
</code></pre></div><h3 id="head"><a href="#head" class="header-anchor">#</a> Head</h3> <p>The final configuration belongs to our <a href="/biome-text/api/biome/text/modules/heads/task_head.html#taskhead">TaskHead</a>.
It reflects the task our problem belongs to and can be easily exchanged with other types of heads keeping the same features and encoder.</p> <div class="custom-block tip"><p class="custom-block-title">Tip</p> <p>Exchanging the heads you can easily pretrain a model on a certain task, such as <a href="/biome-text/api/biome/text/modules/heads/language_modelling.html#languagemodelling">language modelling</a>, and use its pretrained features and encoder for training the model on another task.</p></div> <p>For our task we will use a <a href="/biome-text/api/biome/text/modules/heads/token_classification.html#tokenclassification">TokenClassification</a> head that allows us to tag each token individually:</p> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">from</span> biome<span class="token punctuation">.</span>text<span class="token punctuation">.</span>modules<span class="token punctuation">.</span>heads <span class="token keyword">import</span> TokenClassificationConfiguration

head_config <span class="token operator">=</span> TokenClassificationConfiguration<span class="token punctuation">(</span>
    labels<span class="token operator">=</span><span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">set</span><span class="token punctuation">(</span>labels_total<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    label_encoding<span class="token operator">=</span><span class="token string">&quot;BIO&quot;</span><span class="token punctuation">,</span>
    feedforward<span class="token operator">=</span><span class="token punctuation">{</span>
        <span class="token string">&quot;num_layers&quot;</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span>
        <span class="token string">&quot;hidden_dims&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">128</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token string">&quot;activations&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">&quot;relu&quot;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token string">&quot;dropout&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
</code></pre></div><h3 id="pipeline"><a href="#pipeline" class="header-anchor">#</a> Pipeline</h3> <p>Now we can create a <a href="/biome-text/api/biome/text/configuration.html#pipelineconfiguration">PipelineConfiguration</a> and finally initialize our <a href="/biome-text/api/biome/text/pipeline.html#pipeline">Pipeline</a>.</p> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">from</span> biome<span class="token punctuation">.</span>text<span class="token punctuation">.</span>configuration <span class="token keyword">import</span> PipelineConfiguration

pipeline_config <span class="token operator">=</span> PipelineConfiguration<span class="token punctuation">(</span>
    name<span class="token operator">=</span><span class="token string">&quot;slot_filling_tutorial&quot;</span><span class="token punctuation">,</span>
    features<span class="token operator">=</span>features_config<span class="token punctuation">,</span>
    encoder<span class="token operator">=</span>encoder_config<span class="token punctuation">,</span>
    head<span class="token operator">=</span>head_config<span class="token punctuation">,</span>
<span class="token punctuation">)</span>
</code></pre></div><div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">from</span> biome<span class="token punctuation">.</span>text <span class="token keyword">import</span> Pipeline

pl <span class="token operator">=</span> Pipeline<span class="token punctuation">.</span>from_config<span class="token punctuation">(</span>pipeline_config<span class="token punctuation">)</span>
</code></pre></div><h2 id="create-a-vocabulary"><a href="#create-a-vocabulary" class="header-anchor">#</a> Create a vocabulary</h2> <p>Before we can start the training we need to create the vocabulary for our model.
For this we define a <code>VocabularyConfiguration</code>.</p> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">from</span> biome<span class="token punctuation">.</span>text <span class="token keyword">import</span> VocabularyConfiguration
</code></pre></div><p>Since we use pretrained word embeddings we will also consider the validation data when creating the vocabulary.</p> <div class="language-python extra-class"><pre class="language-python"><code>valid_ds <span class="token operator">=</span> DataSource<span class="token punctuation">(</span>source<span class="token operator">=</span><span class="token string">&quot;https://biome-tutorials-data.s3-eu-west-1.amazonaws.com/token_classifier/valid.json&quot;</span><span class="token punctuation">)</span>
</code></pre></div><p>We also get rid of the rarest words by adding the <code>min_count</code> argument and set it to 2 for the word feature vocabulary.
For a complete list of available arguments see the <a href="/biome-text/api/biome/text/configuration.html#vocabularyconfiguration">VocabularyConfiguration API</a>.</p> <div class="language-python extra-class"><pre class="language-python"><code>vocab_config <span class="token operator">=</span> VocabularyConfiguration<span class="token punctuation">(</span>
    sources<span class="token operator">=</span><span class="token punctuation">[</span>train_ds<span class="token punctuation">,</span> valid_ds<span class="token punctuation">]</span><span class="token punctuation">,</span>
    min_count<span class="token operator">=</span><span class="token punctuation">{</span><span class="token string">&quot;word&quot;</span><span class="token punctuation">:</span> <span class="token number">2</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
</code></pre></div><p>We then pass this configuration to our <code>Pipeline</code> to create the vocabulary.
Apart from the loading bar of building the vocabulary, there will be two more loading bars corresponding to the <code>weights_file</code> provided in the word feature:</p> <ul><li>the progress of downloading the file (this file will be cached)</li> <li>the progress loading the weights from the file</li></ul> <div class="language-python extra-class"><pre class="language-python"><code>pl<span class="token punctuation">.</span>create_vocabulary<span class="token punctuation">(</span>vocab_config<span class="token punctuation">)</span>
</code></pre></div><div class="language- extra-class"><pre><code>HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))






HBox(children=(FloatProgress(value=0.0, max=999994.0), HTML(value='')))
</code></pre></div><p>After creating the vovocab_configbulary we can check the size of our entire model in terms of trainable parameters:</p> <div class="language-python extra-class"><pre class="language-python"><code>pl<span class="token punctuation">.</span>trainable_parameters
</code></pre></div><div class="language- extra-class"><pre><code>1989112
</code></pre></div><h2 id="train-your-model"><a href="#train-your-model" class="header-anchor">#</a> Train your model</h2> <p>Now we have everything ready to start the training of our model:</p> <ul><li>training data set</li> <li>vocabulary</li></ul> <p>As <code>trainer</code> we will use the default configuration that has sensible values and works alright for our experiment.
<a href="/biome-text/documentation/tutorials/Training_a_text_classifier.html#configure-the-trainer">This tutorial</a> shows you an example of how to configure a trainer.</p> <div class="custom-block tip"><p class="custom-block-title">Tip</p> <p>If you want to tune the trainer or use a cuda device, you can pass a <code>trainer = TrainerConfiguration(cuda_device=0, ...)</code> to the <code>Pipeline.train()</code> method.
See the <a href="/biome-text/api/biome/text/configuration.html#trainerconfiguration">TrainerConfiguration API</a> for a complete list of available configurations.</p></div> <p>Apart from the validation data source to estimate the generalization error, we will also pass in a test data set in case we want to do some Hyperparameter optimization and compare different encoder architectures in the end.
For this we will create another <code>DataSource</code> pointing to our test data.</p> <div class="language-python extra-class"><pre class="language-python"><code>test_ds <span class="token operator">=</span> DataSource<span class="token punctuation">(</span>source<span class="token operator">=</span><span class="token string">&quot;https://biome-tutorials-data.s3-eu-west-1.amazonaws.com/token_classifier/test.json&quot;</span><span class="token punctuation">)</span>
</code></pre></div><p>The training output will be saved in a folder specified by the <code>output</code> argument of the <code>train</code> method.
It will contain the trained model weights and the metrics, as well as the vocabulary and a <em>log</em> folder for visualizing the training process with <a href="https://www.tensorflow.org/tensorboard/" target="_blank" rel="noopener noreferrer">tensorboard<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <p>When the training has finished it will automatically make a pass over the test data with the best weights to gather the test metrics.</p> <div class="language-python extra-class"><pre class="language-python"><code>pl<span class="token punctuation">.</span>train<span class="token punctuation">(</span>
    output<span class="token operator">=</span><span class="token string">&quot;output&quot;</span><span class="token punctuation">,</span>
    training<span class="token operator">=</span>train_ds<span class="token punctuation">,</span>
    validation<span class="token operator">=</span>valid_ds<span class="token punctuation">,</span>
    test<span class="token operator">=</span>test_ds<span class="token punctuation">,</span>
<span class="token punctuation">)</span>
</code></pre></div><p>The model above achieves an overall F1 score of around <strong>0.95</strong>, which is not bad when compared to <a href="https://nlpprogress.com/english/intent_detection_slot_filling.html" target="_blank" rel="noopener noreferrer">published results<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> of the same data set.
You could continue the experiment changing the encoder to an LSTM network, try out a transformer architecture or fine tune the trainer.
But for now we will go on and make our first predictions with this trained model.</p> <h2 id="make-your-first-predictions"><a href="#make-your-first-predictions" class="header-anchor">#</a> Make your first predictions</h2> <p>Now that we trained our model we can go on to make our first predictions.
First we must load our trained model into a new <code>Pipeline</code>:</p> <div class="language-python extra-class"><pre class="language-python"><code>pl_trained <span class="token operator">=</span> Pipeline<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">&quot;output/model.tar.gz&quot;</span><span class="token punctuation">)</span>
</code></pre></div><p>We then provide the input expected by our <code>TaskHead</code> of the model to the <code>Pipeline.predict()</code> method.
In our case it is a <code>TokenClassification</code> head that classifies a <code>text</code> input. <strong>Remember that the input has to be pre-tokenized!</strong></p> <div class="language-python extra-class"><pre class="language-python"><code>text <span class="token operator">=</span> <span class="token string">&quot;can you play biome text by backstreet recognais on Spotify&quot;</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>
prediction <span class="token operator">=</span> pl_trained<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>text<span class="token punctuation">)</span>
<span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">zip</span><span class="token punctuation">(</span>text<span class="token punctuation">,</span> prediction<span class="token punctuation">[</span><span class="token string">&quot;tags&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre></div><div class="language- extra-class"><pre><code>[('can', 'O'),
 ('you', 'O'),
 ('play', 'O'),
 ('biome', 'B-track'),
 ('text', 'I-track'),
 ('by', 'O'),
 ('backstreet', 'B-artist'),
 ('recognais', 'I-artist'),
 ('on', 'O'),
 ('Spotify', 'B-service')]
</code></pre></div><p>Apart from the most likely <em>tags</em>, the <code>prediction</code> dictionary contains the <em>logits</em> and <em>probs</em> of each of the label for each of the input token.</p></div> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><span class="page-nav__button prev"><a href="/biome-text/documentation/tutorials/Hyperparameter_optimization_with_Ray_Tune.html" class="prev"><span class="page-nav__button__icon"><vp-icon color="#4A4A4A" name="chev-left" size="18px"></vp-icon></span> <span class="page-nav__button__text">
          Hyperparameter optimization with Ray Tune
        </span></a></span> <span class="page-nav__button next"><a href="/biome-text/documentation/tutorials/Training_a_text_classifier.html"><span class="page-nav__button__text">
          Training a short text classifier of German business names
        </span> <span class="page-nav__button__icon"><vp-icon color="#4A4A4A" name="chev-right" size="18px"></vp-icon></span></a></span></p></div> <footer class="footer" data-v-44f1b6a5><div data-v-44f1b6a5>
          Maintained by
          <a href="https://www.recogn.ai/" target="_blank" data-v-44f1b6a5><img width="70px" src="/biome-text/assets/img/recognai.png" class="footer__img" data-v-44f1b6a5></a></div></footer> </main></div><div class="global-ui"><!----></div></div>
    <script src="/biome-text/assets/js/app.e12e8d61.js" defer></script><script src="/biome-text/assets/js/4.27b9dd11.js" defer></script><script src="/biome-text/assets/js/3.f7207660.js" defer></script><script src="/biome-text/assets/js/61.45c76460.js" defer></script>
  </body>
</html>
