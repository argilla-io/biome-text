(window.webpackJsonp=window.webpackJsonp||[]).push([[31],{418:function(e,t,a){"use strict";a.r(t);var s=a(26),n=Object(s.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h1",{attrs:{id:"biome-text-features"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#biome-text-features"}},[e._v("#")]),e._v(" biome.text.features "),a("Badge",{attrs:{text:"Module"}})],1),e._v(" "),a("div"),e._v(" "),a("div"),e._v(" "),a("pre",{staticClass:"title"},[a("h2",{attrs:{id:"wordfeatures"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#wordfeatures"}},[e._v("#")]),e._v(" WordFeatures "),a("Badge",{attrs:{text:"Class"}})],1),e._v("\n")]),e._v(" "),a("pre",{staticClass:"language-python"},[a("code",[e._v("\n"),a("span",{staticClass:"token keyword"},[e._v("class")]),e._v(" "),a("span",{staticClass:"ident"},[e._v("WordFeatures")]),e._v(" ("),e._v("\n    "),a("span",[e._v("embedding_dim: int")]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("lowercase_tokens: bool = False")]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("trainable: bool = True")]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("weights_file: Union[str, NoneType] = None")]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("**extra_params")]),a("span",[e._v(",")]),e._v("\n"),a("span",[e._v(")")]),e._v("\n")]),e._v("\n")]),e._v(" "),a("p",[e._v("Feature configuration at word level")]),e._v(" "),a("h2",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),a("dl",[a("dt",[a("strong",[a("code",[e._v("embedding_dim")])])]),e._v(" "),a("dd",[e._v("Dimension of the embeddings")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("lowercase_tokens")])])]),e._v(" "),a("dd",[e._v("If True, lowercase tokens before the indexing")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("trainable")])])]),e._v(" "),a("dd",[e._v("If False, freeze the embeddings")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("weights_file")])])]),e._v(" "),a("dd",[e._v("Path to a file with pretrained weights for the embedding")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("**extra_params")])])]),e._v(" "),a("dd",[e._v("Extra parameters passed on to the "),a("code",[e._v("indexer")]),e._v(" and "),a("code",[e._v("embedder")]),e._v(" of the AllenNLP configuration framework.\nFor example: "),a("code",[e._v('WordFeatures(embedding_dim=300, embedder={"padding_index": 0})')])])]),e._v(" "),a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"instance-variables"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#instance-variables"}},[e._v("#")]),e._v(" Instance variables")]),e._v("\n")]),e._v(" "),a("dl",[a("dt",{attrs:{id:"biome.text.features.WordFeatures.config"}},[a("code",{staticClass:"name"},[e._v("var "),a("span",{staticClass:"ident"},[e._v("config")]),e._v(" : Dict")])]),e._v(" "),a("dd",[a("p",[e._v("Returns the config in AllenNLP format")])])]),e._v(" "),a("dl",[a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"to-json"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#to-json"}},[e._v("#")]),e._v(" to_json "),a("Badge",{attrs:{text:"Method"}})],1),e._v("\n")]),e._v(" "),a("dt",[a("div",{staticClass:"language-python extra-class"},[a("pre",{staticClass:"language-python"},[a("code",[e._v("\n"),a("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),a("span",{staticClass:"ident"},[e._v("to_json")]),e._v("("),a("span",[e._v("self) -> Dict")]),e._v("\n")]),e._v("\n")])])]),e._v(" "),a("dd",[a("p",[e._v("Returns the config as dict for the serialized json config file")])])]),e._v(" "),a("div"),e._v(" "),a("pre",{staticClass:"title"},[a("h2",{attrs:{id:"charfeatures"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#charfeatures"}},[e._v("#")]),e._v(" CharFeatures "),a("Badge",{attrs:{text:"Class"}})],1),e._v("\n")]),e._v(" "),a("pre",{staticClass:"language-python"},[a("code",[e._v("\n"),a("span",{staticClass:"token keyword"},[e._v("class")]),e._v(" "),a("span",{staticClass:"ident"},[e._v("CharFeatures")]),e._v(" ("),e._v("\n    "),a("span",[e._v("embedding_dim: int")]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("encoder: Dict[str, Any]")]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("dropout: float = 0.0")]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("lowercase_characters: bool = False")]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("**extra_params")]),a("span",[e._v(",")]),e._v("\n"),a("span",[e._v(")")]),e._v("\n")]),e._v("\n")]),e._v(" "),a("p",[e._v("Feature configuration at character level")]),e._v(" "),a("h2",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),a("dl",[a("dt",[a("strong",[a("code",[e._v("embedding_dim")])])]),e._v(" "),a("dd",[e._v("Dimension of the character embeddings.")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("encoder")])])]),e._v(" "),a("dd",[e._v("A sequence to vector encoder resulting in a word representation based on its characters")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("dropout")])])]),e._v(" "),a("dd",[e._v("Dropout applied to the output of the encoder")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("lowercase_characters")])])]),e._v(" "),a("dd",[e._v("If True, lowercase characters before the indexing")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("**extra_params")])])]),e._v(" "),a("dd",[e._v("Extra parameters passed on to the "),a("code",[e._v("indexer")]),e._v(" and "),a("code",[e._v("embedder")]),e._v(" of the AllenNLP configuration framework.\nFor example: "),a("code",[e._v('CharFeatures(embedding_dim=32, indexer={"min_padding_length": 5}, ...)')])])]),e._v(" "),a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"instance-variables-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#instance-variables-2"}},[e._v("#")]),e._v(" Instance variables")]),e._v("\n")]),e._v(" "),a("dl",[a("dt",{attrs:{id:"biome.text.features.CharFeatures.config"}},[a("code",{staticClass:"name"},[e._v("var "),a("span",{staticClass:"ident"},[e._v("config")]),e._v(" : Dict")])]),e._v(" "),a("dd",[a("p",[e._v("Returns the config in AllenNLP format")])])]),e._v(" "),a("dl",[a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"to-json-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#to-json-2"}},[e._v("#")]),e._v(" to_json "),a("Badge",{attrs:{text:"Method"}})],1),e._v("\n")]),e._v(" "),a("dt",[a("div",{staticClass:"language-python extra-class"},[a("pre",{staticClass:"language-python"},[a("code",[e._v("\n"),a("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),a("span",{staticClass:"ident"},[e._v("to_json")]),e._v("("),a("span",[e._v("self)")]),e._v("\n")]),e._v("\n")])])]),e._v(" "),a("dd",[a("p",[e._v("Returns the config as dict for the serialized json config file")])])]),e._v(" "),a("div"),e._v(" "),a("pre",{staticClass:"title"},[a("h2",{attrs:{id:"transformersfeatures"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#transformersfeatures"}},[e._v("#")]),e._v(" TransformersFeatures "),a("Badge",{attrs:{text:"Class"}})],1),e._v("\n")]),e._v(" "),a("pre",{staticClass:"language-python"},[a("code",[e._v("\n"),a("span",{staticClass:"token keyword"},[e._v("class")]),e._v(" "),a("span",{staticClass:"ident"},[e._v("TransformersFeatures")]),e._v(" ("),e._v("\n    "),a("span",[e._v("model_name: str")]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("trainable: bool = False")]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("max_length: Union[int, NoneType] = None")]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("last_layer_only: bool = True")]),a("span",[e._v(",")]),e._v("\n"),a("span",[e._v(")")]),e._v("\n")]),e._v("\n")]),e._v(" "),a("p",[e._v("Configuration of the feature extracted with the "),a("a",{attrs:{href:"https://huggingface.co/models"}},[e._v("transformers models")]),e._v(".")]),e._v(" "),a("p",[e._v('We use AllenNLPs "mismatched" indexer and embedder to get word-level representations.\nMost of the transformers models work with word-piece tokenizers.')]),e._v(" "),a("h2",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),a("dl",[a("dt",[a("strong",[a("code",[e._v("model_name")])])]),e._v(" "),a("dd",[e._v("Name of one of the "),a("a",{attrs:{href:"https://huggingface.co/models"}},[e._v("transformers models")]),e._v(".")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("trainable")])])]),e._v(" "),a("dd",[e._v("If false, freeze the transformer weights")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("max_length")])])]),e._v(" "),a("dd",[e._v("If positive, split the document into segments of this many tokens (including special tokens)\nbefore feeding into the embedder. The embedder embeds these segments independently and\nconcatenate the results to get the original document representation.")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("last_layer_only")])])]),e._v(" "),a("dd",[e._v("When "),a("code",[e._v("True")]),e._v(", only the final layer of the pretrained transformer is taken\nfor the embeddings. But if set to "),a("code",[e._v("False")]),e._v(", a scalar mix of all of the layers\nis used.")])]),e._v(" "),a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"instance-variables-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#instance-variables-3"}},[e._v("#")]),e._v(" Instance variables")]),e._v("\n")]),e._v(" "),a("dl",[a("dt",{attrs:{id:"biome.text.features.TransformersFeatures.config"}},[a("code",{staticClass:"name"},[e._v("var "),a("span",{staticClass:"ident"},[e._v("config")]),e._v(" : Dict")])]),e._v(" "),a("dd",[a("p",[e._v("Returns the config in AllenNLP format")])])]),e._v(" "),a("dl",[a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"to-json-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#to-json-3"}},[e._v("#")]),e._v(" to_json "),a("Badge",{attrs:{text:"Method"}})],1),e._v("\n")]),e._v(" "),a("dt",[a("div",{staticClass:"language-python extra-class"},[a("pre",{staticClass:"language-python"},[a("code",[e._v("\n"),a("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),a("span",{staticClass:"ident"},[e._v("to_json")]),e._v("("),a("span",[e._v("self) -> Dict")]),e._v("\n")]),e._v("\n")])])]),e._v(" "),a("dd",[a("p",[e._v("Returns the config as dict for the serialized json config file")])])])])}),[],!1,null,null,null);t.default=n.exports}}]);