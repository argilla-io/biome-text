(window.webpackJsonp=window.webpackJsonp||[]).push([[46],{379:function(e,t,n){"use strict";n.r(t);var s=n(26),o=Object(s.a)({},(function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[n("h1",{attrs:{id:"biome-text-tokenizer"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#biome-text-tokenizer"}},[e._v("#")]),e._v(" biome.text.tokenizer "),n("Badge",{attrs:{text:"Module"}})],1),e._v(" "),n("dl",[n("h2",{attrs:{id:"biome.text.tokenizer.Tokenizer"}},[e._v("Tokenizer "),n("Badge",{attrs:{text:"Class"}})],1),e._v(" "),n("dt",[n("div",{staticClass:"language-python extra-class"},[n("pre",{staticClass:"language-python"},[e._v("    "),n("code",[e._v("\n"),n("span",{staticClass:"token keyword"},[e._v("class")]),e._v(" "),n("span",{staticClass:"ident"},[e._v("Tokenizer")]),e._v(" ("),e._v("\n    "),n("span",[e._v("lang: str = 'en'")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("skip_empty_tokens: bool = False")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("max_sequence_length: int = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("max_nr_of_sentences: int = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("text_cleaning: Union[biome.text.text_cleaning.TextCleaning, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("segment_sentences: Union[bool, allennlp.data.tokenizers.sentence_splitter.SentenceSplitter] = False")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("start_tokens: Union[List[str], NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("end_tokens: Union[List[str], NoneType] = None")]),n("span",[e._v(",")]),e._v("\n"),n("span",[e._v(")")]),e._v("\n    ")])])])]),e._v(" "),n("dd",[n("div",{staticClass:"desc"},[n("p",[e._v("Pre-processes and tokenizes input text")]),e._v(" "),n("p",[e._v("Transforms inputs (e.g., a text, a list of texts, etc.) into structures containing "),n("code",[e._v("allennlp.data.Token")]),e._v(" objects.")]),e._v(" "),n("p",[e._v("Use its arguments to configure the first stage of the pipeline (i.e., pre-processing a given set of text inputs.)")]),e._v(" "),n("p",[e._v("Use methods for tokenizing depending on the shape of inputs (e.g., records with multiple fields, sentences lists).")]),e._v(" "),n("h1",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),n("p",[e._v("lang: "),n("code",[e._v("str")]),e._v("\nThe "),n("code",[e._v("spaCy")]),e._v(" language to be used by the tokenizer (default is "),n("code",[e._v("en")]),e._v(")\nskip_empty_tokens: "),n("code",[e._v("bool")]),e._v("\nmax_sequence_length: "),n("code",[e._v("int")]),e._v("\nMaximum length in characters for input texts truncated with "),n("code",[e._v("[:max_sequence_length]")]),e._v(" after "),n("code",[e._v("TextCleaning")]),e._v(".\nmax_nr_of_sentences: "),n("code",[e._v("int")]),e._v("\nMaximum number of sentences to keep when using "),n("code",[e._v("segment_sentences")]),e._v(" truncated with "),n("code",[e._v("[:max_sequence_length]")]),e._v(".\ntext_cleaning: "),n("code",[e._v("Optional[TextCleaning]")]),e._v("\nA "),n("code",[e._v("TextCleaning")]),e._v(" configuration with pre-processing rules for cleaning up and transforming raw input text.\nsegment_sentences:\n"),n("code",[e._v("Union[bool, SentenceSplitter]")]),e._v("\nWhether to segment input texts in to sentences using the default "),n("code",[e._v("SentenceSplitter")]),e._v(" or a given splitter.\nstart_tokens: "),n("code",[e._v("Optional[List[str]]")]),e._v("\nA list of token strings to the sequence before tokenized input text.\nend_tokens: "),n("code",[e._v("Optional[List[str]]")]),e._v("\nA list of token strings to the sequence after tokenized input text.")])]),e._v(" "),n("h3",[e._v("Ancestors")]),e._v(" "),n("ul",{staticClass:"hlist"},[n("li",[e._v("allennlp.common.from_params.FromParams")])]),e._v(" "),n("dl",[n("h3",{attrs:{id:"biome.text.tokenizer.Tokenizer.tokenize_text"}},[e._v("tokenize_text "),n("Badge",{attrs:{text:"Method"}})],1),e._v(" "),n("dt",[n("div",{staticClass:"language-python extra-class"},[n("pre",{staticClass:"language-python"},[n("code",[e._v("\n"),n("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),n("span",{staticClass:"ident"},[e._v("tokenize_text")]),e._v(" ("),e._v("\n   self,\n   text: str,\n)  -> List[allennlp.data.tokenizers.token.Token]\n")]),e._v("\n        ")])])]),e._v(" "),n("dd",[n("div",{staticClass:"desc"},[n("p",[e._v("Tokenizes a text string")]),e._v(" "),n("p",[e._v("Use this for the simplest case where your input is just a "),n("code",[e._v("str")])]),e._v(" "),n("h1",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),n("pre",[n("code",[e._v("text: <code>str</code>\n")])]),e._v(" "),n("h1",{attrs:{id:"returns"}},[e._v("Returns")]),e._v(" "),n("pre",[n("code",[e._v("tokens: <code>List\\[Token]</code>\n")])])])]),e._v(" "),n("h3",{attrs:{id:"biome.text.tokenizer.Tokenizer.tokenize_document"}},[e._v("tokenize_document "),n("Badge",{attrs:{text:"Method"}})],1),e._v(" "),n("dt",[n("div",{staticClass:"language-python extra-class"},[n("pre",{staticClass:"language-python"},[n("code",[e._v("\n"),n("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),n("span",{staticClass:"ident"},[e._v("tokenize_document")]),e._v(" ("),e._v("\n   self,\n   document: List[str],\n)  -> List[List[allennlp.data.tokenizers.token.Token]]\n")]),e._v("\n        ")])])]),e._v(" "),n("dd",[n("div",{staticClass:"desc"},[n("p",[e._v("Tokenizes a document-like structure containing lists of text inputs")]),e._v(" "),n("p",[e._v("Use this to account for hierarchical text structures (e.g., a paragraph is a list of sentences)")]),e._v(" "),n("h1",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),n("pre",[n("code",[e._v("document: <code>List\\[str]</code>\nA <code>List</code> with text inputs, e.g., sentences\n")])]),e._v(" "),n("h1",{attrs:{id:"returns"}},[e._v("Returns")]),e._v(" "),n("pre",[n("code",[e._v("tokens: <code>List\\[List\\[Token]]</code>\n")])])])]),e._v(" "),n("h3",{attrs:{id:"biome.text.tokenizer.Tokenizer.tokenize_record"}},[e._v("tokenize_record "),n("Badge",{attrs:{text:"Method"}})],1),e._v(" "),n("dt",[n("div",{staticClass:"language-python extra-class"},[n("pre",{staticClass:"language-python"},[n("code",[e._v("\n"),n("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),n("span",{staticClass:"ident"},[e._v("tokenize_record")]),e._v(" ("),e._v("\n   self,\n   record: Dict[str, Any],\n)  -> Dict[str, Tuple[List[allennlp.data.tokenizers.token.Token], List[allennlp.data.tokenizers.token.Token]]]\n")]),e._v("\n        ")])])]),e._v(" "),n("dd",[n("div",{staticClass:"desc"},[n("p",[e._v("Tokenizes a record-like structure containing text inputs")]),e._v(" "),n("p",[e._v("Use this to keep information about the record-like data structure as input features to the model.")]),e._v(" "),n("h1",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),n("pre",[n("code",[e._v('record: <code>Dict\\[str, Any]</code>\nA <code>Dict</code> with arbitrary "fields" containing text.\n')])]),e._v(" "),n("h1",{attrs:{id:"returns"}},[e._v("Returns")]),e._v(" "),n("pre",[n("code",[e._v("tokens: <code>Dict\\[str, Tuple\\[List\\[Token], List\\[Token]]]</code>\n    A dictionary with two lists of <code>Token</code>'s for each record entry: <code>key</code> and <code>value</code> tokens.\n")])])])])])])])])}),[],!1,null,null,null);t.default=o.exports}}]);