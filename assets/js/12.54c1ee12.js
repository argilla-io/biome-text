(window.webpackJsonp=window.webpackJsonp||[]).push([[12],{424:function(e,t,a){"use strict";a.r(t);var s=a(26),n=Object(s.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h1",{attrs:{id:"biome-text-backbone"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#biome-text-backbone"}},[e._v("#")]),e._v(" biome.text.backbone "),a("Badge",{attrs:{text:"Module"}})],1),e._v(" "),a("div"),e._v(" "),a("div"),e._v(" "),a("pre",{staticClass:"title"},[a("h2",{attrs:{id:"modelbackbone"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#modelbackbone"}},[e._v("#")]),e._v(" ModelBackbone "),a("Badge",{attrs:{text:"Class"}})],1),e._v("\n")]),e._v(" "),a("pre",{staticClass:"language-python"},[a("code",[e._v("\n"),a("span",{staticClass:"token keyword"},[e._v("class")]),e._v(" "),a("span",{staticClass:"ident"},[e._v("ModelBackbone")]),e._v(" ("),e._v("\n    "),a("span",[e._v("vocab: allennlp.data.vocabulary.Vocabulary")]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("featurizer: "),a("a",{attrs:{title:"biome.text.featurizer.InputFeaturizer",href:"featurizer.html#biome.text.featurizer.InputFeaturizer"}},[e._v("InputFeaturizer")])]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("embedder: allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder")]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("encoder: Union[biome.text.modules.configuration.allennlp_configuration.Seq2SeqEncoderConfiguration, NoneType] = None")]),a("span",[e._v(",")]),e._v("\n"),a("span",[e._v(")")]),e._v("\n")]),e._v("\n")]),e._v(" "),a("p",[e._v("The backbone of the model.")]),e._v(" "),a("p",[e._v("It is composed of a tokenizer, featurizer and an encoder.\nThis component of the model can be pretrained and used with different task heads.")]),e._v(" "),a("h2",{attrs:{id:"attributes"}},[e._v("Attributes")]),e._v(" "),a("dl",[a("dt",[a("strong",[a("code",[e._v("vocab")])]),e._v(" : "),a("code",[e._v("Vocabulary")])]),e._v(" "),a("dd",[e._v("The vocabulary of the pipeline")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("featurizer")])]),e._v(" : "),a("code",[e._v("InputFeaturizer")])]),e._v(" "),a("dd",[e._v("Defines the input features of the tokens and indexes")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("embedder")])]),e._v(" : "),a("code",[e._v("TextFieldEmbedder")])]),e._v(" "),a("dd",[e._v("The embedding layer")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("encoder")])]),e._v(" : "),a("code",[e._v("Encoder")])]),e._v(" "),a("dd",[e._v("Outputs an encoded sequence of the tokens")])]),e._v(" "),a("p",[e._v("Initializes internal Module state, shared by both nn.Module and ScriptModule.")]),e._v(" "),a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"ancestors"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#ancestors"}},[e._v("#")]),e._v(" Ancestors")]),e._v("\n")]),e._v(" "),a("ul",{staticClass:"hlist"},[a("li",[e._v("torch.nn.modules.module.Module")])]),e._v(" "),a("dl",[a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"forward"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#forward"}},[e._v("#")]),e._v(" forward "),a("Badge",{attrs:{text:"Method"}})],1),e._v("\n")]),e._v(" "),a("dt",[a("div",{staticClass:"language-python extra-class"},[a("pre",{staticClass:"language-python"},[a("code",[e._v("\n"),a("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),a("span",{staticClass:"ident"},[e._v("forward")]),e._v(" ("),e._v("\n  self,\n  text: Dict[str, Dict[str, torch.Tensor]],\n  mask: torch.Tensor,\n  num_wrapping_dims: int = 0,\n)  -> torch.Tensor\n")]),e._v("\n")])])]),e._v(" "),a("dd",[a("p",[e._v("Applies the embedding and encoding layer")]),e._v(" "),a("h2",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),a("dl",[a("dt",[a("strong",[a("code",[e._v("text")])])]),e._v(" "),a("dd",[e._v("Output of the "),a("code",[e._v("batch.as_tensor_dict()")]),e._v(" method, basically the indices of the indexed tokens")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("mask")])])]),e._v(" "),a("dd",[e._v("A mask indicating which one of the tokens are padding tokens")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("num_wrapping_dims")])])]),e._v(" "),a("dd",[e._v("0 if "),a("code",[e._v("text")]),e._v(" is the output of a "),a("code",[e._v("TextField")]),e._v(", 1 if it is the output of a "),a("code",[e._v("ListField")])])]),e._v(" "),a("h2",{attrs:{id:"returns"}},[e._v("Returns")]),e._v(" "),a("dl",[a("dt",[a("code",[e._v("tensor")])]),e._v(" "),a("dd",[e._v("Encoded representation of the input")])])]),e._v(" "),a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"on-vocab-update"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#on-vocab-update"}},[e._v("#")]),e._v(" on_vocab_update "),a("Badge",{attrs:{text:"Method"}})],1),e._v("\n")]),e._v(" "),a("dt",[a("div",{staticClass:"language-python extra-class"},[a("pre",{staticClass:"language-python"},[a("code",[e._v("\n"),a("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),a("span",{staticClass:"ident"},[e._v("on_vocab_update")]),e._v("("),a("span",[e._v("self)")]),e._v("\n")]),e._v("\n")])])]),e._v(" "),a("dd",[a("p",[e._v("This method is called when a base model updates the vocabulary")])]),e._v(" "),a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"featurize"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#featurize"}},[e._v("#")]),e._v(" featurize "),a("Badge",{attrs:{text:"Method"}})],1),e._v("\n")]),e._v(" "),a("dt",[a("div",{staticClass:"language-python extra-class"},[a("pre",{staticClass:"language-python"},[a("code",[e._v("\n"),a("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),a("span",{staticClass:"ident"},[e._v("featurize")]),e._v(" ("),e._v("\n  self,\n  record: Union[str, List[str], Dict[str, Any]],\n  to_field: str = 'record',\n  aggregate: bool = False,\n  tokenize: bool = True,\n)  -> allennlp.data.instance.Instance\n")]),e._v("\n")])])]),e._v(" "),a("dd",[a("p",[e._v("Generates an allennlp instance from a record input.\nDEPRECATED: use self.featurizer instead")]),e._v(" "),a("h2",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),a("dl",[a("dt",[a("strong",[a("code",[e._v("record")])]),e._v(" : "),a("code",[e._v("Union[str, List[str], Dict[str, Any]]")])]),e._v(" "),a("dd",[e._v("Input data")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("to_field")])]),e._v(" : "),a("code",[e._v("str")])]),e._v(" "),a("dd",[e._v("The field name in the returned instance")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("aggregate")])]),e._v(" : "),a("code",[e._v("bool")])]),e._v(" "),a("dd",[e._v("If true, the returned instance will contain a single "),a("code",[e._v("TextField")]),e._v(" with all record fields;\nIf false, the instance will contain a "),a("code",[e._v("ListField")]),e._v(" of "),a("code",[e._v("TextField")]),e._v("s.")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("tokenize")])]),e._v(" : "),a("code",[e._v("bool")])]),e._v(" "),a("dd",[e._v("If false, skip tokenization phase and pass record data as tokenized token list.")])]),e._v(" "),a("h2",{attrs:{id:"returns"}},[e._v("Returns")]),e._v(" "),a("dl",[a("dt",[a("strong",[a("code",[e._v("instance")])]),e._v(" : "),a("code",[e._v("Instance")])]),e._v(" "),a("dd",[e._v(" ")])])])])])}),[],!1,null,null,null);t.default=n.exports}}]);