(window.webpackJsonp=window.webpackJsonp||[]).push([[18],{417:function(t,e,a){"use strict";a.r(e);var n=a(26),s=Object(n.a)({},(function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"biome-text-configuration"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#biome-text-configuration"}},[t._v("#")]),t._v(" biome.text.configuration "),a("Badge",{attrs:{text:"Module"}})],1),t._v(" "),a("div"),t._v(" "),a("div"),t._v(" "),a("pre",{staticClass:"title"},[a("h2",{attrs:{id:"featuresconfiguration"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#featuresconfiguration"}},[t._v("#")]),t._v(" FeaturesConfiguration "),a("Badge",{attrs:{text:"Class"}})],1),t._v("\n")]),t._v(" "),a("pre",{staticClass:"language-python"},[a("code",[t._v("\n"),a("span",{staticClass:"token keyword"},[t._v("class")]),t._v(" "),a("span",{staticClass:"ident"},[t._v("FeaturesConfiguration")]),t._v(" ("),t._v("\n    "),a("span",[t._v("word: Union[biome.text.features.WordFeatures, NoneType] = None")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("char: Union[biome.text.features.CharFeatures, NoneType] = None")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("**extra_params")]),a("span",[t._v(",")]),t._v("\n"),a("span",[t._v(")")]),t._v("\n")]),t._v("\n")]),t._v(" "),a("p",[t._v("Creates a input featurizer configuration")]),t._v(" "),a("p",[t._v("This class will create a configuration for the features of the "),a("code",[t._v("Pipeline")]),t._v(".")]),t._v(" "),a("p",[t._v("Use this for defining the main features to be used by the model, namely word and character embeddings.")]),t._v(" "),a("p",[t._v(":::tip\nIf you do not pass "),a("code",[t._v("words")]),t._v(" and "),a("code",[t._v("chars")]),t._v(" your pipeline will be setup with default word features (embedding_dim=50).\n:::")]),t._v(" "),a("p",[t._v("Example:")]),t._v(" "),a("pre",[a("code",{staticClass:"python"},[t._v("word = WordFeatures(embedding_dim=100)\nchar = CharFeatures(embedding_dim=16, encoder={'type': 'gru'})\nconfig = FeaturesConfiguration(word, char)\n")])]),t._v(" "),a("h2",{attrs:{id:"parameters"}},[t._v("Parameters")]),t._v(" "),a("dl",[a("dt",[a("strong",[a("code",[t._v("word")])]),t._v(" : "),a("code",[a("a",{attrs:{title:"biome.text.features.WordFeatures",href:"features.html#biome.text.features.WordFeatures"}},[t._v("WordFeatures")])])]),t._v(" "),a("dd",[t._v(" ")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("char")])]),t._v(" : "),a("code",[a("a",{attrs:{title:"biome.text.features.CharFeatures",href:"features.html#biome.text.features.CharFeatures"}},[t._v("CharFeatures")])])]),t._v(" "),a("dd",[t._v(" ")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("extra_params")])])]),t._v(" "),a("dd",[t._v(" ")])]),t._v(" "),a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"ancestors"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#ancestors"}},[t._v("#")]),t._v(" Ancestors")]),t._v("\n")]),t._v(" "),a("ul",{staticClass:"hlist"},[a("li",[t._v("allennlp.common.from_params.FromParams")])]),t._v(" "),a("dl",[a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"from-params"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#from-params"}},[t._v("#")]),t._v(" from_params "),a("Badge",{attrs:{text:"Static method"}})],1),t._v("\n")]),t._v(" "),a("dt",[a("div",{staticClass:"language-python extra-class"},[a("pre",{staticClass:"language-python"},[a("code",[t._v("\n"),a("span",{staticClass:"token keyword"},[t._v("def")]),t._v(" "),a("span",{staticClass:"ident"},[t._v("from_params")]),t._v(" ("),t._v("\n  params: allennlp.common.params.Params,\n  **extras,\n)  -> "),a("a",{attrs:{title:"biome.text.configuration.FeaturesConfiguration",href:"#biome.text.configuration.FeaturesConfiguration"}},[t._v("FeaturesConfiguration")]),t._v("\n")]),t._v("\n")])])]),t._v(" "),a("dd",[a("p",[t._v("This is the automatic implementation of "),a("code",[t._v("from_params")]),t._v(". Any class that subclasses\n"),a("code",[t._v("FromParams")]),t._v(" (or "),a("code",[t._v("Registrable")]),t._v(", which itself subclasses "),a("code",[t._v("FromParams")]),t._v(') gets this\nimplementation for free.\nIf you want your class to be instantiated from params in the\n"obvious" way – pop off parameters and hand them to your constructor with the same names –\nthis provides that functionality.')]),t._v(" "),a("p",[t._v("If you need more complex logic in your from "),a("code",[t._v("from_params")]),t._v(" method, you'll have to implement\nyour own method that overrides this one.")]),t._v(" "),a("p",[t._v("The "),a("code",[t._v("constructor_to_call")]),t._v(" and "),a("code",[t._v("constructor_to_inspect")]),t._v(" arguments deal with a bit of\nredirection that we do.\nWe allow you to register particular "),a("code",[t._v("@classmethods")]),t._v(" on a class as\nthe constructor to use for a registered name.\nThis lets you, e.g., have a single\n"),a("code",[t._v("Vocabulary")]),t._v(" class that can be constructed in two different ways, with different names\nregistered to each constructor.\nIn order to handle this, we need to know not just the class\nwe're trying to construct ("),a("code",[t._v("cls")]),t._v("), but also what method we should inspect to find its\narguments ("),a("code",[t._v("constructor_to_inspect")]),t._v("), and what method to call when we're done constructing\narguments ("),a("code",[t._v("constructor_to_call")]),t._v(").\nThese two methods are the same when you've used a\n"),a("code",[t._v("@classmethod")]),t._v(" as your constructor, but they are "),a("code",[t._v("different")]),t._v(" when you use the default\nconstructor (because you inspect "),a("code",[t._v("__init__")]),t._v(", but call "),a("code",[t._v("cls()")]),t._v(").")])])]),t._v(" "),a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"instance-variables"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#instance-variables"}},[t._v("#")]),t._v(" Instance variables")]),t._v("\n")]),t._v(" "),a("dl",[a("dt",{attrs:{id:"biome.text.configuration.FeaturesConfiguration.keys"}},[a("code",{staticClass:"name"},[t._v("var "),a("span",{staticClass:"ident"},[t._v("keys")]),t._v(" : List[str]")])]),t._v(" "),a("dd",[a("p",[t._v("Gets the key features")])])]),t._v(" "),a("dl",[a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"compile-embedder"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#compile-embedder"}},[t._v("#")]),t._v(" compile_embedder "),a("Badge",{attrs:{text:"Method"}})],1),t._v("\n")]),t._v(" "),a("dt",[a("div",{staticClass:"language-python extra-class"},[a("pre",{staticClass:"language-python"},[a("code",[t._v("\n"),a("span",{staticClass:"token keyword"},[t._v("def")]),t._v(" "),a("span",{staticClass:"ident"},[t._v("compile_embedder")]),t._v(" ("),t._v("\n  self,\n  vocab: allennlp.data.vocabulary.Vocabulary,\n)  -> allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder\n")]),t._v("\n")])])]),t._v(" "),a("dd",[a("p",[t._v("Creates the embedder from configured features for a given vocabulary")])]),t._v(" "),a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"compile-featurizer"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#compile-featurizer"}},[t._v("#")]),t._v(" compile_featurizer "),a("Badge",{attrs:{text:"Method"}})],1),t._v("\n")]),t._v(" "),a("dt",[a("div",{staticClass:"language-python extra-class"},[a("pre",{staticClass:"language-python"},[a("code",[t._v("\n"),a("span",{staticClass:"token keyword"},[t._v("def")]),t._v(" "),a("span",{staticClass:"ident"},[t._v("compile_featurizer")]),t._v(" ("),t._v("\n  self,\n  tokenizer: "),a("a",{attrs:{title:"biome.text.tokenizer.Tokenizer",href:"tokenizer.html#biome.text.tokenizer.Tokenizer"}},[t._v("Tokenizer")]),t._v(",\n)  -> "),a("a",{attrs:{title:"biome.text.featurizer.InputFeaturizer",href:"featurizer.html#biome.text.featurizer.InputFeaturizer"}},[t._v("InputFeaturizer")]),t._v("\n")]),t._v("\n")])])]),t._v(" "),a("dd",[a("p",[t._v("Creates a featurizer from the configuration object")]),t._v(" "),a("p",[t._v(":::tip")]),t._v(" "),a("p",[t._v("If you are creating configurations programmatically use this method to check that your config object contains\na valid configuration.")]),t._v(" "),a("p",[t._v(":::")]),t._v(" "),a("h2",{attrs:{id:"parameters"}},[t._v("Parameters")]),t._v(" "),a("dl",[a("dt",[a("strong",[a("code",[t._v("tokenizer")])]),t._v(" : "),a("code",[t._v("Tokenizer")])]),t._v(" "),a("dd",[t._v("tokenizer used for this featurizer")])]),t._v(" "),a("h2",{attrs:{id:"returns"}},[t._v("Returns")]),t._v(" "),a("p",[t._v("The configured "),a("code",[t._v("InputFeaturizer")])])])]),t._v(" "),a("div"),t._v(" "),a("pre",{staticClass:"title"},[a("h2",{attrs:{id:"tokenizerconfiguration"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#tokenizerconfiguration"}},[t._v("#")]),t._v(" TokenizerConfiguration "),a("Badge",{attrs:{text:"Class"}})],1),t._v("\n")]),t._v(" "),a("pre",{staticClass:"language-python"},[a("code",[t._v("\n"),a("span",{staticClass:"token keyword"},[t._v("class")]),t._v(" "),a("span",{staticClass:"ident"},[t._v("TokenizerConfiguration")]),t._v(" ("),t._v("\n    "),a("span",[t._v("lang: str = 'en'")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("skip_empty_tokens: bool = False")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("max_sequence_length: int = None")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("max_nr_of_sentences: int = None")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("text_cleaning: Union[Dict[str, Any], NoneType] = None")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("segment_sentences: Union[bool, Dict[str, Any]] = False")]),a("span",[t._v(",")]),t._v("\n"),a("span",[t._v(")")]),t._v("\n")]),t._v("\n")]),t._v(" "),a("p",[t._v("Creates a "),a("code",[t._v("Tokenizer")]),t._v(" configuration")]),t._v(" "),a("h2",{attrs:{id:"parameters"}},[t._v("Parameters")]),t._v(" "),a("dl",[a("dt",[a("strong",[a("code",[t._v("lang")])])]),t._v(" "),a("dd",[t._v(" ")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("skip_empty_tokens")])])]),t._v(" "),a("dd",[t._v(" ")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("max_sequence_length")])])]),t._v(" "),a("dd",[t._v(" ")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("max_nr_of_sentences")])])]),t._v(" "),a("dd",[t._v(" ")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("text_cleaning")])])]),t._v(" "),a("dd",[t._v(" ")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("segment_sentences")])])]),t._v(" "),a("dd",[t._v(" ")])]),t._v(" "),a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"ancestors-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#ancestors-2"}},[t._v("#")]),t._v(" Ancestors")]),t._v("\n")]),t._v(" "),a("ul",{staticClass:"hlist"},[a("li",[t._v("allennlp.common.from_params.FromParams")])]),t._v(" "),a("dl",[a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"compile"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#compile"}},[t._v("#")]),t._v(" compile "),a("Badge",{attrs:{text:"Method"}})],1),t._v("\n")]),t._v(" "),a("dt",[a("div",{staticClass:"language-python extra-class"},[a("pre",{staticClass:"language-python"},[a("code",[t._v("\n"),a("span",{staticClass:"token keyword"},[t._v("def")]),t._v(" "),a("span",{staticClass:"ident"},[t._v("compile")]),t._v("("),a("span",[t._v("self) -> "),a("a",{attrs:{title:"biome.text.tokenizer.Tokenizer",href:"tokenizer.html#biome.text.tokenizer.Tokenizer"}},[t._v("Tokenizer")])]),t._v("\n")]),t._v("\n")])])]),t._v(" "),a("dd",[a("p",[t._v("Build tokenizer object from its configuration")])])]),t._v(" "),a("div"),t._v(" "),a("pre",{staticClass:"title"},[a("h2",{attrs:{id:"pipelineconfiguration"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#pipelineconfiguration"}},[t._v("#")]),t._v(" PipelineConfiguration "),a("Badge",{attrs:{text:"Class"}})],1),t._v("\n")]),t._v(" "),a("pre",{staticClass:"language-python"},[a("code",[t._v("\n"),a("span",{staticClass:"token keyword"},[t._v("class")]),t._v(" "),a("span",{staticClass:"ident"},[t._v("PipelineConfiguration")]),t._v(" ("),t._v("\n    "),a("span",[t._v("name: str")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("features: "),a("a",{attrs:{title:"biome.text.configuration.FeaturesConfiguration",href:"#biome.text.configuration.FeaturesConfiguration"}},[t._v("FeaturesConfiguration")])]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("head: "),a("a",{attrs:{title:"biome.text.modules.heads.task_head.TaskHeadSpec",href:"modules/heads/task_head.html#biome.text.modules.heads.task_head.TaskHeadSpec"}},[t._v("TaskHeadSpec")])]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("tokenizer: Union[biome.text.configuration.TokenizerConfiguration, NoneType] = None")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("encoder: Union[biome.text.modules.specs.allennlp_specs.Seq2SeqEncoderSpec, NoneType] = None")]),a("span",[t._v(",")]),t._v("\n"),a("span",[t._v(")")]),t._v("\n")]),t._v("\n")]),t._v(" "),a("p",[t._v('"Creates a '),a("code",[t._v("Pipeline")]),t._v(" configuration")]),t._v(" "),a("h2",{attrs:{id:"parameters"}},[t._v("Parameters")]),t._v(" "),a("dl",[a("dt",[a("strong",[a("code",[t._v("name")])]),t._v(" : "),a("code",[t._v("str")])]),t._v(" "),a("dd",[t._v("The "),a("code",[t._v("name")]),t._v(" for our pipeline")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("features")])]),t._v(" : "),a("code",[a("a",{attrs:{title:"biome.text.configuration.FeaturesConfiguration",href:"#biome.text.configuration.FeaturesConfiguration"}},[t._v("FeaturesConfiguration")])])]),t._v(" "),a("dd",[t._v("The input "),a("code",[t._v("features")]),t._v(" to be used by the model pipeline. We define this using a "),a("code",[a("a",{attrs:{title:"biome.text.configuration.FeaturesConfiguration",href:"#biome.text.configuration.FeaturesConfiguration"}},[t._v("FeaturesConfiguration")])]),t._v(" object.")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("head")])]),t._v(" : "),a("code",[t._v("TaskHeadSpec")])]),t._v(" "),a("dd",[t._v("The "),a("code",[t._v("head")]),t._v(" for the task, e.g., a LanguageModelling task, using a "),a("code",[t._v("TaskHeadSpec")]),t._v(" object.")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("tokenizer")])]),t._v(" : "),a("code",[a("a",{attrs:{title:"biome.text.configuration.TokenizerConfiguration",href:"#biome.text.configuration.TokenizerConfiguration"}},[t._v("TokenizerConfiguration")])]),t._v(", optional")]),t._v(" "),a("dd",[t._v("The "),a("code",[t._v("tokenizer")]),t._v(" defined with a "),a("code",[a("a",{attrs:{title:"biome.text.configuration.TokenizerConfiguration",href:"#biome.text.configuration.TokenizerConfiguration"}},[t._v("TokenizerConfiguration")])]),t._v(" object.")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("encoder")])]),t._v(" : "),a("code",[t._v("Seq2SeqEncoderSpec")])]),t._v(" "),a("dd",[t._v("The core text seq2seq "),a("code",[t._v("encoder")]),t._v(" of our model using a "),a("code",[t._v("Seq2SeqEncoderSpec")])])]),t._v(" "),a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"ancestors-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#ancestors-3"}},[t._v("#")]),t._v(" Ancestors")]),t._v("\n")]),t._v(" "),a("ul",{staticClass:"hlist"},[a("li",[t._v("allennlp.common.from_params.FromParams")])]),t._v(" "),a("dl",[a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"from-yaml"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#from-yaml"}},[t._v("#")]),t._v(" from_yaml "),a("Badge",{attrs:{text:"Static method"}})],1),t._v("\n")]),t._v(" "),a("dt",[a("div",{staticClass:"language-python extra-class"},[a("pre",{staticClass:"language-python"},[a("code",[t._v("\n"),a("span",{staticClass:"token keyword"},[t._v("def")]),t._v(" "),a("span",{staticClass:"ident"},[t._v("from_yaml")]),t._v("("),a("span",[t._v("path: str) -> "),a("a",{attrs:{title:"biome.text.configuration.PipelineConfiguration",href:"#biome.text.configuration.PipelineConfiguration"}},[t._v("PipelineConfiguration")])]),t._v("\n")]),t._v("\n")])])]),t._v(" "),a("dd",[a("p",[t._v("Creates a pipeline configuration from a config yaml file")]),t._v(" "),a("h2",{attrs:{id:"parameters"}},[t._v("Parameters")]),t._v(" "),a("dl",[a("dt",[a("strong",[a("code",[t._v("path")])]),t._v(" : "),a("code",[t._v("str")])]),t._v(" "),a("dd",[t._v("The path to a YAML configuration file")])]),t._v(" "),a("h2",{attrs:{id:"returns"}},[t._v("Returns")]),t._v(" "),a("dl",[a("dt",[a("strong",[a("code",[t._v("pipeline_configuration")])]),t._v(" : "),a("code",[a("a",{attrs:{title:"biome.text.configuration.PipelineConfiguration",href:"#biome.text.configuration.PipelineConfiguration"}},[t._v("PipelineConfiguration")])])]),t._v(" "),a("dd",[t._v(" ")])])]),t._v(" "),a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"from-dict"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#from-dict"}},[t._v("#")]),t._v(" from_dict "),a("Badge",{attrs:{text:"Static method"}})],1),t._v("\n")]),t._v(" "),a("dt",[a("div",{staticClass:"language-python extra-class"},[a("pre",{staticClass:"language-python"},[a("code",[t._v("\n"),a("span",{staticClass:"token keyword"},[t._v("def")]),t._v(" "),a("span",{staticClass:"ident"},[t._v("from_dict")]),t._v("("),a("span",[t._v("config_dict: dict) -> "),a("a",{attrs:{title:"biome.text.configuration.PipelineConfiguration",href:"#biome.text.configuration.PipelineConfiguration"}},[t._v("PipelineConfiguration")])]),t._v("\n")]),t._v("\n")])])]),t._v(" "),a("dd",[a("p",[t._v("Creates a pipeline configuration from a config dictionary")]),t._v(" "),a("h2",{attrs:{id:"parameters"}},[t._v("Parameters")]),t._v(" "),a("dl",[a("dt",[a("strong",[a("code",[t._v("config_dict")])]),t._v(" : "),a("code",[t._v("dict")])]),t._v(" "),a("dd",[t._v("A configuration dictionary")])]),t._v(" "),a("h2",{attrs:{id:"returns"}},[t._v("Returns")]),t._v(" "),a("dl",[a("dt",[a("strong",[a("code",[t._v("pipeline_configuration")])]),t._v(" : "),a("code",[a("a",{attrs:{title:"biome.text.configuration.PipelineConfiguration",href:"#biome.text.configuration.PipelineConfiguration"}},[t._v("PipelineConfiguration")])])]),t._v(" "),a("dd",[t._v(" ")])])])]),t._v(" "),a("dl",[a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"as-dict"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#as-dict"}},[t._v("#")]),t._v(" as_dict "),a("Badge",{attrs:{text:"Method"}})],1),t._v("\n")]),t._v(" "),a("dt",[a("div",{staticClass:"language-python extra-class"},[a("pre",{staticClass:"language-python"},[a("code",[t._v("\n"),a("span",{staticClass:"token keyword"},[t._v("def")]),t._v(" "),a("span",{staticClass:"ident"},[t._v("as_dict")]),t._v("("),a("span",[t._v("self) -> Dict[str, Any]")]),t._v("\n")]),t._v("\n")])])]),t._v(" "),a("dd"),t._v(" "),a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"to-yaml"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#to-yaml"}},[t._v("#")]),t._v(" to_yaml "),a("Badge",{attrs:{text:"Method"}})],1),t._v("\n")]),t._v(" "),a("dt",[a("div",{staticClass:"language-python extra-class"},[a("pre",{staticClass:"language-python"},[a("code",[t._v("\n"),a("span",{staticClass:"token keyword"},[t._v("def")]),t._v(" "),a("span",{staticClass:"ident"},[t._v("to_yaml")]),t._v(" ("),t._v("\n  self,\n  path: str,\n) \n")]),t._v("\n")])])]),t._v(" "),a("dd",[a("p",[t._v("Saves the pipeline configuration to a yaml formatted file")]),t._v(" "),a("h2",{attrs:{id:"parameters"}},[t._v("Parameters")]),t._v(" "),a("dl",[a("dt",[a("strong",[a("code",[t._v("path")])]),t._v(" : "),a("code",[t._v("str")])]),t._v(" "),a("dd",[t._v("Path to the output file")])])]),t._v(" "),a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"build-tokenizer"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#build-tokenizer"}},[t._v("#")]),t._v(" build_tokenizer "),a("Badge",{attrs:{text:"Method"}})],1),t._v("\n")]),t._v(" "),a("dt",[a("div",{staticClass:"language-python extra-class"},[a("pre",{staticClass:"language-python"},[a("code",[t._v("\n"),a("span",{staticClass:"token keyword"},[t._v("def")]),t._v(" "),a("span",{staticClass:"ident"},[t._v("build_tokenizer")]),t._v("("),a("span",[t._v("self) -> "),a("a",{attrs:{title:"biome.text.tokenizer.Tokenizer",href:"tokenizer.html#biome.text.tokenizer.Tokenizer"}},[t._v("Tokenizer")])]),t._v("\n")]),t._v("\n")])])]),t._v(" "),a("dd",[a("p",[t._v("Build the pipeline tokenizer")])]),t._v(" "),a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"build-featurizer"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#build-featurizer"}},[t._v("#")]),t._v(" build_featurizer "),a("Badge",{attrs:{text:"Method"}})],1),t._v("\n")]),t._v(" "),a("dt",[a("div",{staticClass:"language-python extra-class"},[a("pre",{staticClass:"language-python"},[a("code",[t._v("\n"),a("span",{staticClass:"token keyword"},[t._v("def")]),t._v(" "),a("span",{staticClass:"ident"},[t._v("build_featurizer")]),t._v("("),a("span",[t._v("self) -> "),a("a",{attrs:{title:"biome.text.featurizer.InputFeaturizer",href:"featurizer.html#biome.text.featurizer.InputFeaturizer"}},[t._v("InputFeaturizer")])]),t._v("\n")]),t._v("\n")])])]),t._v(" "),a("dd",[a("p",[t._v("Creates the pipeline featurizer")])]),t._v(" "),a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"build-embedder"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#build-embedder"}},[t._v("#")]),t._v(" build_embedder "),a("Badge",{attrs:{text:"Method"}})],1),t._v("\n")]),t._v(" "),a("dt",[a("div",{staticClass:"language-python extra-class"},[a("pre",{staticClass:"language-python"},[a("code",[t._v("\n"),a("span",{staticClass:"token keyword"},[t._v("def")]),t._v(" "),a("span",{staticClass:"ident"},[t._v("build_embedder")]),t._v(" ("),t._v("\n  self,\n  vocab: allennlp.data.vocabulary.Vocabulary,\n) \n")]),t._v("\n")])])]),t._v(" "),a("dd",[a("p",[t._v("Build the pipeline embedder for aiven dictionary")])])]),t._v(" "),a("div"),t._v(" "),a("pre",{staticClass:"title"},[a("h2",{attrs:{id:"trainerconfiguration"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#trainerconfiguration"}},[t._v("#")]),t._v(" TrainerConfiguration "),a("Badge",{attrs:{text:"Class"}})],1),t._v("\n")]),t._v(" "),a("pre",{staticClass:"language-python"},[a("code",[t._v("\n"),a("span",{staticClass:"token keyword"},[t._v("class")]),t._v(" "),a("span",{staticClass:"ident"},[t._v("TrainerConfiguration")]),t._v(" ("),t._v("\n    "),a("span",[t._v("optimizer: Dict[str, Any] = None")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("validation_metric: str = '-loss'")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("patience: Union[int, NoneType] = 2")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("num_epochs: int = 20")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("cuda_device: int = -1")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("grad_norm: Union[float, NoneType] = None")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("grad_clipping: Union[float, NoneType] = None")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("learning_rate_scheduler: Union[Dict[str, Any], NoneType] = None")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("momentum_scheduler: Union[Dict[str, Any], NoneType] = None")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("moving_average: Union[Dict[str, Any], NoneType] = None")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("batch_size: Union[int, NoneType] = 16")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("cache_instances: bool = True")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("in_memory_batches: int = 2")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("data_bucketing: bool = True")]),a("span",[t._v(",")]),t._v("\n"),a("span",[t._v(")")]),t._v("\n")]),t._v("\n")]),t._v(" "),a("p",[t._v("Creates a "),a("code",[a("a",{attrs:{title:"biome.text.configuration.TrainerConfiguration",href:"#biome.text.configuration.TrainerConfiguration"}},[t._v("TrainerConfiguration")])])]),t._v(" "),a("h2",{attrs:{id:"parameters"}},[t._v("Parameters")]),t._v(" "),a("dl",[a("dt",[a("strong",[a("code",[t._v("optimizer")])])]),t._v(" "),a("dd",[t._v(" ")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("validation_metric")])])]),t._v(" "),a("dd",[t._v(" ")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("patience")])])]),t._v(" "),a("dd",[t._v(" ")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("num_epochs")])])]),t._v(" "),a("dd",[t._v(" ")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("cuda_device")])])]),t._v(" "),a("dd",[t._v(" ")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("grad_norm")])])]),t._v(" "),a("dd",[t._v(" ")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("grad_clipping")])])]),t._v(" "),a("dd",[t._v(" ")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("learning_rate_scheduler")])])]),t._v(" "),a("dd",[t._v(" ")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("momentum_scheduler")])])]),t._v(" "),a("dd",[t._v(" ")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("moving_average")])])]),t._v(" "),a("dd",[t._v(" ")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("batch_size")])])]),t._v(" "),a("dd",[t._v(" ")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("cache_instances")])])]),t._v(" "),a("dd",[t._v(" ")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("in_memory_batches")])])]),t._v(" "),a("dd",[t._v(" ")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("data_bucketing")])])]),t._v(" "),a("dd",[t._v(" ")])]),t._v(" "),a("div"),t._v(" "),a("pre",{staticClass:"title"},[a("h2",{attrs:{id:"vocabularyconfiguration"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#vocabularyconfiguration"}},[t._v("#")]),t._v(" VocabularyConfiguration "),a("Badge",{attrs:{text:"Class"}})],1),t._v("\n")]),t._v(" "),a("pre",{staticClass:"language-python"},[a("code",[t._v("\n"),a("span",{staticClass:"token keyword"},[t._v("class")]),t._v(" "),a("span",{staticClass:"ident"},[t._v("VocabularyConfiguration")]),t._v(" ("),t._v("\n    "),a("span",[t._v("sources: List["),a("a",{attrs:{title:"biome.text.data.datasource.DataSource",href:"data/datasource.html#biome.text.data.datasource.DataSource"}},[t._v("DataSource")]),t._v("]")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("min_count: Dict[str, int] = None")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("max_vocab_size: Union[int, Dict[str, int]] = None")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("pretrained_files: Union[Dict[str, str], NoneType] = None")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("only_include_pretrained_words: bool = False")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("tokens_to_add: Dict[str, List[str]] = None")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("min_pretrained_embeddings: Dict[str, int] = None")]),a("span",[t._v(",")]),t._v("\n"),a("span",[t._v(")")]),t._v("\n")]),t._v("\n")]),t._v(" "),a("p",[t._v("Configures a "),a("code",[t._v("Vocabulary")]),t._v(" before it gets created from data")]),t._v(" "),a("p",[t._v("Use this to configure a Vocabulary using specific arguments from `allennlp.data.Vocabulary``")]),t._v(" "),a("p",[t._v("See "),a("a",{attrs:{href:"https://docs.allennlp.org/master/api/data/vocabulary/#vocabulary]"}},[t._v("AllenNLP Vocabulary docs")])]),t._v(" "),a("h2",{attrs:{id:"parameters"}},[t._v("Parameters")]),t._v(" "),a("dl",[a("dt",[a("strong",[a("code",[t._v("sources")])]),t._v(" : "),a("code",[t._v("List[DataSource]")])]),t._v(" "),a("dd",[t._v("Datasource to be used for data creation")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("min_count")])]),t._v(" : "),a("code",[t._v("Dict[str, int]")]),t._v(", optional "),a("code",[t._v("(default=None)")])]),t._v(" "),a("dd",[t._v("Minimum number of appearances of a token to be included in the vocabulary")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("max_vocab_size")])]),t._v(" : "),a("code"),t._v("Union[int, Dict[str, int]]"),a("code",[a("code",[t._v(", optional </code>(default=<code>None</code>)")])])]),t._v(" "),a("dd",[t._v("Maximum number of tokens of the vocabulary")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("pretrained_files")])]),t._v(" : "),a("code",[t._v("Optional[Dict[str, str]]")]),t._v(", optional")]),t._v(" "),a("dd",[t._v("Pretrained files with word vectors")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("only_include_pretrained_words")])]),t._v(" : "),a("code",[t._v("bool")]),t._v(", optional "),a("code",[t._v("(default=False)")])]),t._v(" "),a("dd",[t._v("Only include tokens present in pretrained_files")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("tokens_to_add")])]),t._v(" : "),a("code",[t._v("Dict[str, int]")]),t._v(", optional")]),t._v(" "),a("dd",[t._v("A list of tokens to add to the vocabulary, even if they are not present in the "),a("code",[t._v("sources")])]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("min_pretrained_embeddings")])]),t._v(" : "),a("code",[t._v("Dict[str, int]")]),t._v(", optional")]),t._v(" "),a("dd",[t._v("Minimum number of lines to keep from pretrained_files, even for tokens not appearing in the sources.")])])])}),[],!1,null,null,null);e.default=s.exports}}]);