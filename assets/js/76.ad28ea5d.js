(window.webpackJsonp=window.webpackJsonp||[]).push([[76],{401:function(e,s,t){"use strict";t.r(s);var i=t(18),a=Object(i.a)({},(function(){var e=this,s=e.$createElement,t=e._self._c||s;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("h1",{attrs:{id:"biome-text-models-sequence-classifier-base"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#biome-text-models-sequence-classifier-base"}},[e._v("#")]),e._v(" biome.text.models.sequence_classifier_base "),t("Badge",{attrs:{text:"Module"}})],1),e._v(" "),t("dl",[t("h2",{attrs:{id:"biome.text.models.sequence_classifier_base.SequenceClassifierBase"}},[e._v("SequenceClassifierBase "),t("Badge",{attrs:{text:"Class"}})],1),e._v(" "),t("dt",[t("div",{staticClass:"language-python extra-class"},[t("pre",{staticClass:"language-python"},[e._v("    "),t("code",[e._v("\n"),t("span",{staticClass:"token keyword"},[e._v("class")]),e._v(" "),t("span",{staticClass:"ident"},[e._v("SequenceClassifierBase")]),e._v(" ("),e._v("\n    "),t("span",[e._v("vocab: allennlp.data.vocabulary.Vocabulary")]),t("span",[e._v(",")]),e._v("\n    "),t("span",[e._v("text_field_embedder: allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder")]),t("span",[e._v(",")]),e._v("\n    "),t("span",[e._v("seq2vec_encoder: allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder")]),t("span",[e._v(",")]),e._v("\n    "),t("span",[e._v("seq2seq_encoder: Union[allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder, NoneType] = None")]),t("span",[e._v(",")]),e._v("\n    "),t("span",[e._v("multifield_seq2seq_encoder: Union[allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder, NoneType] = None")]),t("span",[e._v(",")]),e._v("\n    "),t("span",[e._v("multifield_seq2vec_encoder: Union[allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder, NoneType] = None")]),t("span",[e._v(",")]),e._v("\n    "),t("span",[e._v("feed_forward: Union[allennlp.modules.feedforward.FeedForward, NoneType] = None")]),t("span",[e._v(",")]),e._v("\n    "),t("span",[e._v("dropout: Union[float, NoneType] = None")]),t("span",[e._v(",")]),e._v("\n    "),t("span",[e._v("multifield_dropout: Union[float, NoneType] = None")]),t("span",[e._v(",")]),e._v("\n    "),t("span",[e._v("initializer: Union[allennlp.nn.initializers.InitializerApplicator, NoneType] = None")]),t("span",[e._v(",")]),e._v("\n    "),t("span",[e._v("regularizer: Union[allennlp.nn.regularizers.regularizer_applicator.RegularizerApplicator, NoneType] = None")]),t("span",[e._v(",")]),e._v("\n    "),t("span",[e._v("accuracy: Union[allennlp.training.metrics.categorical_accuracy.CategoricalAccuracy, NoneType] = None")]),t("span",[e._v(",")]),e._v("\n    "),t("span",[e._v("loss_weights: Dict[str, float] = None")]),t("span",[e._v(",")]),e._v("\n"),t("span",[e._v(")")]),e._v("\n    ")])])])]),e._v(" "),t("dd",[t("div",{staticClass:"desc"},[t("p",[e._v("In the most simple form this "),t("code",[e._v("BaseModelClassifier")]),e._v(" encodes a sequence with a "),t("code",[e._v("Seq2VecEncoder")]),e._v(", then\npredicts a label for the sequence.")]),e._v(" "),t("h2",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),t("dl",[t("dt",[t("strong",[t("code",[e._v("vocab")])])]),e._v(" "),t("dd",[e._v("A Vocabulary, required in order to compute sizes for input/output projections\nand passed on to the :class:"),t("code",[e._v("~allennlp.models.model.Model")]),e._v(" class.")]),e._v(" "),t("dt",[t("strong",[t("code",[e._v("text_field_embedder")])])]),e._v(" "),t("dd",[e._v("Used to embed the input text into a "),t("code",[e._v("TextField")])]),e._v(" "),t("dt",[t("strong",[t("code",[e._v("seq2seq_encoder")])])]),e._v(" "),t("dd",[e._v("Optional Seq2Seq encoder layer for the input text.")]),e._v(" "),t("dt",[t("strong",[t("code",[e._v("seq2vec_encoder")])])]),e._v(" "),t("dd",[e._v("Required Seq2Vec encoder layer. If "),t("code",[e._v("seq2seq_encoder")]),e._v(" is provided, this encoder\nwill pool its output. Otherwise, this encoder will operate directly on the output\nof the "),t("code",[e._v("text_field_embedder")]),e._v(".")]),e._v(" "),t("dt",[t("strong",[t("code",[e._v("dropout")])])]),e._v(" "),t("dd",[e._v("Dropout percentage to use on the output of the Seq2VecEncoder")]),e._v(" "),t("dt",[t("strong",[t("code",[e._v("multifield_seq2seq_encoder")])])]),e._v(" "),t("dd",[e._v("Optional Seq2Seq encoder layer for the encoded fields.")]),e._v(" "),t("dt",[t("strong",[t("code",[e._v("multifield_seq2vec_encoder")])])]),e._v(" "),t("dd",[e._v("If we use "),t("code",[e._v("ListField")]),e._v("s, this Seq2Vec encoder is required.\nIf "),t("code",[e._v("multifield_seq2seq_encoder")]),e._v(" is provided, this encoder will pool its output.\nOtherwise, this encoder will operate directly on the output of the "),t("code",[e._v("seq2vec_encoder")]),e._v(".")]),e._v(" "),t("dt",[t("strong",[t("code",[e._v("multifield_dropout")])])]),e._v(" "),t("dd",[e._v("Dropout percentage to use on the output of the doc Seq2VecEncoder")]),e._v(" "),t("dt",[t("strong",[t("code",[e._v("feed_forward")])])]),e._v(" "),t("dd",[e._v("A feed forward layer applied to the encoded inputs.")]),e._v(" "),t("dt",[t("strong",[t("code",[e._v("initializer")])])]),e._v(" "),t("dd",[e._v("Used to initialize the model parameters.")]),e._v(" "),t("dt",[t("strong",[t("code",[e._v("regularizer")])])]),e._v(" "),t("dd",[e._v("Used to regularize the model. Passed on to :class:"),t("code",[e._v("~allennlp.models.model.Model")]),e._v(".")]),e._v(" "),t("dt",[t("strong",[t("code",[e._v("accuracy")])])]),e._v(" "),t("dd",[e._v("The accuracy you want to use. By default, we choose a categorical top-1 accuracy.")]),e._v(" "),t("dt",[t("strong",[t("code",[e._v("loss_weights")])])]),e._v(" "),t("dd",[e._v("A dict with the labels and the corresponding weights.\nThese weights will be used in the CrossEntropyLoss function.")])]),e._v(" "),t("p",[e._v("Initializes internal Module state, shared by both nn.Module and ScriptModule.")])]),e._v(" "),t("h3",[e._v("Ancestors")]),e._v(" "),t("ul",{staticClass:"hlist"},[t("li",[t("a",{attrs:{title:"biome.text.models.mixins.BiomeClassifierMixin",href:"mixins.html#biome.text.models.mixins.BiomeClassifierMixin"}},[e._v("BiomeClassifierMixin")])]),e._v(" "),t("li",[e._v("allennlp.models.model.Model")]),e._v(" "),t("li",[e._v("torch.nn.modules.module.Module")]),e._v(" "),t("li",[e._v("allennlp.common.registrable.Registrable")]),e._v(" "),t("li",[e._v("allennlp.common.from_params.FromParams")])]),e._v(" "),t("h3",[e._v("Subclasses")]),e._v(" "),t("ul",{staticClass:"hlist"},[t("li",[t("a",{attrs:{title:"biome.text.models.sequence_classifier.SequenceClassifier",href:"sequence_classifier.html#biome.text.models.sequence_classifier.SequenceClassifier"}},[e._v("SequenceClassifier")])]),e._v(" "),t("li",[t("a",{attrs:{title:"biome.text.models.sequence_pair_classifier.SequencePairClassifier",href:"sequence_pair_classifier.html#biome.text.models.sequence_pair_classifier.SequencePairClassifier"}},[e._v("SequencePairClassifier")])]),e._v(" "),t("li",[t("a",{attrs:{title:"biome.text.models.similarity_classifier.SimilarityClassifier",href:"similarity_classifier.html#biome.text.models.similarity_classifier.SimilarityClassifier"}},[e._v("SimilarityClassifier")])])]),e._v(" "),t("h3",[e._v("Instance variables")]),e._v(" "),t("dl",[t("dt",{attrs:{id:"biome.text.models.sequence_classifier_base.SequenceClassifierBase.n_inputs"}},[t("code",{staticClass:"name"},[e._v("var "),t("span",{staticClass:"ident"},[e._v("n_inputs")])])]),e._v(" "),t("dd",[t("div",{staticClass:"desc"},[t("p",[e._v("This value is used for calculate the output layer dimension. Default value is 1")])])]),e._v(" "),t("dt",{attrs:{id:"biome.text.models.sequence_classifier_base.SequenceClassifierBase.num_classes"}},[t("code",{staticClass:"name"},[e._v("var "),t("span",{staticClass:"ident"},[e._v("num_classes")])])]),e._v(" "),t("dd",[t("div",{staticClass:"desc"},[t("p",[e._v("Number of output classes")])])]),e._v(" "),t("dt",{attrs:{id:"biome.text.models.sequence_classifier_base.SequenceClassifierBase.output_classes"}},[t("code",{staticClass:"name"},[e._v("var "),t("span",{staticClass:"ident"},[e._v("output_classes")]),e._v(" : List[str]")])]),e._v(" "),t("dd",[t("div",{staticClass:"desc"},[t("p",[e._v("The output token classes")])])])]),e._v(" "),t("dl",[t("h3",{attrs:{id:"biome.text.models.sequence_classifier_base.SequenceClassifierBase.label_for_index"}},[e._v("label_for_index "),t("Badge",{attrs:{text:"Method"}})],1),e._v(" "),t("dt",[t("div",{staticClass:"language-python extra-class"},[t("pre",{staticClass:"language-python"},[t("code",[e._v("\n"),t("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),t("span",{staticClass:"ident"},[e._v("label_for_index")]),e._v(" ("),e._v("\n   self,\n   idx,\n)  -> str\n")]),e._v("\n        ")])])]),e._v(" "),t("dd",[t("div",{staticClass:"desc"},[t("p",[e._v("Token label for label index")])])]),e._v(" "),t("h3",{attrs:{id:"biome.text.models.sequence_classifier_base.SequenceClassifierBase.extend_labels"}},[e._v("extend_labels "),t("Badge",{attrs:{text:"Method"}})],1),e._v(" "),t("dt",[t("div",{staticClass:"language-python extra-class"},[t("pre",{staticClass:"language-python"},[t("code",[e._v("\n"),t("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),t("span",{staticClass:"ident"},[e._v("extend_labels")]),e._v(" ("),e._v("\n   self,\n   labels: List[str],\n) \n")]),e._v("\n        ")])])]),e._v(" "),t("dd",[t("div",{staticClass:"desc"},[t("p",[e._v("Extends the number of output labels")])])]),e._v(" "),t("h3",{attrs:{id:"biome.text.models.sequence_classifier_base.SequenceClassifierBase.forward_tokens"}},[e._v("forward_tokens "),t("Badge",{attrs:{text:"Method"}})],1),e._v(" "),t("dt",[t("div",{staticClass:"language-python extra-class"},[t("pre",{staticClass:"language-python"},[t("code",[e._v("\n"),t("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),t("span",{staticClass:"ident"},[e._v("forward_tokens")]),e._v(" ("),e._v("\n   self,\n   tokens: Dict[str, torch.Tensor],\n)  -> torch.Tensor\n")]),e._v("\n        ")])])]),e._v(" "),t("dd",[t("div",{staticClass:"desc"},[t("p",[e._v("Apply the whole forward chain but last layer (output)")]),e._v(" "),t("h2",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),t("dl",[t("dt",[t("strong",[t("code",[e._v("tokens")])])]),e._v(" "),t("dd",[e._v("The tokens tensor")])]),e._v(" "),t("h2",{attrs:{id:"returns"}},[e._v("Returns")]),e._v(" "),t("p",[e._v("A "),t("code",[e._v("Tensor")])])])]),e._v(" "),t("h3",{attrs:{id:"biome.text.models.sequence_classifier_base.SequenceClassifierBase.output_layer"}},[e._v("output_layer "),t("Badge",{attrs:{text:"Method"}})],1),e._v(" "),t("dt",[t("div",{staticClass:"language-python extra-class"},[t("pre",{staticClass:"language-python"},[t("code",[e._v("\n"),t("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),t("span",{staticClass:"ident"},[e._v("output_layer")]),e._v(" ("),e._v("\n   self,\n   encoded_text: torch.Tensor,\n   label,\n)  -> Dict[str, torch.Tensor]\n")]),e._v("\n        ")])])]),e._v(" "),t("dd",[t("div",{staticClass:"desc"},[t("h2",{attrs:{id:"returns"}},[e._v("Returns")]),e._v(" "),t("dl",[t("dt",[t("code",[e._v("An output dictionary consisting of:")])]),e._v(" "),t("dd",[e._v(" ")]),e._v(" "),t("dt",[t("strong",[t("code",[e._v("logits")])]),e._v(" : "),t("code",[e._v(":class:")]),e._v("~torch.Tensor``")]),e._v(" "),t("dd",[e._v("A tensor of shape "),t("code",[e._v("(batch_size, num_classes)")]),e._v(" representing\nthe logits of the classifier model.")]),e._v(" "),t("dt",[t("strong",[t("code",[e._v("class_probabilities")])]),e._v(" : "),t("code",[e._v(":class:")]),e._v("~torch.Tensor``")]),e._v(" "),t("dd",[e._v("A tensor of shape "),t("code",[e._v("(batch_size, num_classes)")]),e._v(" representing\nthe softmax probabilities of the classes.")]),e._v(" "),t("dt",[t("strong",[t("code",[e._v("loss")])]),e._v(" : "),t("code",[e._v(":class:")]),e._v("~torch.Tensor``, optional")]),e._v(" "),t("dd",[e._v("A scalar loss to be optimised.")])])])]),e._v(" "),t("h3",{attrs:{id:"biome.text.models.sequence_classifier_base.SequenceClassifierBase.forward"}},[e._v("forward "),t("Badge",{attrs:{text:"Method"}})],1),e._v(" "),t("dt",[t("div",{staticClass:"language-python extra-class"},[t("pre",{staticClass:"language-python"},[t("code",[e._v("\n"),t("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),t("span",{staticClass:"ident"},[e._v("forward")]),e._v(" ("),e._v("\n   self,\n   *inputs,\n)  -> Dict[str, torch.Tensor]\n")]),e._v("\n        ")])])]),e._v(" "),t("dd",[t("div",{staticClass:"desc"},[t("p",[e._v("Defines the forward pass of the model. In addition, to facilitate easy training,\nthis method is designed to compute a loss function defined by a user.")]),e._v(" "),t("p",[e._v("The input is comprised of everything required to perform a\ntraining update, "),t("code",[e._v("including")]),e._v(" labels - you define the signature here!\nIt is down to the user to ensure that inference can be performed\nwithout the presence of these labels. Hence, any inputs not available at\ninference time should only be used inside a conditional block.")]),e._v(" "),t("p",[e._v("The intended sketch of this method is as follows::")]),e._v(" "),t("pre",[t("code",[e._v('def forward(self, input1, input2, targets=None):\n    ....\n    ....\n    output1 = self.layer1(input1)\n    output2 = self.layer2(input2)\n    output_dict = {"output1": output1, "output2": output2}\n    if targets is not None:\n        # Function returning a scalar torch.Tensor, defined by the user.\n        loss = self._compute_loss(output1, output2, targets)\n        output_dict["loss"] = loss\n    return output_dict\n')])]),e._v(" "),t("h2",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),t("p",[e._v("inputs:\nTensors comprising everything needed to perform a training update, "),t("code",[e._v("including")]),e._v(" labels,\nwhich should be optional (i.e have a default value of "),t("code",[e._v("None")]),e._v(").\nAt inference time,\nsimply pass the relevant inputs, not including the labels.")]),e._v(" "),t("h2",{attrs:{id:"returns"}},[e._v("Returns")]),e._v(" "),t("dl",[t("dt",[t("strong",[t("code",[e._v("output_dict")])]),e._v(" : "),t("code",[e._v("Dict[str, torch.Tensor]")])]),e._v(" "),t("dd",[e._v("The outputs from the model. In order to train a model using the\n:class:"),t("code",[e._v("~allennlp.training.Trainer")]),e._v(' api, you must provide a "loss" key pointing to a\nscalar '),t("code",[e._v("torch.Tensor")]),e._v(" representing the loss to be optimized.")])])])])]),e._v(" "),t("h3",[e._v("Inherited members")]),e._v(" "),t("ul",{staticClass:"hlist"},[t("li",[t("code",[t("b",[t("a",{attrs:{title:"biome.text.models.mixins.BiomeClassifierMixin",href:"mixins.html#biome.text.models.mixins.BiomeClassifierMixin"}},[e._v("BiomeClassifierMixin")])])]),e._v(":\n"),t("ul",{staticClass:"hlist"},[t("li",[t("code",[t("a",{attrs:{title:"biome.text.models.mixins.BiomeClassifierMixin.decode",href:"mixins.html#biome.text.models.mixins.BiomeClassifierMixin.decode"}},[e._v("decode")])])]),e._v(" "),t("li",[t("code",[t("a",{attrs:{title:"biome.text.models.mixins.BiomeClassifierMixin.get_metrics",href:"mixins.html#biome.text.models.mixins.BiomeClassifierMixin.get_metrics"}},[e._v("get_metrics")])])])])])])])])])}),[],!1,null,null,null);s.default=a.exports}}]);