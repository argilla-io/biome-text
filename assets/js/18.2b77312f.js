(window.webpackJsonp=window.webpackJsonp||[]).push([[18],{410:function(e,t,a){"use strict";a.r(t);var n=a(26),s=Object(n.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h1",{attrs:{id:"biome-text-configuration"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#biome-text-configuration"}},[e._v("#")]),e._v(" biome.text.configuration "),a("Badge",{attrs:{text:"Module"}})],1),e._v(" "),a("div"),e._v(" "),a("div"),e._v(" "),a("pre",{staticClass:"title"},[a("h2",{attrs:{id:"featuresconfiguration"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#featuresconfiguration"}},[e._v("#")]),e._v(" FeaturesConfiguration "),a("Badge",{attrs:{text:"Class"}})],1),e._v("\n")]),e._v(" "),a("pre",{staticClass:"language-python"},[a("code",[e._v("\n"),a("span",{staticClass:"token keyword"},[e._v("class")]),e._v(" "),a("span",{staticClass:"ident"},[e._v("FeaturesConfiguration")]),e._v(" ("),e._v("\n    "),a("span",[e._v("word: Union[biome.text.features.WordFeatures, NoneType] = None")]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("char: Union[biome.text.features.CharFeatures, NoneType] = None")]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("**extra_params")]),a("span",[e._v(",")]),e._v("\n"),a("span",[e._v(")")]),e._v("\n")]),e._v("\n")]),e._v(" "),a("p",[e._v("Creates a input featurizer configuration")]),e._v(" "),a("p",[e._v("This class will create a configuration for the features of the "),a("code",[e._v("Pipeline")]),e._v(".")]),e._v(" "),a("p",[e._v("Use this for defining the main features to be used by the model, namely word and character embeddings.")]),e._v(" "),a("p",[e._v(":::tip\nIf you do not pass "),a("code",[e._v("words")]),e._v(" and "),a("code",[e._v("chars")]),e._v(" your pipeline will be setup with default word features (embedding_dim=50).\n:::")]),e._v(" "),a("p",[e._v("Example:")]),e._v(" "),a("pre",[a("code",{staticClass:"python"},[e._v("word = WordFeatures(embedding_dim=100)\nchar = CharFeatures(embedding_dim=16, encoder={'type': 'gru'})\nconfig = FeaturesConfiguration(word, char)\n")])]),e._v(" "),a("h2",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),a("dl",[a("dt",[a("strong",[a("code",[e._v("word")])]),e._v(" : "),a("code",[a("a",{attrs:{title:"biome.text.features.WordFeatures",href:"features.html#biome.text.features.WordFeatures"}},[e._v("WordFeatures")])])]),e._v(" "),a("dd",[e._v(" ")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("char")])]),e._v(" : "),a("code",[a("a",{attrs:{title:"biome.text.features.CharFeatures",href:"features.html#biome.text.features.CharFeatures"}},[e._v("CharFeatures")])])]),e._v(" "),a("dd",[e._v(" ")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("extra_params")])])]),e._v(" "),a("dd",[e._v(" ")])]),e._v(" "),a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"ancestors"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#ancestors"}},[e._v("#")]),e._v(" Ancestors")]),e._v("\n")]),e._v(" "),a("ul",{staticClass:"hlist"},[a("li",[e._v("allennlp.common.from_params.FromParams")])]),e._v(" "),a("dl",[a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"from-params"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#from-params"}},[e._v("#")]),e._v(" from_params "),a("Badge",{attrs:{text:"Static method"}})],1),e._v("\n")]),e._v(" "),a("dt",[a("div",{staticClass:"language-python extra-class"},[a("pre",{staticClass:"language-python"},[a("code",[e._v("\n"),a("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),a("span",{staticClass:"ident"},[e._v("from_params")]),e._v(" ("),e._v("\n  params: allennlp.common.params.Params,\n  **extras,\n)  -> "),a("a",{attrs:{title:"biome.text.configuration.FeaturesConfiguration",href:"#biome.text.configuration.FeaturesConfiguration"}},[e._v("FeaturesConfiguration")]),e._v("\n")]),e._v("\n")])])]),e._v(" "),a("dd",[a("p",[e._v("This is the automatic implementation of "),a("code",[e._v("from_params")]),e._v(". Any class that subclasses "),a("code",[e._v("FromParams")]),e._v("\n(or "),a("code",[e._v("Registrable")]),e._v(", which itself subclasses "),a("code",[e._v("FromParams")]),e._v(') gets this implementation for free.\nIf you want your class to be instantiated from params in the "obvious" way – pop off parameters\nand hand them to your constructor with the same names – this provides that functionality.')]),e._v(" "),a("p",[e._v("If you need more complex logic in your from "),a("code",[e._v("from_params")]),e._v(" method, you'll have to implement\nyour own method that overrides this one.")])])]),e._v(" "),a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"instance-variables"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#instance-variables"}},[e._v("#")]),e._v(" Instance variables")]),e._v("\n")]),e._v(" "),a("dl",[a("dt",{attrs:{id:"biome.text.configuration.FeaturesConfiguration.keys"}},[a("code",{staticClass:"name"},[e._v("var "),a("span",{staticClass:"ident"},[e._v("keys")]),e._v(" : List[str]")])]),e._v(" "),a("dd",[a("p",[e._v("Gets the key features")])])]),e._v(" "),a("dl",[a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"compile-embedder"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#compile-embedder"}},[e._v("#")]),e._v(" compile_embedder "),a("Badge",{attrs:{text:"Method"}})],1),e._v("\n")]),e._v(" "),a("dt",[a("div",{staticClass:"language-python extra-class"},[a("pre",{staticClass:"language-python"},[a("code",[e._v("\n"),a("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),a("span",{staticClass:"ident"},[e._v("compile_embedder")]),e._v(" ("),e._v("\n  self,\n  vocab: allennlp.data.vocabulary.Vocabulary,\n)  -> allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder\n")]),e._v("\n")])])]),e._v(" "),a("dd",[a("p",[e._v("Creates the embedder from configured features for a given vocabulary")])]),e._v(" "),a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"compile-featurizer"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#compile-featurizer"}},[e._v("#")]),e._v(" compile_featurizer "),a("Badge",{attrs:{text:"Method"}})],1),e._v("\n")]),e._v(" "),a("dt",[a("div",{staticClass:"language-python extra-class"},[a("pre",{staticClass:"language-python"},[a("code",[e._v("\n"),a("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),a("span",{staticClass:"ident"},[e._v("compile_featurizer")]),e._v(" ("),e._v("\n  self,\n  tokenizer: "),a("a",{attrs:{title:"biome.text.tokenizer.Tokenizer",href:"tokenizer.html#biome.text.tokenizer.Tokenizer"}},[e._v("Tokenizer")]),e._v(",\n)  -> "),a("a",{attrs:{title:"biome.text.featurizer.InputFeaturizer",href:"featurizer.html#biome.text.featurizer.InputFeaturizer"}},[e._v("InputFeaturizer")]),e._v("\n")]),e._v("\n")])])]),e._v(" "),a("dd",[a("p",[e._v("Creates a featurizer from the configuration object")]),e._v(" "),a("p",[e._v(":::tip")]),e._v(" "),a("p",[e._v("If you are creating configurations programmatically use this method to check that your config object contains\na valid configuration.")]),e._v(" "),a("p",[e._v(":::")]),e._v(" "),a("h2",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),a("dl",[a("dt",[a("strong",[a("code",[e._v("tokenizer")])]),e._v(" : "),a("code",[e._v("Tokenizer")])]),e._v(" "),a("dd",[e._v("tokenizer used for this featurizer")])]),e._v(" "),a("h2",{attrs:{id:"returns"}},[e._v("Returns")]),e._v(" "),a("p",[e._v("The configured "),a("code",[e._v("InputFeaturizer")])])])]),e._v(" "),a("div"),e._v(" "),a("pre",{staticClass:"title"},[a("h2",{attrs:{id:"tokenizerconfiguration"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#tokenizerconfiguration"}},[e._v("#")]),e._v(" TokenizerConfiguration "),a("Badge",{attrs:{text:"Class"}})],1),e._v("\n")]),e._v(" "),a("pre",{staticClass:"language-python"},[a("code",[e._v("\n"),a("span",{staticClass:"token keyword"},[e._v("class")]),e._v(" "),a("span",{staticClass:"ident"},[e._v("TokenizerConfiguration")]),e._v(" ("),e._v("\n    "),a("span",[e._v("lang: str = 'en'")]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("skip_empty_tokens: bool = False")]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("max_sequence_length: int = None")]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("max_nr_of_sentences: int = None")]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("text_cleaning: Union[Dict[str, Any], NoneType] = None")]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("segment_sentences: Union[bool, Dict[str, Any]] = False")]),a("span",[e._v(",")]),e._v("\n"),a("span",[e._v(")")]),e._v("\n")]),e._v("\n")]),e._v(" "),a("p",[e._v("Creates a "),a("code",[e._v("Tokenizer")]),e._v(" configuration")]),e._v(" "),a("h2",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),a("dl",[a("dt",[a("strong",[a("code",[e._v("lang")])])]),e._v(" "),a("dd",[e._v(" ")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("skip_empty_tokens")])])]),e._v(" "),a("dd",[e._v(" ")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("max_sequence_length")])])]),e._v(" "),a("dd",[e._v(" ")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("max_nr_of_sentences")])])]),e._v(" "),a("dd",[e._v(" ")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("text_cleaning")])])]),e._v(" "),a("dd",[e._v(" ")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("segment_sentences")])])]),e._v(" "),a("dd",[e._v(" ")])]),e._v(" "),a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"ancestors-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#ancestors-2"}},[e._v("#")]),e._v(" Ancestors")]),e._v("\n")]),e._v(" "),a("ul",{staticClass:"hlist"},[a("li",[e._v("allennlp.common.from_params.FromParams")])]),e._v(" "),a("dl",[a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"compile"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#compile"}},[e._v("#")]),e._v(" compile "),a("Badge",{attrs:{text:"Method"}})],1),e._v("\n")]),e._v(" "),a("dt",[a("div",{staticClass:"language-python extra-class"},[a("pre",{staticClass:"language-python"},[a("code",[e._v("\n"),a("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),a("span",{staticClass:"ident"},[e._v("compile")]),e._v("("),a("span",[e._v("self) -> "),a("a",{attrs:{title:"biome.text.tokenizer.Tokenizer",href:"tokenizer.html#biome.text.tokenizer.Tokenizer"}},[e._v("Tokenizer")])]),e._v("\n")]),e._v("\n")])])]),e._v(" "),a("dd",[a("p",[e._v("Build tokenizer object from its configuration")])])]),e._v(" "),a("div"),e._v(" "),a("pre",{staticClass:"title"},[a("h2",{attrs:{id:"pipelineconfiguration"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#pipelineconfiguration"}},[e._v("#")]),e._v(" PipelineConfiguration "),a("Badge",{attrs:{text:"Class"}})],1),e._v("\n")]),e._v(" "),a("pre",{staticClass:"language-python"},[a("code",[e._v("\n"),a("span",{staticClass:"token keyword"},[e._v("class")]),e._v(" "),a("span",{staticClass:"ident"},[e._v("PipelineConfiguration")]),e._v(" ("),e._v("\n    "),a("span",[e._v("name: str")]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("features: "),a("a",{attrs:{title:"biome.text.configuration.FeaturesConfiguration",href:"#biome.text.configuration.FeaturesConfiguration"}},[e._v("FeaturesConfiguration")])]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("head: "),a("a",{attrs:{title:"biome.text.modules.heads.defs.TaskHeadSpec",href:"modules/heads/defs.html#biome.text.modules.heads.defs.TaskHeadSpec"}},[e._v("TaskHeadSpec")])]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("tokenizer: Union[biome.text.configuration.TokenizerConfiguration, NoneType] = None")]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("encoder: Union[biome.text.modules.specs.allennlp_specs.Seq2SeqEncoderSpec, NoneType] = None")]),a("span",[e._v(",")]),e._v("\n"),a("span",[e._v(")")]),e._v("\n")]),e._v("\n")]),e._v(" "),a("p",[e._v('"Creates a '),a("code",[e._v("Pipeline")]),e._v(" configuration")]),e._v(" "),a("h2",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),a("dl",[a("dt",[a("strong",[a("code",[e._v("name")])]),e._v(" : "),a("code",[e._v("str")])]),e._v(" "),a("dd",[e._v("The "),a("code",[e._v("name")]),e._v(" for our pipeline")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("features")])]),e._v(" : "),a("code",[a("a",{attrs:{title:"biome.text.configuration.FeaturesConfiguration",href:"#biome.text.configuration.FeaturesConfiguration"}},[e._v("FeaturesConfiguration")])])]),e._v(" "),a("dd",[e._v("The input "),a("code",[e._v("features")]),e._v(" to be used by the model pipeline. We define this using a "),a("code",[a("a",{attrs:{title:"biome.text.configuration.FeaturesConfiguration",href:"#biome.text.configuration.FeaturesConfiguration"}},[e._v("FeaturesConfiguration")])]),e._v(" object.")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("head")])]),e._v(" : "),a("code",[e._v("TaskHeadSpec")])]),e._v(" "),a("dd",[e._v("The "),a("code",[e._v("head")]),e._v(" for the task, e.g., a LanguageModelling task, using a "),a("code",[e._v("TaskHeadSpec")]),e._v(" object.")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("tokenizer")])]),e._v(" : "),a("code",[a("a",{attrs:{title:"biome.text.configuration.TokenizerConfiguration",href:"#biome.text.configuration.TokenizerConfiguration"}},[e._v("TokenizerConfiguration")])]),e._v(", optional")]),e._v(" "),a("dd",[e._v("The "),a("code",[e._v("tokenizer")]),e._v(" defined with a "),a("code",[a("a",{attrs:{title:"biome.text.configuration.TokenizerConfiguration",href:"#biome.text.configuration.TokenizerConfiguration"}},[e._v("TokenizerConfiguration")])]),e._v(" object.")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("encoder")])]),e._v(" : "),a("code",[e._v("Seq2SeqEncoderSpec")])]),e._v(" "),a("dd",[e._v("The core text seq2seq "),a("code",[e._v("encoder")]),e._v(" of our model using a "),a("code",[e._v("Seq2SeqEncoderSpec")])])]),e._v(" "),a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"ancestors-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#ancestors-3"}},[e._v("#")]),e._v(" Ancestors")]),e._v("\n")]),e._v(" "),a("ul",{staticClass:"hlist"},[a("li",[e._v("allennlp.common.from_params.FromParams")])]),e._v(" "),a("dl",[a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"as-dict"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#as-dict"}},[e._v("#")]),e._v(" as_dict "),a("Badge",{attrs:{text:"Method"}})],1),e._v("\n")]),e._v(" "),a("dt",[a("div",{staticClass:"language-python extra-class"},[a("pre",{staticClass:"language-python"},[a("code",[e._v("\n"),a("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),a("span",{staticClass:"ident"},[e._v("as_dict")]),e._v("("),a("span",[e._v("self) -> Dict[str, Any]")]),e._v("\n")]),e._v("\n")])])]),e._v(" "),a("dd"),e._v(" "),a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"build-tokenizer"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#build-tokenizer"}},[e._v("#")]),e._v(" build_tokenizer "),a("Badge",{attrs:{text:"Method"}})],1),e._v("\n")]),e._v(" "),a("dt",[a("div",{staticClass:"language-python extra-class"},[a("pre",{staticClass:"language-python"},[a("code",[e._v("\n"),a("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),a("span",{staticClass:"ident"},[e._v("build_tokenizer")]),e._v("("),a("span",[e._v("self) -> "),a("a",{attrs:{title:"biome.text.tokenizer.Tokenizer",href:"tokenizer.html#biome.text.tokenizer.Tokenizer"}},[e._v("Tokenizer")])]),e._v("\n")]),e._v("\n")])])]),e._v(" "),a("dd",[a("p",[e._v("Build the pipeline tokenizer")])]),e._v(" "),a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"build-featurizer"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#build-featurizer"}},[e._v("#")]),e._v(" build_featurizer "),a("Badge",{attrs:{text:"Method"}})],1),e._v("\n")]),e._v(" "),a("dt",[a("div",{staticClass:"language-python extra-class"},[a("pre",{staticClass:"language-python"},[a("code",[e._v("\n"),a("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),a("span",{staticClass:"ident"},[e._v("build_featurizer")]),e._v("("),a("span",[e._v("self) -> "),a("a",{attrs:{title:"biome.text.featurizer.InputFeaturizer",href:"featurizer.html#biome.text.featurizer.InputFeaturizer"}},[e._v("InputFeaturizer")])]),e._v("\n")]),e._v("\n")])])]),e._v(" "),a("dd",[a("p",[e._v("Creates the pipeline featurizer")])]),e._v(" "),a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"build-embedder"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#build-embedder"}},[e._v("#")]),e._v(" build_embedder "),a("Badge",{attrs:{text:"Method"}})],1),e._v("\n")]),e._v(" "),a("dt",[a("div",{staticClass:"language-python extra-class"},[a("pre",{staticClass:"language-python"},[a("code",[e._v("\n"),a("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),a("span",{staticClass:"ident"},[e._v("build_embedder")]),e._v(" ("),e._v("\n  self,\n  vocab: allennlp.data.vocabulary.Vocabulary,\n) \n")]),e._v("\n")])])]),e._v(" "),a("dd",[a("p",[e._v("Build the pipeline embedder for aiven dictionary")])])]),e._v(" "),a("div"),e._v(" "),a("pre",{staticClass:"title"},[a("h2",{attrs:{id:"trainerconfiguration"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#trainerconfiguration"}},[e._v("#")]),e._v(" TrainerConfiguration "),a("Badge",{attrs:{text:"Class"}})],1),e._v("\n")]),e._v(" "),a("pre",{staticClass:"language-python"},[a("code",[e._v("\n"),a("span",{staticClass:"token keyword"},[e._v("class")]),e._v(" "),a("span",{staticClass:"ident"},[e._v("TrainerConfiguration")]),e._v(" ("),e._v("\n    "),a("span",[e._v("optimizer: Dict[str, Any]")]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("validation_metric: str = '-loss'")]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("patience: Union[int, NoneType] = None")]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("shuffle: bool = True")]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("num_epochs: int = 20")]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("cuda_device: int = -1")]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("grad_norm: Union[float, NoneType] = None")]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("grad_clipping: Union[float, NoneType] = None")]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("learning_rate_scheduler: Union[Dict[str, Any], NoneType] = None")]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("momentum_scheduler: Union[Dict[str, Any], NoneType] = None")]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("moving_average: Union[Dict[str, Any], NoneType] = None")]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("batch_size: Union[int, NoneType] = None")]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("cache_instances: bool = True")]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("in_memory_batches: int = 2")]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("data_bucketing: bool = True")]),a("span",[e._v(",")]),e._v("\n"),a("span",[e._v(")")]),e._v("\n")]),e._v("\n")]),e._v(" "),a("p",[e._v("Creates a "),a("code",[a("a",{attrs:{title:"biome.text.configuration.TrainerConfiguration",href:"#biome.text.configuration.TrainerConfiguration"}},[e._v("TrainerConfiguration")])])]),e._v(" "),a("h2",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),a("dl",[a("dt",[a("strong",[a("code",[e._v("optimizer")])])]),e._v(" "),a("dd",[e._v(" ")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("validation_metric")])])]),e._v(" "),a("dd",[e._v(" ")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("patience")])])]),e._v(" "),a("dd",[e._v(" ")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("shuffle")])])]),e._v(" "),a("dd",[e._v(" ")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("num_epochs")])])]),e._v(" "),a("dd",[e._v(" ")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("cuda_device")])])]),e._v(" "),a("dd",[e._v(" ")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("grad_norm")])])]),e._v(" "),a("dd",[e._v(" ")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("grad_clipping")])])]),e._v(" "),a("dd",[e._v(" ")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("learning_rate_scheduler")])])]),e._v(" "),a("dd",[e._v(" ")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("momentum_scheduler")])])]),e._v(" "),a("dd",[e._v(" ")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("moving_average")])])]),e._v(" "),a("dd",[e._v(" ")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("batch_size")])])]),e._v(" "),a("dd",[e._v(" ")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("cache_instances")])])]),e._v(" "),a("dd",[e._v(" ")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("in_memory_batches")])])]),e._v(" "),a("dd",[e._v(" ")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("data_bucketing")])])]),e._v(" "),a("dd",[e._v(" ")])]),e._v(" "),a("div"),e._v(" "),a("pre",{staticClass:"title"},[a("h2",{attrs:{id:"vocabularyconfiguration"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#vocabularyconfiguration"}},[e._v("#")]),e._v(" VocabularyConfiguration "),a("Badge",{attrs:{text:"Class"}})],1),e._v("\n")]),e._v(" "),a("pre",{staticClass:"language-python"},[a("code",[e._v("\n"),a("span",{staticClass:"token keyword"},[e._v("class")]),e._v(" "),a("span",{staticClass:"ident"},[e._v("VocabularyConfiguration")]),e._v(" ("),e._v("\n    "),a("span",[e._v("sources: List["),a("a",{attrs:{title:"biome.text.data.datasource.DataSource",href:"data/datasource.html#biome.text.data.datasource.DataSource"}},[e._v("DataSource")]),e._v("]")]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("min_count: Dict[str, int] = None")]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("max_vocab_size: Union[int, Dict[str, int]] = None")]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("pretrained_files: Union[Dict[str, str], NoneType] = None")]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("only_include_pretrained_words: bool = False")]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("tokens_to_add: Dict[str, List[str]] = None")]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("min_pretrained_embeddings: Dict[str, int] = None")]),a("span",[e._v(",")]),e._v("\n"),a("span",[e._v(")")]),e._v("\n")]),e._v("\n")]),e._v(" "),a("p",[e._v("Configures a "),a("code",[e._v("Vocabulary")]),e._v(" before it gets created from data")]),e._v(" "),a("p",[e._v("Use this to configure a Vocabulary using specific arguments from `allennlp.data.Vocabulary``")]),e._v(" "),a("p",[e._v("See "),a("a",{attrs:{href:"https://docs.allennlp.org/master/api/data/vocabulary/#vocabulary]"}},[e._v("AllenNLP Vocabulary docs")])]),e._v(" "),a("h2",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),a("dl",[a("dt",[a("strong",[a("code",[e._v("sources")])]),e._v(" : "),a("code",[e._v("List[DataSource]")])]),e._v(" "),a("dd",[e._v("Datasource to be used for data creation")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("min_count")])]),e._v(" : "),a("code",[e._v("Dict[str, int]")]),e._v(", optional "),a("code",[e._v("(default=None)")])]),e._v(" "),a("dd",[e._v("Minimum number of appearances of a token to be included in the vocabulary")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("max_vocab_size")])]),e._v(" : "),a("code"),e._v("Union[int, Dict[str, int]]"),a("code",[a("code",[e._v(", optional </code>(default=<code>None</code>)")])])]),e._v(" "),a("dd",[e._v("Maximum number of tokens of the vocabulary")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("pretrained_files")])]),e._v(" : "),a("code",[e._v("Optional[Dict[str, str]]")]),e._v(", optional")]),e._v(" "),a("dd",[e._v("Pretrained files with word vectors")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("only_include_pretrained_words")])]),e._v(" : "),a("code",[e._v("bool")]),e._v(", optional "),a("code",[e._v("(default=False)")])]),e._v(" "),a("dd",[e._v("Only include tokens present in pretrained_files")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("tokens_to_add")])]),e._v(" : "),a("code",[e._v("Dict[str, int]")]),e._v(", optional")]),e._v(" "),a("dd",[e._v("A list of tokens to add to the vocabulary, even if they are not present in the "),a("code",[e._v("sources")])]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("min_pretrained_embeddings")])]),e._v(" : "),a("code",[e._v("Dict[str, int]")]),e._v(", optional")]),e._v(" "),a("dd",[e._v("Minimum number of lines to keep from pretrained_files, even for tokens not appearing in the sources.")])])])}),[],!1,null,null,null);t.default=s.exports}}]);