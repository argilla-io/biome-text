(window.webpackJsonp=window.webpackJsonp||[]).push([[63],{441:function(t,a,e){"use strict";e.r(a);var n=e(26),s=Object(n.a)({},(function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("h1",{attrs:{id:"training-a-sequence-tagger-for-slot-filling"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#training-a-sequence-tagger-for-slot-filling"}},[t._v("#")]),t._v(" Training a sequence tagger for Slot Filling")]),t._v(" "),e("p",[e("a",{attrs:{target:"_blank",href:"https://www.recogn.ai/biome-text/documentation/tutorials/2-Training_a_sequence_tagger_for_Slot_Filling.html"}},[e("img",{staticClass:"icon",attrs:{src:"https://www.recogn.ai/biome-text/assets/img/biome-isotype.svg",width:"24"}})]),t._v(" "),e("a",{attrs:{href:"https://www.recogn.ai/biome-text/documentation/tutorials/2-Training_a_sequence_tagger_for_Slot_Filling.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("View on recogn.ai"),e("OutboundLink")],1)]),t._v(" "),e("p",[e("a",{attrs:{target:"_blank",href:"https://colab.research.google.com/github/recognai/biome-text/blob/master/docs/docs/documentation/tutorials/2-Training_a_sequence_tagger_for_Slot_Filling.ipynb"}},[e("img",{staticClass:"icon",attrs:{src:"https://www.tensorflow.org/images/colab_logo_32px.png",width:"24"}})]),t._v(" "),e("a",{attrs:{href:"https://colab.research.google.com/github/recognai/biome-text/blob/master/docs/docs/documentation/tutorials/2-Training_a_sequence_tagger_for_Slot_Filling.ipynb",target:"_blank",rel:"noopener noreferrer"}},[t._v("Run in Google Colab"),e("OutboundLink")],1)]),t._v(" "),e("p",[e("a",{attrs:{target:"_blank",href:"https://github.com/recognai/biome-text/blob/master/docs/docs/documentation/tutorials/2-Training_a_sequence_tagger_for_Slot_Filling.ipynb"}},[e("img",{staticClass:"icon",attrs:{src:"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png",width:"24"}})]),t._v(" "),e("a",{attrs:{href:"https://github.com/recognai/biome-text/blob/master/docs/docs/documentation/tutorials/2-Training_a_sequence_tagger_for_Slot_Filling.ipynb",target:"_blank",rel:"noopener noreferrer"}},[t._v("View source on GitHub"),e("OutboundLink")],1)]),t._v(" "),e("p",[t._v("In this tutorial we will train a sequence tagger for filling slots in spoken requests.\nThe goal is to look for specific pieces of information in the request and tag the corresponding tokens accordingly.\nThe requests will include several intents, from getting weather information to adding a song to a playlist, each requiring its own set of slots.\nTherefore, slot filling often goes hand in hand with intent classification.\nIn this tutorial, however, we will only focus on the slot filling task.")]),t._v(" "),e("p",[t._v("Slot filling is closely related to "),e("a",{attrs:{href:"https://en.wikipedia.org/wiki/Named-entity_recognition",target:"_blank",rel:"noopener noreferrer"}},[t._v("Named-entity recognition (NER)"),e("OutboundLink")],1),t._v(" and the model of this tutorial can also be used to train a NER system.")]),t._v(" "),e("p",[t._v("In this tutorial we will use the "),e("a",{attrs:{href:"https://github.com/snipsco/nlu-benchmark/tree/master/2017-06-custom-intent-engines",target:"_blank",rel:"noopener noreferrer"}},[t._v("SNIPS data set"),e("OutboundLink")],1),t._v(" adapted by "),e("a",{attrs:{href:"https://github.com/sz128/slot_filling_and_intent_detection_of_SLU/tree/master/data/snips",target:"_blank",rel:"noopener noreferrer"}},[t._v("Su Zhu"),e("OutboundLink")],1),t._v(" and our simple "),e("a",{attrs:{href:"https://biome-tutorials-data.s3-eu-west-1.amazonaws.com/token_classifier/data_prep.ipynb",target:"_blank",rel:"noopener noreferrer"}},[t._v("data preparation notebook"),e("OutboundLink")],1),t._v(".")]),t._v(" "),e("p",[t._v("When running this tutorial in Google Colab, make sure to install "),e("em",[t._v("biome.text")]),t._v(" first:")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("!pip install "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("U pip\n!pip install "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("U git"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v("https"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("//")]),t._v("github"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("com"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("recognai"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("biome"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("text"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("git\n")])])]),e("p",[t._v("Ignore warnings and don't forget to restart your runtime afterwards ("),e("em",[t._v("Runtime -> Restart runtime")]),t._v(").")]),t._v(" "),e("p",[e("em",[t._v("If")]),t._v(" you want to log your runs with "),e("a",{attrs:{href:"https://wandb.ai/home",target:"_blank",rel:"noopener noreferrer"}},[t._v("WandB"),e("OutboundLink")],1),t._v(", don't forget to install its client and log in.")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("!pip install wandb\n!wandb login\n")])])]),e("h2",{attrs:{id:"imports"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#imports"}},[t._v("#")]),t._v(" Imports")]),t._v(" "),e("p",[t._v("Let us first import all the stuff we need for this tutorial.")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" biome"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Pipeline"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Dataset"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" PipelineConfiguration"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" VocabularyConfiguration\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" biome"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("configuration "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" FeaturesConfiguration"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" WordFeatures"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" CharFeatures\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" biome"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("modules"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("configuration "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Seq2SeqEncoderConfiguration\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" biome"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("modules"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("heads "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" TokenClassificationConfiguration\n")])])]),e("h2",{attrs:{id:"explore-the-data"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#explore-the-data"}},[t._v("#")]),t._v(" Explore the data")]),t._v(" "),e("p",[t._v("Let's take a look at the data before starting with the configuration of our pipeline.\nFor this we create a "),e("code",[t._v("Dataset")]),t._v(" instance providing a path to our downloaded data.")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("!curl "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("O https"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("//")]),t._v("biome"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("tutorials"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("data"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("s3"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("eu"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("west"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.")]),t._v("amazonaws"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("com"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("token_classifier"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("train"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("json\n!curl "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("O https"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("//")]),t._v("biome"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("tutorials"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("data"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("s3"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("eu"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("west"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.")]),t._v("amazonaws"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("com"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("token_classifier"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("valid"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("json\n!curl "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("O https"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("//")]),t._v("biome"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("tutorials"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("data"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("s3"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("eu"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("west"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.")]),t._v("amazonaws"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("com"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("token_classifier"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("test"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("json\n")])])]),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("train_ds "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Dataset"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_json"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"train.json"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nvalid_ds "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Dataset"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_json"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"valid.json"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntest_ds "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Dataset"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_json"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"test.json"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("p",[t._v("The "),e("a",{attrs:{href:"https://www.recogn.ai/biome-text/api/biome/text/dataset.html#dataset",target:"_blank",rel:"noopener noreferrer"}},[t._v("Dataset"),e("OutboundLink")],1),t._v(" class is a very thin wrapper around HuggingFace's awesome "),e("a",{attrs:{href:"https://huggingface.co/docs/datasets/master/package_reference/main_classes.html#datasets.Dataset",target:"_blank",rel:"noopener noreferrer"}},[t._v("datasets.Dataset"),e("OutboundLink")],1),t._v(".\nMost of HuggingFace's "),e("code",[t._v("Dataset")]),t._v(" API is exposed and you can checkout their nice "),e("a",{attrs:{href:"https://huggingface.co/docs/datasets/master/processing.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("documentation"),e("OutboundLink")],1),t._v(" on how to work with data in a "),e("code",[t._v("Dataset")]),t._v(". For example, let's quickly check the size of our training data and print the first 10 examples as a pandas dataframe:")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Training data size:"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train_ds"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),t._v(" train_ds"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("formatted_as"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"pandas"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    display"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train_ds"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("div",{staticClass:"language- extra-class"},[e("pre",[e("code",[t._v("Training data size: 13084\n")])])]),e("div",[e("style",{attrs:{scoped:""}},[t._v('\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\x3c!--beforebegin--\x3e<div class="language- extra-class">\x3c!--afterbegin--\x3e<pre><code>.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n</code></pre>\n\x3c!--beforeend--\x3e</div>\x3c!--afterend--\x3e')]),t._v(" "),e("table",{staticClass:"dataframe",attrs:{border:"1"}},[e("thead",[e("tr",{staticStyle:{"text-align":"right"}},[e("th"),t._v(" "),e("th",[t._v("text")]),t._v(" "),e("th",[t._v("labels")]),t._v(" "),e("th",[t._v("intent")])])]),t._v(" "),e("tbody",[e("tr",[e("th",[t._v("0")]),t._v(" "),e("td",[t._v("[Find, the, schedule, for, Across, the, Line, ...")]),t._v(" "),e("td",[t._v("[O, O, B-object_type, O, B-movie_name, I-movie...")]),t._v(" "),e("td",[t._v("SearchScreeningEvent")])]),t._v(" "),e("tr",[e("th",[t._v("1")]),t._v(" "),e("td",[t._v("[play, Party, Ben, on, Slacker]")]),t._v(" "),e("td",[t._v("[O, B-artist, I-artist, O, B-service]")]),t._v(" "),e("td",[t._v("PlayMusic")])]),t._v(" "),e("tr",[e("th",[t._v("2")]),t._v(" "),e("td",[t._v("[play, a, 1988, soundtrack]")]),t._v(" "),e("td",[t._v("[O, O, B-year, B-music_item]")]),t._v(" "),e("td",[t._v("PlayMusic")])]),t._v(" "),e("tr",[e("th",[t._v("3")]),t._v(" "),e("td",[t._v("[Can, you, play, The, Change, Is, Made, on, Ne...")]),t._v(" "),e("td",[t._v("[O, O, O, B-track, I-track, I-track, I-track, ...")]),t._v(" "),e("td",[t._v("PlayMusic")])]),t._v(" "),e("tr",[e("th",[t._v("4")]),t._v(" "),e("td",[t._v("[what, is, the, forecast, for, colder, in, Ans...")]),t._v(" "),e("td",[t._v("[O, O, O, O, O, B-condition_temperature, O, B-...")]),t._v(" "),e("td",[t._v("GetWeather")])]),t._v(" "),e("tr",[e("th",[t._v("5")]),t._v(" "),e("td",[t._v("[What's, the, weather, in, Totowa, WY, one, mi...")]),t._v(" "),e("td",[t._v("[O, O, O, O, B-city, B-state, B-timeRange, I-t...")]),t._v(" "),e("td",[t._v("GetWeather")])]),t._v(" "),e("tr",[e("th",[t._v("6")]),t._v(" "),e("td",[t._v("[Play, a, tune, from, Space, Mandino, .]")]),t._v(" "),e("td",[t._v("[O, O, B-music_item, O, B-artist, I-artist, O]")]),t._v(" "),e("td",[t._v("PlayMusic")])]),t._v(" "),e("tr",[e("th",[t._v("7")]),t._v(" "),e("td",[t._v("[give, five, out, of, 6, stars, to, current, e...")]),t._v(" "),e("td",[t._v("[O, B-rating_value, O, O, B-best_rating, B-rat...")]),t._v(" "),e("td",[t._v("RateBook")])]),t._v(" "),e("tr",[e("th",[t._v("8")]),t._v(" "),e("td",[t._v("[Play, some, chanson, style, music.]")]),t._v(" "),e("td",[t._v("[O, O, B-genre, O, O]")]),t._v(" "),e("td",[t._v("PlayMusic")])]),t._v(" "),e("tr",[e("th",[t._v("9")]),t._v(" "),e("td",[t._v("[I, would, give, French, Poets, and, Novelists...")]),t._v(" "),e("td",[t._v("[O, O, O, B-object_name, I-object_name, I-obje...")]),t._v(" "),e("td",[t._v("RateBook")])])])])]),t._v(" "),e("p",[t._v("As we can see we have two relevant columns for our task: "),e("em",[t._v("text")]),t._v(" and "),e("em",[t._v("labels")]),t._v(".\nThe "),e("em",[t._v("intent")]),t._v(" column will be ignored in this tutorial.")]),t._v(" "),e("p",[t._v("The text input already comes pre-tokenized as a list of strings and each token in the "),e("em",[t._v("text")]),t._v(" column has a label/tag in the "),e("em",[t._v("labels")]),t._v(" column, this means that both list always have the same length.\nThe labels are given in the "),e("a",{attrs:{href:"https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)",target:"_blank",rel:"noopener noreferrer"}},[t._v("BIO tagging scheme"),e("OutboundLink")],1),t._v(", which is widely used in Slot Filling/NER systems.")]),t._v(" "),e("p",[t._v("We can quickly check how many different labels there are in our dataset:")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("labels "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("tag"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" tags "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" train_ds"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"labels"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" tag "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" tags "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" tag "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("!=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"O"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"number of lables:"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("labels"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nlabels\n")])])]),e("div",{staticClass:"language- extra-class"},[e("pre",[e("code",[t._v("number of lables: 39\n\n\n\n\n\n{'album',\n 'artist',\n 'best_rating',\n 'city',\n 'condition_description',\n 'condition_temperature',\n 'country',\n 'cuisine',\n 'current_location',\n 'entity_name',\n 'facility',\n 'genre',\n 'geographic_poi',\n 'location_name',\n 'movie_name',\n 'movie_type',\n 'music_item',\n 'object_location_type',\n 'object_name',\n 'object_part_of_series_type',\n 'object_select',\n 'object_type',\n 'party_size_description',\n 'party_size_number',\n 'playlist',\n 'playlist_owner',\n 'poi',\n 'rating_unit',\n 'rating_value',\n 'restaurant_name',\n 'restaurant_type',\n 'served_dish',\n 'service',\n 'sort',\n 'spatial_relation',\n 'state',\n 'timeRange',\n 'track',\n 'year'}\n")])])]),e("p",[t._v("Since the the "),e("a",{attrs:{href:"https://www.recogn.ai/biome-text/api/biome/text/modules/heads/task_head.html#taskhead",target:"_blank",rel:"noopener noreferrer"}},[t._v("TaskHead"),e("OutboundLink")],1),t._v(" of our model (the "),e("a",{attrs:{href:"https://www.recogn.ai/biome-text/api/biome/text/modules/heads/token_classification.html#tokenclassification",target:"_blank",rel:"noopener noreferrer"}},[t._v("TokenClassification"),e("OutboundLink")],1),t._v(" head) expects a "),e("em",[t._v("text")]),t._v(" and a "),e("em",[t._v("tags")]),t._v(" column to be present in the Dataset, we need to rename the "),e("em",[t._v("labels")]),t._v(" column:")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" ds "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("train_ds"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" valid_ds"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" test_ds"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n     ds"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("rename_column_"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"labels"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"tags"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("div",{staticClass:"custom-block tip"},[e("p",{staticClass:"custom-block-title"},[t._v("Tip")]),t._v(" "),e("p",[t._v("The "),e("a",{attrs:{href:"https://www.recogn.ai/biome-text/api/biome/text/modules/heads/token_classification.html#tokenclassification",target:"_blank",rel:"noopener noreferrer"}},[t._v("TokenClassification"),e("OutboundLink")],1),t._v(" head also supports a "),e("em",[t._v("entities")]),t._v(" column instead of a "),e("em",[t._v("tags")]),t._v(" column, in which case the entities have to be a list of python dictionaries with a "),e("code",[t._v("start")]),t._v(", "),e("code",[t._v("end")]),t._v(" and "),e("code",[t._v("label")]),t._v(" key that correspond to the char indexes and the label of the entity, respectively.")])]),t._v(" "),e("h2",{attrs:{id:"configure-your-biome-text-pipeline"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#configure-your-biome-text-pipeline"}},[t._v("#")]),t._v(" Configure your "),e("em",[t._v("biome.text")]),t._v(" Pipeline")]),t._v(" "),e("p",[t._v("A typical "),e("a",{attrs:{href:"https://www.recogn.ai/biome-text/api/biome/text/pipeline.html#pipeline",target:"_blank",rel:"noopener noreferrer"}},[t._v("Pipeline"),e("OutboundLink")],1),t._v(" consists of tokenizing the input, extracting features, applying a language encoding (optionally) and executing a task-specific head in the end.\nAfter training a pipeline, you can use it to make predictions or explore the underlying model via the explore UI.")]),t._v(" "),e("p",[t._v("A "),e("em",[t._v("biome.text")]),t._v(" pipeline has the following main components:")]),t._v(" "),e("div",{staticClass:"language-yaml extra-class"},[e("pre",{pre:!0,attrs:{class:"language-yaml"}},[e("code",[e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("name")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# a descriptive name of your pipeline")]),t._v("\n\n"),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("tokenizer")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# how to tokenize the input")]),t._v("\n\n"),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("features")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# input features of the model")]),t._v("\n\n"),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("encoder")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# the language encoder")]),t._v("\n\n"),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("head")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# your task configuration")]),t._v("\n\n")])])]),e("p",[t._v("See the "),e("a",{attrs:{href:"https://www.recogn.ai/biome-text/documentation/user-guides/2-configuration.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("Configuration section"),e("OutboundLink")],1),t._v(" for a detailed description of how these main components can be configured.")]),t._v(" "),e("p",[t._v("In this tutorial we will create a "),e("a",{attrs:{href:"https://www.recogn.ai/biome-text/api/biome/text/configuration.html#pipelineconfiguration",target:"_blank",rel:"noopener noreferrer"}},[t._v("PipelineConfiguration"),e("OutboundLink")],1),t._v(" programmatically, and use it to initialize the "),e("a",{attrs:{href:"https://www.recogn.ai/biome-text/api/biome/text/pipeline.html#pipeline",target:"_blank",rel:"noopener noreferrer"}},[t._v("Pipeline"),e("OutboundLink")],1),t._v(".\nYou can also create your pipelines by providing a "),e("a",{attrs:{href:"https://www.recogn.ai/biome-text/api/biome/text/pipeline.html#from-config",target:"_blank",rel:"noopener noreferrer"}},[t._v("python dictionary"),e("OutboundLink")],1),t._v(" (see the text classification "),e("a",{attrs:{href:"https://www.recogn.ai/biome-text/documentation/tutorials/1-Training_a_text_classifier.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("tutorial"),e("OutboundLink")],1),t._v("), a YAML "),e("a",{attrs:{href:"https://www.recogn.ai/biome-text/api/biome/text/pipeline.html#from-yaml",target:"_blank",rel:"noopener noreferrer"}},[t._v("configuration file"),e("OutboundLink")],1),t._v(" or a "),e("a",{attrs:{href:"https://www.recogn.ai/biome-text/api/biome/text/pipeline.html#from-pretrained",target:"_blank",rel:"noopener noreferrer"}},[t._v("pretrained model"),e("OutboundLink")],1),t._v(".")]),t._v(" "),e("p",[t._v("A pipeline configuration is composed of several other "),e("a",{attrs:{href:"https://www.recogn.ai/biome-text/api/biome/text/configuration.html#biome-text-configuration",target:"_blank",rel:"noopener noreferrer"}},[t._v("configuration classes"),e("OutboundLink")],1),t._v(", each one corresponding to one of the main components.")]),t._v(" "),e("h3",{attrs:{id:"features"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#features"}},[t._v("#")]),t._v(" Features")]),t._v(" "),e("p",[t._v("Let us first configure the features of our pipeline.\nFor our "),e("code",[t._v("word")]),t._v(" feature we will use pretrained embeddings from "),e("a",{attrs:{href:"https://fasttext.cc/docs/en/english-vectors.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("fasttext"),e("OutboundLink")],1),t._v(", and our "),e("code",[t._v("char")]),t._v(" feature will use the last hidden state of a "),e("a",{attrs:{href:"https://en.wikipedia.org/wiki/Gated_recurrent_unit",target:"_blank",rel:"noopener noreferrer"}},[t._v("GRU"),e("OutboundLink")],1),t._v(" encoder to represent a word based on its characters.\nKeep in mind that the "),e("code",[t._v("embedding_dim")]),t._v(" parameter for the "),e("code",[t._v("word")]),t._v(" feature must be equal to the dimensions of the pretrained embeddings!")]),t._v(" "),e("div",{staticClass:"custom-block tip"},[e("p",{staticClass:"custom-block-title"},[t._v("Tip")]),t._v(" "),e("p",[t._v("If you do not provide any feature configurations, we will choose a very basic "),e("code",[t._v("word")]),t._v(" feature by default.")])]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("word_feature "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" WordFeatures"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    embedding_dim"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("300")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    weights_file"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nchar_feature "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" CharFeatures"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    embedding_dim"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("32")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    encoder"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"gru"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"bidirectional"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"num_layers"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"hidden_size"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("32")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    dropout"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nfeatures_config "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" FeaturesConfiguration"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    word"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("word_feature"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n    char"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("char_feature\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("h3",{attrs:{id:"encoder"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#encoder"}},[t._v("#")]),t._v(" Encoder")]),t._v(" "),e("p",[t._v("Next we will configure our encoder that takes as input a sequence of embedded word vectors and returns a sequence of encoded word vectors.\nFor this encoding we will use another larger GRU:")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("encoder_config "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Seq2SeqEncoderConfiguration"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("type")]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"gru"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    bidirectional"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    num_layers"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    hidden_size"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("h3",{attrs:{id:"head"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#head"}},[t._v("#")]),t._v(" Head")]),t._v(" "),e("p",[t._v("The final configuration belongs to our "),e("a",{attrs:{href:"https://www.recogn.ai/biome-text/api/biome/text/modules/heads/task_head.html#taskhead",target:"_blank",rel:"noopener noreferrer"}},[t._v("TaskHead"),e("OutboundLink")],1),t._v(".\nIt reflects the task our problem belongs to and can be easily exchanged with other types of heads keeping the same features and encoder.")]),t._v(" "),e("div",{staticClass:"custom-block tip"},[e("p",{staticClass:"custom-block-title"},[t._v("Tip")]),t._v(" "),e("p",[t._v("Exchanging the heads you can easily pretrain a model on a certain task, such as "),e("a",{attrs:{href:"https://www.recogn.ai/biome-text/api/biome/text/modules/heads/language_modelling.html#languagemodelling",target:"_blank",rel:"noopener noreferrer"}},[t._v("language modelling"),e("OutboundLink")],1),t._v(", and use its pretrained features and encoder for training the model on another task.")])]),t._v(" "),e("p",[t._v("For our task we will use a "),e("a",{attrs:{href:"https://www.recogn.ai/biome-text/api/biome/text/modules/heads/token_classification.html#tokenclassification",target:"_blank",rel:"noopener noreferrer"}},[t._v("TokenClassification"),e("OutboundLink")],1),t._v(" head that allows us to tag each token individually:")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("head_config "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TokenClassificationConfiguration"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    labels"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("labels"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    label_encoding"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"BIO"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    top_k"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    feedforward"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"num_layers"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"hidden_dims"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"activations"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"relu"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"dropout"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("h3",{attrs:{id:"pipeline"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#pipeline"}},[t._v("#")]),t._v(" Pipeline")]),t._v(" "),e("p",[t._v("Now we can create a "),e("a",{attrs:{href:"https://www.recogn.ai/biome-text/api/biome/text/configuration.html#pipelineconfiguration",target:"_blank",rel:"noopener noreferrer"}},[t._v("PipelineConfiguration"),e("OutboundLink")],1),t._v(" and finally initialize our "),e("a",{attrs:{href:"https://www.recogn.ai/biome-text/api/biome/text/pipeline.html#pipeline",target:"_blank",rel:"noopener noreferrer"}},[t._v("Pipeline"),e("OutboundLink")],1),t._v(".")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("pipeline_config "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" PipelineConfiguration"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    name"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"slot_filling_tutorial"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    features"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("features_config"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    encoder"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("encoder_config"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    head"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("head_config"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("pl "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Pipeline"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_config"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pipeline_config"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("h2",{attrs:{id:"create-a-vocabulary"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#create-a-vocabulary"}},[t._v("#")]),t._v(" Create a vocabulary")]),t._v(" "),e("p",[t._v("Before we can start the training we need to create the vocabulary for our model.\nSince we use pretrained word embeddings we will not only consider the training data, but also the validation data when creating the vocabulary.\nIn addition, we get rid of the rarest words by adding the "),e("code",[t._v("min_count")]),t._v(" argument and set it to 2 for the word feature vocabulary.\nFor a complete list of available arguments see the "),e("a",{attrs:{href:"https://www.recogn.ai/biome-text/api/biome/text/configuration.html#vocabularyconfiguration",target:"_blank",rel:"noopener noreferrer"}},[t._v("VocabularyConfiguration API"),e("OutboundLink")],1),t._v(".")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("vocab_config "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" VocabularyConfiguration"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    sources"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("train_ds"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" valid_ds"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    min_count"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"word"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("p",[t._v("We then pass this configuration to our "),e("code",[t._v("Pipeline")]),t._v(" to create the vocabulary.\nApart from the loading bar of building the vocabulary, there will be two more loading bars corresponding to the "),e("code",[t._v("weights_file")]),t._v(" provided in the word feature:")]),t._v(" "),e("ul",[e("li",[t._v("the progress of downloading the file (this file will be cached)")]),t._v(" "),e("li",[t._v("the progress loading the weights from the file")])]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("pl"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("create_vocabulary"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("vocab_config"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("p",[t._v("After creating the vovocab_configbulary we can check the size of our entire model in terms of trainable parameters:")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("pl"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("num_trainable_parameters\n")])])]),e("div",{staticClass:"language- extra-class"},[e("pre",[e("code",[t._v("1991086\n")])])]),e("h2",{attrs:{id:"train-your-model"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#train-your-model"}},[t._v("#")]),t._v(" Train your model")]),t._v(" "),e("p",[t._v("Now we have everything ready to start the training of our model:")]),t._v(" "),e("ul",[e("li",[t._v("training data set")]),t._v(" "),e("li",[t._v("vocabulary")])]),t._v(" "),e("p",[t._v("As "),e("code",[t._v("trainer")]),t._v(" we will use the default configuration that has sensible values and works alright for our experiment.\n"),e("a",{attrs:{href:"https://www.recogn.ai/biome-text/documentation/tutorials/1-Training_a_text_classifier.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("This tutorial"),e("OutboundLink")],1),t._v(" shows you an example of how to configure a trainer.")]),t._v(" "),e("div",{staticClass:"custom-block tip"},[e("p",{staticClass:"custom-block-title"},[t._v("Tip")]),t._v(" "),e("p",[t._v("By default we will automatically use a CUDA device if available. If you want to tune the trainer or specifically not use a CUDA device, you can pass a "),e("code",[t._v("trainer = TrainerConfiguration(cuda_device=-1, ...)")]),t._v(" to the "),e("code",[t._v("Pipeline.train()")]),t._v(" method.\nSee the "),e("a",{attrs:{href:"https://www.recogn.ai/biome-text/api/biome/text/configuration.html#trainerconfiguration",target:"_blank",rel:"noopener noreferrer"}},[t._v("TrainerConfiguration API"),e("OutboundLink")],1),t._v(" for a complete list of available configurations.")])]),t._v(" "),e("p",[t._v("The training output will be saved in a folder specified by the "),e("code",[t._v("output")]),t._v(" argument of the "),e("code",[t._v("train")]),t._v(" method.\nIt will contain the trained model weights and the metrics, as well as the vocabulary and a "),e("em",[t._v("log")]),t._v(" folder for visualizing the training process with "),e("a",{attrs:{href:"https://www.tensorflow.org/tensorboard/",target:"_blank",rel:"noopener noreferrer"}},[t._v("tensorboard"),e("OutboundLink")],1),t._v(".")]),t._v(" "),e("p",[t._v("Apart from the validation data source to estimate the generalization error, we will also pass in a test data set in case we want to do some Hyperparameter optimization and compare different encoder architectures in the end.")]),t._v(" "),e("p",[t._v("When the training has finished it will automatically make a pass over the test data with the best weights to gather the test metrics.")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("pl"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    output"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"output"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    training"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("train_ds"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    validation"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("valid_ds"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    test"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("test_ds"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("p",[t._v("The model above achieves an overall F1 score of around "),e("strong",[t._v("0.95")]),t._v(", which is not bad when compared to "),e("a",{attrs:{href:"https://nlpprogress.com/english/intent_detection_slot_filling.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("published results"),e("OutboundLink")],1),t._v(" of the same data set.\nYou could continue the experiment changing the encoder to an LSTM network, try out a transformer architecture or fine tune the trainer.\nBut for now we will go on and make our first predictions with this trained model.")]),t._v(" "),e("h2",{attrs:{id:"make-your-first-predictions"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#make-your-first-predictions"}},[t._v("#")]),t._v(" Make your first predictions")]),t._v(" "),e("p",[t._v("Now that we trained our model we can go on to make our first predictions.\nFirst we must load our trained model into a new "),e("code",[t._v("Pipeline")]),t._v(":")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("pl_trained "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Pipeline"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_pretrained"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"output/model.tar.gz"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("p",[t._v("We then provide the input expected by our "),e("code",[t._v("TaskHead")]),t._v(" to the "),e("code",[t._v("Pipeline.predict()")]),t._v(" method.\nIn our case it is a "),e("code",[t._v("TokenClassification")]),t._v(" head that classifies a "),e("code",[t._v("text")]),t._v(" input.")]),t._v(" "),e("p",[t._v("You can either provide pretokenized tokens (list of strings) "),e("strong",[t._v("or")]),t._v(" a raw string to the "),e("code",[t._v("predict")]),t._v(" method. In the first case, you should make sure that those tokens were tokenized the same way the training data was tokenized, in the latter case you should make sure that the pipeline uses the same tokenizer as was used for the training data.")]),t._v(" "),e("p",[t._v("The prediction of the "),e("code",[t._v("TokenClassification")]),t._v(" head will always consist of a "),e("code",[t._v("tags")]),t._v(" and "),e("code",[t._v("entities")]),t._v(" key. Both keys will include the "),e("code",[t._v("top_k")]),t._v(" most likely tag/entity sequences for the input, where "),e("code",[t._v("top_k")]),t._v(" is a parameter specified in the "),e("code",[t._v("TokenClassificationConfiguration")]),t._v(" before the training.")]),t._v(" "),e("h3",{attrs:{id:"pretokenized-input"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#pretokenized-input"}},[t._v("#")]),t._v(" pretokenized input")]),t._v(" "),e("p",[t._v("For pretokenized input, the "),e("code",[t._v("entities")]),t._v(" key of the output holds dictionaries with the "),e("code",[t._v("start_token")]),t._v(" id, "),e("code",[t._v("end_token")]),t._v(" id and the label of the entity:")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("text "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Can you play biome text by backstreet recognais on Spotify ?"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nprediction "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pl_trained"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("text"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("text"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Predicted tags:\\n"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("zip")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("text"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" prediction"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"tags"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Predicted entities:\\n"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" prediction"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"entities"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("div",{staticClass:"language- extra-class"},[e("pre",[e("code",[t._v("Predicted tags:\n [('Can', 'O'), ('you', 'O'), ('play', 'O'), ('biome', 'B-album'), ('text', 'I-album'), ('by', 'O'), ('backstreet', 'B-artist'), ('recognais', 'I-artist'), ('on', 'O'), ('Spotify', 'B-service'), ('?', 'O')]\nPredicted entities:\n [{'start_token': 3, 'end_token': 5, 'label': 'album'}, {'start_token': 6, 'end_token': 8, 'label': 'artist'}, {'start_token': 9, 'end_token': 10, 'label': 'service'}]\n")])])]),e("h3",{attrs:{id:"string-input"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#string-input"}},[t._v("#")]),t._v(" string input")]),t._v(" "),e("p",[t._v("For a raw string input, the "),e("code",[t._v("entities")]),t._v(" key of the output holds dictionaries with the "),e("code",[t._v("start_token")]),t._v(" id, "),e("code",[t._v("end_token")]),t._v(" id, "),e("code",[t._v("start")]),t._v(" char id, "),e("code",[t._v("end")]),t._v(" char id and the label of the entity:")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("text "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Can you play biome text by backstreet recognais on Spotify ?"')]),t._v("\nprediction "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pl_trained"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("text"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("text"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Predicted tags:\\n"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("zip")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("text"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("split"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" prediction"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"tags"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Predicted entities:\\n"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" prediction"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"entities"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("div",{staticClass:"language- extra-class"},[e("pre",[e("code",[t._v("Predicted tags:\n [('Can', 'O'), ('you', 'O'), ('play', 'O'), ('biome', 'B-album'), ('text', 'I-album'), ('by', 'O'), ('backstreet', 'B-artist'), ('recognais', 'I-artist'), ('on', 'O'), ('Spotify', 'B-service'), ('?', 'O')]\nPredicted entities:\n [{'start_token': 3, 'end_token': 5, 'label': 'album', 'start': 13, 'end': 23}, {'start_token': 6, 'end_token': 8, 'label': 'artist', 'start': 27, 'end': 47}, {'start_token': 9, 'end_token': 10, 'label': 'service', 'start': 51, 'end': 58}]\n")])])])])}),[],!1,null,null,null);a.default=s.exports}}]);