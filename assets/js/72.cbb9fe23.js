(window.webpackJsonp=window.webpackJsonp||[]).push([[72],{413:function(e,t,o){"use strict";o.r(t);var n=o(18),a=Object(n.a)({},(function(){var e=this,t=e.$createElement,o=e._self._c||t;return o("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[o("h1",{attrs:{id:"biome-text-models-multifield-bimpm"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#biome-text-models-multifield-bimpm"}},[e._v("#")]),e._v(" biome.text.models.multifield_bimpm "),o("Badge",{attrs:{text:"Module"}})],1),e._v(" "),o("dl",[o("h2",{attrs:{id:"biome.text.models.multifield_bimpm.MultifieldBiMpm"}},[e._v("MultifieldBiMpm "),o("Badge",{attrs:{text:"Class"}})],1),e._v(" "),o("dt",[o("div",{staticClass:"language-python extra-class"},[o("pre",{staticClass:"language-python"},[e._v("    "),o("code",[e._v("\n"),o("span",{staticClass:"token keyword"},[e._v("class")]),e._v(" "),o("span",{staticClass:"ident"},[e._v("MultifieldBiMpm")]),e._v(" ("),e._v("\n    "),o("span",[e._v("vocab: allennlp.data.vocabulary.Vocabulary")]),o("span",[e._v(",")]),e._v("\n    "),o("span",[e._v("text_field_embedder: allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder")]),o("span",[e._v(",")]),e._v("\n    "),o("span",[e._v("matcher_word: allennlp.modules.bimpm_matching.BiMpmMatching")]),o("span",[e._v(",")]),e._v("\n    "),o("span",[e._v("encoder: allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder")]),o("span",[e._v(",")]),e._v("\n    "),o("span",[e._v("matcher_forward: allennlp.modules.bimpm_matching.BiMpmMatching")]),o("span",[e._v(",")]),e._v("\n    "),o("span",[e._v("aggregator: allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder")]),o("span",[e._v(",")]),e._v("\n    "),o("span",[e._v("classifier_feedforward: allennlp.modules.feedforward.FeedForward")]),o("span",[e._v(",")]),e._v("\n    "),o("span",[e._v("matcher_backward: allennlp.modules.bimpm_matching.BiMpmMatching = None")]),o("span",[e._v(",")]),e._v("\n    "),o("span",[e._v("encoder2: allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder = None")]),o("span",[e._v(",")]),e._v("\n    "),o("span",[e._v("matcher2_forward: allennlp.modules.bimpm_matching.BiMpmMatching = None")]),o("span",[e._v(",")]),e._v("\n    "),o("span",[e._v("matcher2_backward: allennlp.modules.bimpm_matching.BiMpmMatching = None")]),o("span",[e._v(",")]),e._v("\n    "),o("span",[e._v("dropout: float = 0.1")]),o("span",[e._v(",")]),e._v("\n    "),o("span",[e._v("multifield: bool = True")]),o("span",[e._v(",")]),e._v("\n    "),o("span",[e._v("initializer: allennlp.nn.initializers.InitializerApplicator = <allennlp.nn.initializers.InitializerApplicator object>")]),o("span",[e._v(",")]),e._v("\n    "),o("span",[e._v("regularizer: Union[allennlp.nn.regularizers.regularizer_applicator.RegularizerApplicator, NoneType] = None")]),o("span",[e._v(",")]),e._v("\n    "),o("span",[e._v("accuracy: Union[allennlp.training.metrics.categorical_accuracy.CategoricalAccuracy, NoneType] = None")]),o("span",[e._v(",")]),e._v("\n"),o("span",[e._v(")")]),e._v("\n    ")])])])]),e._v(" "),o("dd",[o("div",{staticClass:"desc"},[o("p",[e._v("This "),o("code",[e._v("Model")]),e._v(" is a version of AllenNLPs implementation of the BiMPM model described in\n"),o("code",[e._v("Bilateral Multi-Perspective Matching for Natural Language Sentences <https://arxiv.org/abs/1702.03814>")]),e._v("_\nby Zhiguo Wang et al., 2017.")]),e._v(" "),o("p",[e._v("This version adds the feature of being compatible with multiple inputs for the two records.\nThe matching will be done for all possible combinations between the two records, that is:\n(r1_1, r2_1), (r1_1, r2_2), …, (r1_2, r2_1), (r1_2, r2_2), …")]),e._v(" "),o("p",[e._v("This version also allows you to apply only one encoder, and to leave out the backward matching.")]),e._v(" "),o("h2",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),o("dl",[o("dt",[o("strong",[o("code",[e._v("vocab")])]),e._v(" : "),o("code",[e._v("Vocabulary")])]),e._v(" "),o("dd",[e._v(" ")]),e._v(" "),o("dt",[o("strong",[o("code",[e._v("text_field_embedder")])]),e._v(" : "),o("code",[e._v("TextFieldEmbedder")])]),e._v(" "),o("dd",[e._v("Used to embed the "),o("code",[e._v("record1")]),e._v(" and "),o("code",[e._v("record2")]),e._v(" "),o("code",[e._v("TextFields")]),e._v(" we get as input to the\nmodel.")]),e._v(" "),o("dt",[o("strong",[o("code",[e._v("matcher_word")])]),e._v(" : "),o("code",[e._v("BiMpmMatching")])]),e._v(" "),o("dd",[e._v("BiMPM matching on the output of word embeddings of record1 and record2.")]),e._v(" "),o("dt",[o("strong",[o("code",[e._v("encoder")])]),e._v(" : "),o("code",[e._v("Seq2SeqEncoder")])]),e._v(" "),o("dd",[e._v("Encoder layer for record1 and record2")]),e._v(" "),o("dt",[o("strong",[o("code",[e._v("matcher_forward")])]),e._v(" : "),o("code",[e._v("BiMPMMatching")])]),e._v(" "),o("dd",[e._v("BiMPM matching for the forward output of the encoder layer")]),e._v(" "),o("dt",[o("strong",[o("code",[e._v("aggregator")])]),e._v(" : "),o("code",[e._v("Seq2VecEncoder")])]),e._v(" "),o("dd",[e._v("Aggregator of all BiMPM matching vectors")]),e._v(" "),o("dt",[o("strong",[o("code",[e._v("classifier_feedforward")])]),e._v(" : "),o("code",[e._v("FeedForward")])]),e._v(" "),o("dd",[e._v("Fully connected layers for classification.\nA linear output layer with the number of labels at the end will be added automatically!!!")]),e._v(" "),o("dt",[o("strong",[o("code",[e._v("matcher_backward")])]),e._v(" : "),o("code",[e._v("BiMPMMatching")]),e._v(", optional")]),e._v(" "),o("dd",[e._v("BiMPM matching for the backward output of the encoder layer")]),e._v(" "),o("dt",[o("strong",[o("code",[e._v("encoder2")])]),e._v(" : "),o("code",[e._v("Seq2SeqEncoder")]),e._v(", optional")]),e._v(" "),o("dd",[e._v("Encoder layer for encoded record1 and encoded record2")]),e._v(" "),o("dt",[o("strong",[o("code",[e._v("matcher2_forward")])]),e._v(" : "),o("code",[e._v("BiMPMMatching")]),e._v(", optional")]),e._v(" "),o("dd",[e._v("BiMPM matching for the forward output of the second encoder layer")]),e._v(" "),o("dt",[o("strong",[o("code",[e._v("matcher2_backward")])]),e._v(" : "),o("code",[e._v("BiMPMMatching")]),e._v(", optional")]),e._v(" "),o("dd",[e._v("BiMPM matching for the backward output of the second encoder layer")]),e._v(" "),o("dt",[o("strong",[o("code",[e._v("dropout")])]),e._v(" : "),o("code",[e._v("float")]),e._v(", optional "),o("code",[e._v("(default=0.1)")])]),e._v(" "),o("dd",[e._v("Dropout percentage to use.")]),e._v(" "),o("dt",[o("strong",[o("code",[e._v("multifield")])]),e._v(" : "),o("code",[e._v("bool")]),e._v(", optional "),o("code",[e._v("(default=False)")])]),e._v(" "),o("dd",[e._v("Are there multiple inputs for each record, that is do the inputs come from "),o("code",[e._v("ListField")]),e._v("s?")]),e._v(" "),o("dt",[o("strong",[o("code",[e._v("initializer")])]),e._v(" : "),o("code",[e._v("InitializerApplicator")]),e._v(", optional "),o("code",[e._v("(default=``InitializerApplicator()``)")])]),e._v(" "),o("dd",[e._v("If provided, will be used to initialize the model parameters.")]),e._v(" "),o("dt",[o("strong",[o("code",[e._v("regularizer")])]),e._v(" : "),o("code",[e._v("RegularizerApplicator")]),e._v(", optional "),o("code",[e._v("(default=``None``)")])]),e._v(" "),o("dd",[e._v("If provided, will be used to calculate the regularization penalty during training.")]),e._v(" "),o("dt",[o("strong",[o("code",[e._v("accuracy")])])]),e._v(" "),o("dd",[e._v("The accuracy you want to use. By default, we choose a categorical top-1 accuracy.")])]),e._v(" "),o("p",[e._v("Initializes internal Module state, shared by both nn.Module and ScriptModule.")])]),e._v(" "),o("h3",[e._v("Ancestors")]),e._v(" "),o("ul",{staticClass:"hlist"},[o("li",[o("a",{attrs:{title:"biome.text.models.mixins.BiomeClassifierMixin",href:"mixins.html#biome.text.models.mixins.BiomeClassifierMixin"}},[e._v("BiomeClassifierMixin")])]),e._v(" "),o("li",[e._v("allennlp.models.model.Model")]),e._v(" "),o("li",[e._v("torch.nn.modules.module.Module")]),e._v(" "),o("li",[e._v("allennlp.common.registrable.Registrable")]),e._v(" "),o("li",[e._v("allennlp.common.from_params.FromParams")])]),e._v(" "),o("dl",[o("h3",{attrs:{id:"biome.text.models.multifield_bimpm.MultifieldBiMpm.forward"}},[e._v("forward "),o("Badge",{attrs:{text:"Method"}})],1),e._v(" "),o("dt",[o("div",{staticClass:"language-python extra-class"},[o("pre",{staticClass:"language-python"},[o("code",[e._v("\n"),o("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),o("span",{staticClass:"ident"},[e._v("forward")]),e._v(" ("),e._v("\n   self,\n   record1: Dict[str, torch.LongTensor],\n   record2: Dict[str, torch.LongTensor],\n   label: torch.LongTensor = None,\n)  -> Dict[str, torch.Tensor]\n")]),e._v("\n        ")])])]),e._v(" "),o("dd",[o("div",{staticClass:"desc"},[o("h2",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),o("dl",[o("dt",[o("strong",[o("code",[e._v("record1")])])]),e._v(" "),o("dd",[e._v("The first input tokens.\nThe dictionary is the output of a "),o("code",[e._v("TextField.as_array()")]),e._v(". It gives names to the tensors created by\nthe "),o("code",[e._v("TokenIndexer")]),e._v("s.\nIn its most basic form, using a "),o("code",[e._v("SingleIdTokenIndexer")]),e._v(", the dictionary is composed of:\n"),o("code",[e._v('{"tokens": Tensor(batch_size, num_tokens)}')]),e._v(".\nThe keys of the dictionary are defined in the "),o("code",[e._v("model.yml")]),e._v(" input.\nThe dictionary is designed to be passed on directly to a "),o("code",[e._v("TextFieldEmbedder")]),e._v(", that has a\n"),o("code",[e._v("TokenEmbedder")]),e._v(" for each key in the dictionary (except you set "),o("code",[e._v("allow_unmatched_keys")]),e._v(" in the\n"),o("code",[e._v("TextFieldEmbedder")]),e._v(" to False) and knows how to combine different word/character representations into a\nsingle vector per token in your input.")]),e._v(" "),o("dt",[o("strong",[o("code",[e._v("record2")])])]),e._v(" "),o("dd",[e._v("The second input tokens.")]),e._v(" "),o("dt",[o("strong",[o("code",[e._v("label")])]),e._v(" : "),o("code",[e._v("torch.LongTensor")]),e._v(", optional "),o("code",[e._v("(default = None)")])]),e._v(" "),o("dd",[e._v("A torch tensor representing the sequence of integer gold class label of shape\n"),o("code",[e._v("(batch_size, num_classes)")]),e._v(".")])]),e._v(" "),o("h2",{attrs:{id:"returns"}},[e._v("Returns")]),e._v(" "),o("dl",[o("dt",[o("code",[e._v("An output dictionary consisting of:")])]),e._v(" "),o("dd",[e._v(" ")]),e._v(" "),o("dt",[o("strong",[o("code",[e._v("logits")])]),e._v(" : "),o("code",[e._v("torch.FloatTensor")])]),e._v(" "),o("dd",[e._v("A tensor of shape "),o("code",[e._v("(batch_size, num_tokens, tag_vocab_size)")]),e._v(" representing\nunnormalised log probabilities of the tag classes.")]),e._v(" "),o("dt",[o("strong",[o("code",[e._v("class_probabilities")])]),e._v(" : "),o("code",[e._v("torch.FloatTensor")])]),e._v(" "),o("dd",[e._v("A tensor of shape "),o("code",[e._v("(batch_size, num_tokens, tag_vocab_size)")]),e._v(" representing\na distribution of the tag classes per word.")]),e._v(" "),o("dt",[o("strong",[o("code",[e._v("loss")])]),e._v(" : "),o("code",[e._v("torch.FloatTensor")]),e._v(", optional")]),e._v(" "),o("dd",[e._v("A scalar loss to be optimised.")])])])])]),e._v(" "),o("h3",[e._v("Inherited members")]),e._v(" "),o("ul",{staticClass:"hlist"},[o("li",[o("code",[o("b",[o("a",{attrs:{title:"biome.text.models.mixins.BiomeClassifierMixin",href:"mixins.html#biome.text.models.mixins.BiomeClassifierMixin"}},[e._v("BiomeClassifierMixin")])])]),e._v(":\n"),o("ul",{staticClass:"hlist"},[o("li",[o("code",[o("a",{attrs:{title:"biome.text.models.mixins.BiomeClassifierMixin.decode",href:"mixins.html#biome.text.models.mixins.BiomeClassifierMixin.decode"}},[e._v("decode")])])]),e._v(" "),o("li",[o("code",[o("a",{attrs:{title:"biome.text.models.mixins.BiomeClassifierMixin.get_metrics",href:"mixins.html#biome.text.models.mixins.BiomeClassifierMixin.get_metrics"}},[e._v("get_metrics")])])])])])])])])])}),[],!1,null,null,null);t.default=a.exports}}]);