(window.webpackJsonp=window.webpackJsonp||[]).push([[5],{331:function(t,a,s){t.exports=s.p+"assets/img/text_classifier_explore_screenshot.b068fd60.png"},373:function(t,a,s){t.exports=s.p+"assets/img/hpo_tensorboard.99b04f5c.png"},374:function(t,a,s){t.exports=s.p+"assets/img/analysis_df.33ad9b08.png"},440:function(t,a,s){"use strict";s.r(a);var n=s(26),e=Object(n.a)({},(function(){var t=this,a=t.$createElement,n=t._self._c||a;return n("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[n("h1",{attrs:{id:"hyperparameter-optimization-with-ray-tune"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#hyperparameter-optimization-with-ray-tune"}},[t._v("#")]),t._v(" Hyperparameter optimization with Ray Tune")]),t._v(" "),n("p",[n("a",{attrs:{target:"_blank",href:"https://www.recogn.ai/biome-text/documentation/tutorials/3-Hyperparameter_optimization_with_Ray_Tune.html"}},[n("img",{staticClass:"icon",attrs:{src:"https://www.recogn.ai/biome-text/assets/img/biome-isotype.svg",width:"24"}})]),t._v(" "),n("a",{attrs:{href:"https://www.recogn.ai/biome-text/documentation/tutorials/3-Hyperparameter_optimization_with_Ray_Tune.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("View on recogn.ai"),n("OutboundLink")],1)]),t._v(" "),n("p",[n("a",{attrs:{target:"_blank",href:"https://colab.research.google.com/github/recognai/biome-text/blob/master/docs/docs/documentation/tutorials/3-Hyperparameter_optimization_with_Ray_Tune.ipynb"}},[n("img",{staticClass:"icon",attrs:{src:"https://www.tensorflow.org/images/colab_logo_32px.png",width:"24"}})]),t._v(" "),n("a",{attrs:{href:"https://colab.research.google.com/github/recognai/biome-text/blob/master/docs/docs/documentation/tutorials/3-Hyperparameter_optimization_with_Ray_Tune.ipynb",target:"_blank",rel:"noopener noreferrer"}},[t._v("Run in Google Colab"),n("OutboundLink")],1)]),t._v(" "),n("p",[n("a",{attrs:{target:"_blank",href:"https://github.com/recognai/biome-text/blob/master/docs/docs/documentation/tutorials/3-Hyperparameter_optimization_with_Ray_Tune.ipynb"}},[n("img",{staticClass:"icon",attrs:{src:"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png",width:"24"}})]),t._v(" "),n("a",{attrs:{href:"https://github.com/recognai/biome-text/blob/master/docs/docs/documentation/tutorials/3-Hyperparameter_optimization_with_Ray_Tune.ipynb",target:"_blank",rel:"noopener noreferrer"}},[t._v("View source on GitHub"),n("OutboundLink")],1)]),t._v(" "),n("p",[t._v("In this tutorial we will optimize the hyperparameters of the short-text classifier from "),n("a",{attrs:{href:"https://www.recogn.ai/biome-text/documentation/tutorials/1-Training_a_text_classifier.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("this tutorial"),n("OutboundLink")],1),t._v(".\nWe recommend to have a look at it first before going through the following tutorial.\nFor the Hyper-Parameter Optimization (HPO) we rely on the awesome "),n("a",{attrs:{href:"https://docs.ray.io/en/latest/tune.html#tune-index",target:"_blank",rel:"noopener noreferrer"}},[t._v("Ray Tune library"),n("OutboundLink")],1),t._v(" that is "),n("strong",[t._v("not")]),t._v(" a dependency of "),n("em",[t._v("biome.text")]),t._v(" and has to be installed additionally.")]),t._v(" "),n("p",[t._v("For a short introduction to HPO with Ray Tune you can have a look at this nice "),n("a",{attrs:{href:"https://www.youtube.com/watch?v=VX7HvEoMrsA",target:"_blank",rel:"noopener noreferrer"}},[t._v("talk"),n("OutboundLink")],1),t._v(" by Richard Liaw.\nWe will follow his terminology and use the term "),n("em",[t._v("trial")]),t._v(" to refer to a training run of one set of hyperparameters.")]),t._v(" "),n("p",[t._v("When running this tutorial in Google Colab, make sure to install "),n("em",[t._v("biome.text")]),t._v(" and "),n("em",[t._v("ray tune")]),t._v(" first:")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("!pip install "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("U git"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v("https"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("//")]),t._v("github"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("com"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("recognai"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("biome"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("text"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("git ray"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("tune"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),n("p",[t._v("Ignore warnings and don't forget to restart your runtime afterwards ("),n("em",[t._v("Runtime -> Restart runtime")]),t._v(").")]),t._v(" "),n("div",{staticClass:"custom-block tip"},[n("p",{staticClass:"custom-block-title"},[t._v("Note")]),t._v(" "),n("p",[t._v("In this tutorial we will use a GPU by default.\nSo when running this tutorial in Google Colab, make sure that you request one ("),n("em",[t._v("Edit -> Notebook settings")]),t._v(").")])]),t._v(" "),n("h2",{attrs:{id:"download-the-data-and-create-the-vocabulary"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#download-the-data-and-create-the-vocabulary"}},[t._v("#")]),t._v(" Download the data and create the vocabulary")]),t._v(" "),n("p",[t._v("As a first step we will download the training and validation data to our local machine.\nThis will save us some time in the long run, since we will perform the hyperparameter search on our local machine and access the data frequently.")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("!curl "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("O https"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("//")]),t._v("biome"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("tutorials"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("data"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("s3"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("eu"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("west"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.")]),t._v("amazonaws"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("com"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("text_classifier"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("business"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cat"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("csv\n")])])]),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("!curl "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("O https"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("//")]),t._v("biome"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("tutorials"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("data"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("s3"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("eu"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("west"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.")]),t._v("amazonaws"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("com"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("text_classifier"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("business"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cat"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("valid"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("csv\n")])])]),n("p",[t._v("We will store the absolute path of the data to use them later on when creating our "),n("code",[t._v("DataSource")]),t._v("s.")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" os\n")])])]),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("train_path "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" os"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("abspath"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"business.cat.train.csv"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nvalid_path "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" os"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("abspath"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"business.cat.valid.csv"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("h3",{attrs:{id:"reuse-the-vocabulary"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#reuse-the-vocabulary"}},[t._v("#")]),t._v(" Reuse the vocabulary")]),t._v(" "),n("p",[t._v("In order to be more efficient and speed things up, we will create the vocabulary beforehand and reuse it in every trial.\nFor this we have to create a "),n("code",[t._v("Pipeline")]),t._v(" first, create the vocabulary from our "),n("code",[t._v("DataSource")]),t._v(" and save it to a folder.")]),t._v(" "),n("p",[t._v("Let's start with defining the configuration of our pipeline (for details see the "),n("a",{attrs:{href:"https://www.recogn.ai/biome-text/documentation/tutorials/1-Training_a_text_classifier.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("base tutorial"),n("OutboundLink")],1),t._v("):")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" biome"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("data "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" DataSource\n")])])]),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("labels "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" DataSource"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train_path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_dataframe"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("label"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("compute"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("pipeline_dict "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"name"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"german_business_names"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"tokenizer"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"text_cleaning"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"rules"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"strip_spaces"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"features"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"word"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"embedding_dim"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("64")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"lowercase_tokens"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"char"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"embedding_dim"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("32")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"lowercase_characters"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"encoder"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"gru"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"num_layers"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"hidden_size"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("32")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"bidirectional"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"dropout"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"head"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"TextClassification"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"labels"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("labels"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("value_counts"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"pooler"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"gru"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"num_layers"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"hidden_size"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("32")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"bidirectional"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"feedforward"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"num_layers"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"hidden_dims"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("32")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"activations"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"relu"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"dropout"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("       \n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),n("p",[t._v("We will use this configuration dictionary to create our pipeline:")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" biome"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Pipeline\n")])])]),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("pl "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Pipeline"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_config"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pipeline_dict"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("Next, we have to define the vocabulary configuration and create our vocabulary.")]),t._v(" "),n("div",{staticClass:"custom-block tip"},[n("p",{staticClass:"custom-block-title"},[t._v("Note")]),t._v(" "),n("p",[t._v("If you want to optimize the vocabulary configuration in the hyperparameter search (for example, the "),n("code",[t._v("min_count")]),t._v(" argument), you have to move the vocabulary creation to the "),n("code",[t._v("trainable")]),t._v(" function below. That is, in each trial the vocabulary will be created anew.")])]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" biome"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("configuration "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" VocabularyConfiguration"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" WordFeatures\n")])])]),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("vocab_config "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" VocabularyConfiguration"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sources"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("DataSource"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train_path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" min_count"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("WordFeatures"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("namespace"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("pl"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("create_vocabulary"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("vocab_config"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("To be able to reuse the vocabulary in each trial, we have to save it to a folder and store its absolute path:")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("vocab_absolute_path "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" os"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("abspath"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"./vocabulary"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("pl"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("save_vocabulary"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("vocab_absolute_path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("h2",{attrs:{id:"implementing-the-callback-for-early-stopping"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#implementing-the-callback-for-early-stopping"}},[t._v("#")]),t._v(" Implementing the callback for early stopping")]),t._v(" "),n("p",[t._v("In this tutorial we will use a trial scheduler that adaptively allocates resources to promising hyperparameter configurations by terminating less promising candidates early.\nThe early stopping mechanism requires the reporting of some metric during a trial.\nFor this we use a "),n("code",[t._v("BaseTrainLogger")]),t._v(" that defines a method "),n("code",[t._v("log_epoch_metrics()")]),t._v(" which is executed after each epoch, and pass it on to the "),n("code",[t._v("Pipeline.train()")]),t._v(" method.")]),t._v(" "),n("p",[t._v("Our "),n("code",[t._v("TuneReport")]),t._v(" class simply reports some metrics back to tune, which in turn are used to define promising trials during the hyperparameter search.")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" biome"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loggers "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" BaseTrainLogger\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" ray "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" tune\n")])])]),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("TuneReport")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("BaseTrainLogger"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("log_epoch_metrics")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" epoch"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" metrics"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        tune"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("report"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n            validation_loss"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("metrics"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"validation_loss"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n            validation_accuracy"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("metrics"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"validation_accuracy"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("tune_report "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TuneReport"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("h2",{attrs:{id:"defining-the-training-loop"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#defining-the-training-loop"}},[t._v("#")]),t._v(" Defining the training loop")]),t._v(" "),n("p",[t._v("For the HPO with "),n("em",[t._v("biome.text")]),t._v(" we will use the "),n("a",{attrs:{href:"https://docs.ray.io/en/latest/tune/api_docs/trainable.html#tune-function-api",target:"_blank",rel:"noopener noreferrer"}},[t._v("function-based Trainable API"),n("OutboundLink")],1),t._v(" of Ray Tune.\nTherefore, we have to define a "),n("code",[t._v("trainable")]),t._v(" function that takes as input a configuration dictionary and executes a training run.")]),t._v(" "),n("p",[t._v("We will use the configuration dictionary to create a "),n("code",[t._v("Pipeline")]),t._v(" and a "),n("code",[t._v("TrainerConfiguration")]),t._v(" in order to optimize the parameters of our architecture and the learning rate, respectively.\nIn the "),n("code",[t._v("Pipeline.train()")]),t._v(" method we will add our "),n("code",[t._v("tune_report")]),t._v(" instance to the epoch callbacks, and completely silence the output of the training by setting "),n("code",[t._v("quiet=True")]),t._v(".\nThis avoids cluttering the output of the hyperparameter search and makes it easier to follow the progress.")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" biome"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("configuration "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" TrainerConfiguration\n")])])]),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("trainable")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("config"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    pl "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Pipeline"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_config"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("config"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"pipeline"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" vocab_absolute_path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    trainer_config "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TrainerConfiguration"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("**")]),t._v("config"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"trainer"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    \n    train_ds "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" DataSource"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train_path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    valid_ds "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" DataSource"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("valid_path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    \n    pl"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n        output"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"output"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        training"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("train_ds"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        validation"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("valid_ds"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        trainer"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("trainer_config"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        loggers"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("tune_report"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        quiet"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("h2",{attrs:{id:"random-search-with-a-trial-scheduler"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#random-search-with-a-trial-scheduler"}},[t._v("#")]),t._v(" Random search with a trial scheduler")]),t._v(" "),n("p",[t._v("To perform a random hyperparameter search (as well as a grid search) we simply have to replace the parameters we want to optimize with methods from the "),n("a",{attrs:{href:"https://docs.ray.io/en/latest/tune/api_docs/grid_random.html#random-distributions-api",target:"_blank",rel:"noopener noreferrer"}},[t._v("Random Distributions API"),n("OutboundLink")],1),t._v(" and the "),n("a",{attrs:{href:"https://docs.ray.io/en/latest/tune/api_docs/grid_random.html#grid-search-api",target:"_blank",rel:"noopener noreferrer"}},[t._v("Grid Search API"),n("OutboundLink")],1),t._v(", respectively.\nFor a complete description of both APIs and how they interplay with each other, see the corresponding section in the "),n("a",{attrs:{href:"https://docs.ray.io/en/latest/tune/api_docs/grid_random.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("Ray Tune docs"),n("OutboundLink")],1),t._v(".")]),t._v(" "),n("p",[t._v("In our case we will tune 9 parameters:")]),t._v(" "),n("ul",[n("li",[t._v("the output dimensions of our "),n("code",[t._v("word")]),t._v(" and "),n("code",[t._v("char")]),t._v(" features")]),t._v(" "),n("li",[t._v("the dropout of our "),n("code",[t._v("char")]),t._v(" feature")]),t._v(" "),n("li",[t._v("the architecture of our pooler ("),n("em",[t._v("GRU")]),t._v(" versus "),n("em",[t._v("LSTM")]),t._v(")")]),t._v(" "),n("li",[t._v("number of layers and hidden size of our pooler, as well as if it should be bidirectional")]),t._v(" "),n("li",[t._v("hidden dimension of our feed forward network")]),t._v(" "),n("li",[t._v("and the learning rate")])]),t._v(" "),n("p",[t._v("For most of the parameters we will provide discrete values from which Tune will sample randomly, while for the dropout and learning rate we will provide a continuous linear and logarithmic range, respectively.\nSince we want to directly compare the outcome of the optimization with the base configuration of the "),n("a",{attrs:{href:"https://www.recogn.ai/biome-text/documentation/tutorials/1-Training_a_text_classifier.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("underlying tutorial"),n("OutboundLink")],1),t._v(", we will fix the number of epochs to 3.")]),t._v(" "),n("p",[t._v("Not all of the parameters above are worth tuning, but we want to stress the flexibility that "),n("em",[t._v("Ray Tune")]),t._v(" and "),n("em",[t._v("biome.text")]),t._v(" offers you.")]),t._v(" "),n("div",{staticClass:"custom-block tip"},[n("p",{staticClass:"custom-block-title"},[t._v("Tip")]),t._v(" "),n("p",[t._v('Keep in mind that the learning rate "'),n("em",[t._v("is often the single most important hyper-parameter and one should always make sure that it has been tuned (up to approximately a factor of 2). ... If there is only time to optimize one hyper-parameter and one uses stochastic gradient descent, then this is the hyper-parameter that is worth tuning.")]),t._v('" ('),n("a",{attrs:{href:"https://arxiv.org/abs/1206.5533",target:"_blank",rel:"noopener noreferrer"}},[t._v("Yoshua Bengio"),n("OutboundLink")],1),t._v(").")])]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("configs "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"pipeline"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"name"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"german_business_names"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"tokenizer"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"text_cleaning"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"rules"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"strip_spaces"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"features"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"word"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"embedding_dim"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" tune"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("choice"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("32")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("64")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"lowercase_tokens"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"char"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"embedding_dim"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("32")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"lowercase_characters"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"encoder"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n                    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"gru"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"num_layers"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"hidden_size"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" tune"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("choice"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("32")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("64")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"bidirectional"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"dropout"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" tune"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("uniform"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.5")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"head"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"TextClassification"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"labels"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("labels"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("value_counts"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("index"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"pooler"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" tune"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("choice"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"gru"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"lstm"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"num_layers"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" tune"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("choice"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"hidden_size"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" tune"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("choice"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("32")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("64")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"bidirectional"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" tune"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("choice"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"feedforward"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"num_layers"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"hidden_dims"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("tune"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("choice"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("32")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("64")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"activations"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"relu"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"dropout"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("       \n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"trainer"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"optimizer"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"adam"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"lr"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" tune"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loguniform"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.001")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.01")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"num_epochs"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"cuda_device"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),n("div",{staticClass:"custom-block tip"},[n("p",{staticClass:"custom-block-title"},[t._v("Note")]),t._v(" "),n("p",[t._v("By default we will use a GPU.\nIf you do not have one available, just comment out the line "),n("code",[t._v('"cuda_device": 0')]),t._v(" in the trainer section of the dictionary above.")])]),t._v(" "),n("p",[t._v("In this tutorial we will perform a random search together with the "),n("a",{attrs:{href:"https://blog.ml.cmu.edu/2018/12/12/massively-parallel-hyperparameter-optimization/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Asynchronous Successive Halving Algorithm (ASHA)"),n("OutboundLink")],1),t._v(" to schedule our trials.\nThe Ray Tune developers advocate this scheduler as a good starting point for its aggressive termination of low-performing trials.")]),t._v(" "),n("p",[t._v("To create an instance of the "),n("code",[t._v("ASHAScheduler")]),t._v(" we have to specify the decisive metric for terminating low-performing trials and the mode of this metric (is the objective to "),n("em",[t._v("minimize")]),t._v(" the metric, "),n("code",[t._v("min")]),t._v(", or to "),n("em",[t._v("maximize")]),t._v(" it, "),n("code",[t._v("max")]),t._v(").\nFor a complete description of the configurations, see the "),n("a",{attrs:{href:"https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#asha-tune-schedulers-ashascheduler",target:"_blank",rel:"noopener noreferrer"}},[t._v("ASHAScheduler docs"),n("OutboundLink")],1),t._v(".")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" ray"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tune"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("schedulers "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" ASHAScheduler\n")])])]),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("asha "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ASHAScheduler"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("metric"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"validation_loss"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" mode"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"min"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("h2",{attrs:{id:"following-the-progress-with-tensorboard-optional"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#following-the-progress-with-tensorboard-optional"}},[t._v("#")]),t._v(" Following the progress with tensorboard (optional)")]),t._v(" "),n("p",[t._v("Ray Tune automatically logs its results with "),n("a",{attrs:{href:"https://www.tensorflow.org/tensorboard/",target:"_blank",rel:"noopener noreferrer"}},[t._v("TensorBoard"),n("OutboundLink")],1),t._v(".\nWe can take advantage of this and launch a TensorBoard instance before starting the hyperparameter search to follow its progress.")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v("load_ext tensorboard\n")])])]),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v("tensorboard "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("logdir "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("tune"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("trainable\n")])])]),n("p",[n("img",{attrs:{src:s(373),alt:"Screenshot of TensorBoard with Ray Tune"}}),t._v(" "),n("em",[t._v("Screenshot of TensorBoard")])]),t._v(" "),n("h2",{attrs:{id:"start-the-hyperparameter-search"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#start-the-hyperparameter-search"}},[t._v("#")]),t._v(" Start the hyperparameter search")]),t._v(" "),n("p",[t._v("Now we have everything ready to start our hyperparameter search with the "),n("code",[t._v("tune.run()")]),t._v(" method.")]),t._v(" "),n("p",[t._v("The number of trials our search will go through depends on the "),n("code",[t._v("num_samples")]),t._v(" parameter.\nIn our case, a random search, it equals the number of trials, whereas in the case of a grid search the total number of trials is "),n("code",[t._v("num_samples")]),t._v(" times the grid configurations (see the "),n("a",{attrs:{href:"https://docs.ray.io/en/latest/tune/api_docs/grid_random.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("Tune docs"),n("OutboundLink")],1),t._v(" for illustrative examples).")]),t._v(" "),n("p",[t._v("The number of parallel running trials depends on your "),n("code",[t._v("resources_per_trial")]),t._v(" configuration and your local resources.\nThe default value is "),n("code",[t._v('{"cpu": 1, "gpu": 0}')]),t._v(" and results, for example, in 8 parallel running trials on a machine with 8 CPUs.\nYou can also use fractional values. To share a GPU between 2 trials, for example, pass on "),n("code",[t._v('{"gpu": 0.5}')]),t._v(".")]),t._v(" "),n("p",[t._v("The "),n("code",[t._v("local_dir")]),t._v(" parameter defines the output directory of the HPO results and will also contain the training results of each trial (that is the model weights and metrics).")]),t._v(" "),n("div",{staticClass:"custom-block tip"},[n("p",{staticClass:"custom-block-title"},[t._v("Note")]),t._v(" "),n("p",[t._v("Keep in mind: to run your HPO on GPUs, you have to specify them in the "),n("code",[t._v("TrainerConfiguration")]),t._v(" in the "),n("code",[t._v("trainable")]),t._v(" function, as well as in the "),n("code",[t._v("resources_per_trial")]),t._v(" parameter when calling "),n("code",[t._v("tune.run()")]),t._v(".\nIf you do not want to use a GPU, just set the value to 0 "),n("code",[t._v('{"cpu": 1, "gpu": 0}')]),t._v(".")])]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("analysis "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tune"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("run"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    trainable"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n    config"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("configs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n    scheduler"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("asha"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n    num_samples"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("50")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n    resources_per_trial"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"cpu"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"gpu"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.5")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    local_dir"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"./tune"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("h2",{attrs:{id:"checking-the-results"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#checking-the-results"}},[t._v("#")]),t._v(" Checking the results")]),t._v(" "),n("p",[t._v("The "),n("em",[t._v("analysis")]),t._v(" object returned by "),n("code",[t._v("tune.run()")]),t._v(" can be accessed through a "),n("em",[t._v("pandas DataFrame")]),t._v(".")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("analysis"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dataframe"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sort_values"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"validation_loss"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[n("img",{attrs:{src:s(374),alt:"Screenshot of the analysis dataframe"}}),t._v(" "),n("em",[t._v("Screenshot of the analysis dataframe")])]),t._v(" "),n("p",[t._v("Event though with 50 trials we visit just a small space of our possible configurations, we should have achieved an accuracy of ~0.94, an increase of roughly 3 points compared to the original configuration of the "),n("a",{attrs:{href:"https://www.recogn.ai/biome-text/documentation/tutorials/1-Training_a_text_classifier.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("base tutorial"),n("OutboundLink")],1),t._v(".")]),t._v(" "),n("p",[t._v("In a real-life example, though, you probably should increase the number of epochs, since the validation loss in general seems to be decreasing further.")]),t._v(" "),n("p",[t._v("A next step could be to fix some of the tuned parameters to the preferred value, and tune other parameters further or limit their value space.")]),t._v(" "),n("div",{staticClass:"custom-block tip"},[n("p",{staticClass:"custom-block-title"},[t._v("Tip")]),t._v(" "),n("p",[t._v("To obtain insights about the importance and tendencies of each hyperparameter for the model, we recommend using TensorBoard's "),n("em",[t._v("HPARAM")]),t._v(" section and follow Richard Liaw's suggestions at the end of his "),n("a",{attrs:{href:"https://www.youtube.com/watch?v=VX7HvEoMrsA",target:"_blank",rel:"noopener noreferrer"}},[t._v("talk"),n("OutboundLink")],1),t._v(".")])]),t._v(" "),n("h3",{attrs:{id:"evaluate-the-best-performing-model"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#evaluate-the-best-performing-model"}},[t._v("#")]),t._v(" Evaluate the best performing model")]),t._v(" "),n("p",[t._v("The "),n("em",[t._v("analysis")]),t._v(" object also provides some convenient methods to obtain the best performing configuration, as well as the "),n("code",[t._v("logdir")]),t._v(" where the results of the trial are saved.")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("best_config "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" analysis"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get_best_config"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("metric"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"validation_loss"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" mode"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"min"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nbest_logdir "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" analysis"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get_best_logdir"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("metric"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"validation_loss"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" mode"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"min"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("We can use the "),n("code",[t._v("best_logdir")]),t._v(" to create a pipeline with the best performing model and start making predictions.")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("best_model "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" os"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("join"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("best_logdir"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"output"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"model.tar.gz"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\npl_trained "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Pipeline"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_pretrained"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("best_model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("pl_trained"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("text"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Autohaus Recognai"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("Or we can use "),n("em",[t._v("biome.text")]),t._v("'s explore UI to evaluate the performance of our model in more detail.")]),t._v(" "),n("div",{staticClass:"custom-block warning"},[n("p",{staticClass:"custom-block-title"},[t._v("Warning")]),t._v(" "),n("p",[t._v("For the UI to work you need a running "),n("a",{attrs:{href:"https://www.elastic.co/guide/en/elasticsearch/reference/current/install-elasticsearch.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("Elasticsearch"),n("OutboundLink")],1),t._v(" instance.\nWe recommend installing "),n("a",{attrs:{href:"https://www.elastic.co/guide/en/elasticsearch/reference/7.7/docker.html#docker-cli-run-dev-mode",target:"_blank",rel:"noopener noreferrer"}},[t._v("Elasticsearch with docker"),n("OutboundLink")],1),t._v(".")])]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("pl_trained"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("explore"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("DataSource"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("valid_path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" explain"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[n("img",{attrs:{src:s(331),alt:"Screenshot of the biome.text explore UI"}}),t._v(" "),n("em",[t._v("Screenshot of the biome.text explore UI")])]),t._v(" "),n("div",{staticClass:"custom-block tip"},[n("p",{staticClass:"custom-block-title"},[t._v("Note")]),t._v(" "),n("p",[t._v("For an unbiased evaluation of the model you should use a test dataset that was not used during the HPO!")])])])}),[],!1,null,null,null);a.default=e.exports}}]);