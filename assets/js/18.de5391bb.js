(window.webpackJsonp=window.webpackJsonp||[]).push([[18],{408:function(t,e,a){"use strict";a.r(e);var n=a(26),s=Object(n.a)({},(function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"biome-text-configuration"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#biome-text-configuration"}},[t._v("#")]),t._v(" biome.text.configuration "),a("Badge",{attrs:{text:"Module"}})],1),t._v(" "),a("div"),t._v(" "),a("div"),t._v(" "),a("pre",{staticClass:"title"},[a("h2",{attrs:{id:"featuresconfiguration"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#featuresconfiguration"}},[t._v("#")]),t._v(" FeaturesConfiguration "),a("Badge",{attrs:{text:"Class"}})],1),t._v("\n")]),t._v(" "),a("pre",{staticClass:"language-python"},[a("code",[t._v("\n"),a("span",{staticClass:"token keyword"},[t._v("class")]),t._v(" "),a("span",{staticClass:"ident"},[t._v("FeaturesConfiguration")]),t._v(" ("),t._v("\n    "),a("span",[t._v("words: Union[Dict[str, Any], NoneType] = None")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("chars: Union[Dict[str, Any], NoneType] = None")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("**extra_params")]),a("span",[t._v(",")]),t._v("\n"),a("span",[t._v(")")]),t._v("\n")]),t._v("\n")]),t._v(" "),a("p",[t._v("Creates a input featurizer configuration")]),t._v(" "),a("p",[t._v("This class will create a configuration for the features of the "),a("code",[t._v("Pipeline")]),t._v(".")]),t._v(" "),a("p",[t._v("Use this for defining the main features to be used by the model, namely word and character embeddings.")]),t._v(" "),a("p",[t._v(":::tip\nIf you do not pass "),a("code",[t._v("words")]),t._v(" and "),a("code",[t._v("chars")]),t._v(" your pipeline will be setup with default word features (embedding_dim=50).\n:::")]),t._v(" "),a("p",[t._v("Example:")]),t._v(" "),a("pre",[a("code",{staticClass:"python"},[t._v("words = {'embedding_dim': 100}\nchars = {'embedding_dim': 16, 'encoder': {'type': 'gru'}}\nconfig = FeaturesConfiguration(words,chars)\n")])]),t._v(" "),a("h2",{attrs:{id:"parameters"}},[t._v("Parameters")]),t._v(" "),a("dl",[a("dt",[a("strong",[a("code",[t._v("words")])]),t._v(" : "),a("code",[t._v("Dict[str, Any]")])]),t._v(" "),a("dd",[t._v(" ")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("chars")])]),t._v(" : "),a("code",[t._v("Dict[str, Any]")])]),t._v(" "),a("dd",[t._v(" ")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("extra_params")])])]),t._v(" "),a("dd",[t._v(" ")])]),t._v(" "),a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"ancestors"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#ancestors"}},[t._v("#")]),t._v(" Ancestors")]),t._v("\n")]),t._v(" "),a("ul",{staticClass:"hlist"},[a("li",[t._v("allennlp.common.from_params.FromParams")])]),t._v(" "),a("dl",[a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"from-params"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#from-params"}},[t._v("#")]),t._v(" from_params "),a("Badge",{attrs:{text:"Static method"}})],1),t._v("\n")]),t._v(" "),a("dt",[a("div",{staticClass:"language-python extra-class"},[a("pre",{staticClass:"language-python"},[a("code",[t._v("\n"),a("span",{staticClass:"token keyword"},[t._v("def")]),t._v(" "),a("span",{staticClass:"ident"},[t._v("from_params")]),t._v(" ("),t._v("\n  params: allennlp.common.params.Params,\n  **extras,\n)  -> "),a("a",{attrs:{title:"biome.text.configuration.FeaturesConfiguration",href:"#biome.text.configuration.FeaturesConfiguration"}},[t._v("FeaturesConfiguration")]),t._v("\n")]),t._v("\n")])])]),t._v(" "),a("dd",[a("p",[t._v("This is the automatic implementation of "),a("code",[t._v("from_params")]),t._v(". Any class that subclasses "),a("code",[t._v("FromParams")]),t._v("\n(or "),a("code",[t._v("Registrable")]),t._v(", which itself subclasses "),a("code",[t._v("FromParams")]),t._v(') gets this implementation for free.\nIf you want your class to be instantiated from params in the "obvious" way – pop off parameters\nand hand them to your constructor with the same names – this provides that functionality.')]),t._v(" "),a("p",[t._v("If you need more complex logic in your from "),a("code",[t._v("from_params")]),t._v(" method, you'll have to implement\nyour own method that overrides this one.")])])]),t._v(" "),a("dl",[a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"compile"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#compile"}},[t._v("#")]),t._v(" compile "),a("Badge",{attrs:{text:"Method"}})],1),t._v("\n")]),t._v(" "),a("dt",[a("div",{staticClass:"language-python extra-class"},[a("pre",{staticClass:"language-python"},[a("code",[t._v("\n"),a("span",{staticClass:"token keyword"},[t._v("def")]),t._v(" "),a("span",{staticClass:"ident"},[t._v("compile")]),t._v("("),a("span",[t._v("self) -> "),a("a",{attrs:{title:"biome.text.featurizer.InputFeaturizer",href:"featurizer.html#biome.text.featurizer.InputFeaturizer"}},[t._v("InputFeaturizer")])]),t._v("\n")]),t._v("\n")])])]),t._v(" "),a("dd",[a("p",[t._v("Creates a featurizer from the configuration object")]),t._v(" "),a("p",[t._v(":::tip")]),t._v(" "),a("p",[t._v("If you are creating configurations programmatically use this method to check that your config object contains\na valid configuration.")]),t._v(" "),a("p",[t._v(":::")]),t._v(" "),a("h2",{attrs:{id:"returns"}},[t._v("Returns")]),t._v(" "),a("p",[t._v("The configured "),a("code",[t._v("InputFeaturizer")])])])]),t._v(" "),a("div"),t._v(" "),a("pre",{staticClass:"title"},[a("h2",{attrs:{id:"tokenizerconfiguration"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#tokenizerconfiguration"}},[t._v("#")]),t._v(" TokenizerConfiguration "),a("Badge",{attrs:{text:"Class"}})],1),t._v("\n")]),t._v(" "),a("pre",{staticClass:"language-python"},[a("code",[t._v("\n"),a("span",{staticClass:"token keyword"},[t._v("class")]),t._v(" "),a("span",{staticClass:"ident"},[t._v("TokenizerConfiguration")]),t._v(" ("),t._v("\n    "),a("span",[t._v("lang: str = 'en'")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("skip_empty_tokens: bool = False")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("max_sequence_length: int = None")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("max_nr_of_sentences: int = None")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("text_cleaning: Union[Dict[str, Any], NoneType] = None")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("segment_sentences: Union[bool, Dict[str, Any]] = False")]),a("span",[t._v(",")]),t._v("\n"),a("span",[t._v(")")]),t._v("\n")]),t._v("\n")]),t._v(" "),a("p",[t._v("Creates a "),a("code",[t._v("Tokenizer")]),t._v(" configuration")]),t._v(" "),a("h2",{attrs:{id:"parameters"}},[t._v("Parameters")]),t._v(" "),a("dl",[a("dt",[a("strong",[a("code",[t._v("lang")])])]),t._v(" "),a("dd",[t._v(" ")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("skip_empty_tokens")])])]),t._v(" "),a("dd",[t._v(" ")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("max_sequence_length")])])]),t._v(" "),a("dd",[t._v(" ")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("max_nr_of_sentences")])])]),t._v(" "),a("dd",[t._v(" ")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("text_cleaning")])])]),t._v(" "),a("dd",[t._v(" ")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("segment_sentences")])])]),t._v(" "),a("dd",[t._v(" ")])]),t._v(" "),a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"ancestors-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#ancestors-2"}},[t._v("#")]),t._v(" Ancestors")]),t._v("\n")]),t._v(" "),a("ul",{staticClass:"hlist"},[a("li",[t._v("allennlp.common.from_params.FromParams")])]),t._v(" "),a("dl",[a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"compile-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#compile-2"}},[t._v("#")]),t._v(" compile "),a("Badge",{attrs:{text:"Method"}})],1),t._v("\n")]),t._v(" "),a("dt",[a("div",{staticClass:"language-python extra-class"},[a("pre",{staticClass:"language-python"},[a("code",[t._v("\n"),a("span",{staticClass:"token keyword"},[t._v("def")]),t._v(" "),a("span",{staticClass:"ident"},[t._v("compile")]),t._v("("),a("span",[t._v("self) -> "),a("a",{attrs:{title:"biome.text.tokenizer.Tokenizer",href:"tokenizer.html#biome.text.tokenizer.Tokenizer"}},[t._v("Tokenizer")])]),t._v("\n")]),t._v("\n")])])]),t._v(" "),a("dd",[a("p",[t._v("Build tokenizer object from its configuration")])])]),t._v(" "),a("div"),t._v(" "),a("pre",{staticClass:"title"},[a("h2",{attrs:{id:"pipelineconfiguration"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#pipelineconfiguration"}},[t._v("#")]),t._v(" PipelineConfiguration "),a("Badge",{attrs:{text:"Class"}})],1),t._v("\n")]),t._v(" "),a("pre",{staticClass:"language-python"},[a("code",[t._v("\n"),a("span",{staticClass:"token keyword"},[t._v("class")]),t._v(" "),a("span",{staticClass:"ident"},[t._v("PipelineConfiguration")]),t._v(" ("),t._v("\n    "),a("span",[t._v("name: str")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("features: "),a("a",{attrs:{title:"biome.text.configuration.FeaturesConfiguration",href:"#biome.text.configuration.FeaturesConfiguration"}},[t._v("FeaturesConfiguration")])]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("head: "),a("a",{attrs:{title:"biome.text.modules.heads.defs.TaskHeadSpec",href:"modules/heads/defs.html#biome.text.modules.heads.defs.TaskHeadSpec"}},[t._v("TaskHeadSpec")])]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("tokenizer: Union[biome.text.configuration.TokenizerConfiguration, NoneType] = None")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("encoder: Union[biome.text.modules.specs.allennlp_specs.Seq2SeqEncoderSpec, NoneType] = None")]),a("span",[t._v(",")]),t._v("\n"),a("span",[t._v(")")]),t._v("\n")]),t._v("\n")]),t._v(" "),a("p",[t._v('"Creates a '),a("code",[t._v("Pipeline")]),t._v(" configuration")]),t._v(" "),a("h2",{attrs:{id:"parameters"}},[t._v("Parameters")]),t._v(" "),a("dl",[a("dt",[a("strong",[a("code",[t._v("name")])]),t._v(" : "),a("code",[t._v("str")])]),t._v(" "),a("dd",[t._v("The "),a("code",[t._v("name")]),t._v(" for our pipeline")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("features")])]),t._v(" : "),a("code",[a("a",{attrs:{title:"biome.text.configuration.FeaturesConfiguration",href:"#biome.text.configuration.FeaturesConfiguration"}},[t._v("FeaturesConfiguration")])])]),t._v(" "),a("dd",[t._v("The input "),a("code",[t._v("features")]),t._v(" to be used by the model pipeline. We define this using a "),a("code",[a("a",{attrs:{title:"biome.text.configuration.FeaturesConfiguration",href:"#biome.text.configuration.FeaturesConfiguration"}},[t._v("FeaturesConfiguration")])]),t._v(" object.")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("head")])]),t._v(" : "),a("code",[t._v("TaskHeadSpec")])]),t._v(" "),a("dd",[t._v("The "),a("code",[t._v("head")]),t._v(" for the task, e.g., a LanguageModelling task, using a "),a("code",[t._v("TaskHeadSpec")]),t._v(" object.")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("tokenizer")])]),t._v(" : "),a("code",[a("a",{attrs:{title:"biome.text.configuration.TokenizerConfiguration",href:"#biome.text.configuration.TokenizerConfiguration"}},[t._v("TokenizerConfiguration")])]),t._v(", optional")]),t._v(" "),a("dd",[t._v("The "),a("code",[t._v("tokenizer")]),t._v(" defined with a "),a("code",[a("a",{attrs:{title:"biome.text.configuration.TokenizerConfiguration",href:"#biome.text.configuration.TokenizerConfiguration"}},[t._v("TokenizerConfiguration")])]),t._v(" object.")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("encoder")])]),t._v(" : "),a("code",[t._v("Seq2SeqEncoderSpec")])]),t._v(" "),a("dd",[t._v("The core text seq2seq "),a("code",[t._v("encoder")]),t._v(" of our model using a "),a("code",[t._v("Seq2SeqEncoderSpec")])])]),t._v(" "),a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"ancestors-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#ancestors-3"}},[t._v("#")]),t._v(" Ancestors")]),t._v("\n")]),t._v(" "),a("ul",{staticClass:"hlist"},[a("li",[t._v("allennlp.common.from_params.FromParams")])]),t._v(" "),a("dl",[a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"as-dict"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#as-dict"}},[t._v("#")]),t._v(" as_dict "),a("Badge",{attrs:{text:"Method"}})],1),t._v("\n")]),t._v(" "),a("dt",[a("div",{staticClass:"language-python extra-class"},[a("pre",{staticClass:"language-python"},[a("code",[t._v("\n"),a("span",{staticClass:"token keyword"},[t._v("def")]),t._v(" "),a("span",{staticClass:"ident"},[t._v("as_dict")]),t._v("("),a("span",[t._v("self) -> Dict[str, Any]")]),t._v("\n")]),t._v("\n")])])]),t._v(" "),a("dd")]),t._v(" "),a("div"),t._v(" "),a("pre",{staticClass:"title"},[a("h2",{attrs:{id:"trainerconfiguration"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#trainerconfiguration"}},[t._v("#")]),t._v(" TrainerConfiguration "),a("Badge",{attrs:{text:"Class"}})],1),t._v("\n")]),t._v(" "),a("pre",{staticClass:"language-python"},[a("code",[t._v("\n"),a("span",{staticClass:"token keyword"},[t._v("class")]),t._v(" "),a("span",{staticClass:"ident"},[t._v("TrainerConfiguration")]),t._v(" ("),t._v("\n    "),a("span",[t._v("optimizer: Dict[str, Any]")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("validation_metric: str = '-loss'")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("patience: Union[int, NoneType] = None")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("shuffle: bool = True")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("num_epochs: int = 20")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("cuda_device: int = -1")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("grad_norm: Union[float, NoneType] = None")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("grad_clipping: Union[float, NoneType] = None")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("learning_rate_scheduler: Union[Dict[str, Any], NoneType] = None")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("momentum_scheduler: Union[Dict[str, Any], NoneType] = None")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("moving_average: Union[Dict[str, Any], NoneType] = None")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("batch_size: Union[int, NoneType] = None")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("cache_instances: bool = True")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("in_memory_batches: int = 2")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("data_bucketing: bool = True")]),a("span",[t._v(",")]),t._v("\n"),a("span",[t._v(")")]),t._v("\n")]),t._v("\n")]),t._v(" "),a("p",[t._v("Creates a "),a("code",[a("a",{attrs:{title:"biome.text.configuration.TrainerConfiguration",href:"#biome.text.configuration.TrainerConfiguration"}},[t._v("TrainerConfiguration")])])]),t._v(" "),a("h2",{attrs:{id:"parameters"}},[t._v("Parameters")]),t._v(" "),a("dl",[a("dt",[a("strong",[a("code",[t._v("optimizer")])])]),t._v(" "),a("dd",[t._v(" ")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("validation_metric")])])]),t._v(" "),a("dd",[t._v(" ")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("patience")])])]),t._v(" "),a("dd",[t._v(" ")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("shuffle")])])]),t._v(" "),a("dd",[t._v(" ")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("num_epochs")])])]),t._v(" "),a("dd",[t._v(" ")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("cuda_device")])])]),t._v(" "),a("dd",[t._v(" ")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("grad_norm")])])]),t._v(" "),a("dd",[t._v(" ")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("grad_clipping")])])]),t._v(" "),a("dd",[t._v(" ")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("learning_rate_scheduler")])])]),t._v(" "),a("dd",[t._v(" ")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("momentum_scheduler")])])]),t._v(" "),a("dd",[t._v(" ")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("moving_average")])])]),t._v(" "),a("dd",[t._v(" ")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("batch_size")])])]),t._v(" "),a("dd",[t._v(" ")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("cache_instances")])])]),t._v(" "),a("dd",[t._v(" ")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("in_memory_batches")])])]),t._v(" "),a("dd",[t._v(" ")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("data_bucketing")])])]),t._v(" "),a("dd",[t._v(" ")])]),t._v(" "),a("div"),t._v(" "),a("pre",{staticClass:"title"},[a("h2",{attrs:{id:"vocabularyconfiguration"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#vocabularyconfiguration"}},[t._v("#")]),t._v(" VocabularyConfiguration "),a("Badge",{attrs:{text:"Class"}})],1),t._v("\n")]),t._v(" "),a("pre",{staticClass:"language-python"},[a("code",[t._v("\n"),a("span",{staticClass:"token keyword"},[t._v("class")]),t._v(" "),a("span",{staticClass:"ident"},[t._v("VocabularyConfiguration")]),t._v(" ("),t._v("\n    "),a("span",[t._v("sources: List[str]")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("min_count: Dict[str, int] = None")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("max_vocab_size: Union[int, Dict[str, int]] = None")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("pretrained_files: Union[Dict[str, str], NoneType] = None")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("only_include_pretrained_words: bool = False")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("tokens_to_add: Dict[str, List[str]] = None")]),a("span",[t._v(",")]),t._v("\n    "),a("span",[t._v("min_pretrained_embeddings: Dict[str, int] = None")]),a("span",[t._v(",")]),t._v("\n"),a("span",[t._v(")")]),t._v("\n")]),t._v("\n")]),t._v(" "),a("p",[t._v("Configures a "),a("code",[t._v("Vocabulary")]),t._v(" before it gets created from data")]),t._v(" "),a("p",[t._v("Use this to configure a Vocabulary using specific arguments from `allennlp.data.Vocabulary``")]),t._v(" "),a("p",[t._v("See "),a("a",{attrs:{href:"https://docs.allennlp.org/master/api/data/vocabulary/#vocabulary]"}},[t._v("AllenNLP Vocabulary docs")])]),t._v(" "),a("h2",{attrs:{id:"parameters"}},[t._v("Parameters")]),t._v(" "),a("dl",[a("dt",[a("strong",[a("code",[t._v("sources")])]),t._v(" : "),a("code",[t._v("List[str]")])]),t._v(" "),a("dd",[t._v("Datasource paths to be used for data creation")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("min_count")])]),t._v(" : "),a("code",[t._v("Dict[str, int]")]),t._v(", optional "),a("code",[t._v("(default=None)")])]),t._v(" "),a("dd",[t._v("Minimum number of appearances of a token to be included in the vocabulary")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("max_vocab_size")])]),t._v(" : "),a("code"),t._v("Union[int, Dict[str, int]]"),a("code",[a("code",[t._v(", optional </code>(default=<code>None</code>)")])])]),t._v(" "),a("dd",[t._v("Maximum number of tokens of the vocabulary")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("pretrained_files")])]),t._v(" : "),a("code",[t._v("Optional[Dict[str, str]]")]),t._v(", optional")]),t._v(" "),a("dd",[t._v("Pretrained files with word vectors")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("only_include_pretrained_words")])]),t._v(" : "),a("code",[t._v("bool")]),t._v(", optional "),a("code",[t._v("(default=False)")])]),t._v(" "),a("dd",[t._v("Only include tokens present in pretrained_files")]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("tokens_to_add")])]),t._v(" : "),a("code",[t._v("Dict[str, int]")]),t._v(", optional")]),t._v(" "),a("dd",[t._v("A list of tokens to add to the vocabulary, even if they are not present in the "),a("code",[t._v("sources")])]),t._v(" "),a("dt",[a("strong",[a("code",[t._v("min_pretrained_embeddings")])]),t._v(" : "),a("code",[t._v("Dict[str, int]")]),t._v(", optional")]),t._v(" "),a("dd",[t._v("Minimum number of lines to keep from pretrained_files, even for tokens not appearing in the sources.")])])])}),[],!1,null,null,null);e.default=s.exports}}]);