(window.webpackJsonp=window.webpackJsonp||[]).push([[54],{393:function(t,e,s){"use strict";s.r(e);var n=s(26),a=Object(n.a)({},(function(){var t=this,e=t.$createElement,s=t._self._c||e;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h1",{attrs:{id:"biome-text-tokenizer"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#biome-text-tokenizer"}},[t._v("#")]),t._v(" biome.text.tokenizer "),s("Badge",{attrs:{text:"Module"}})],1),t._v(" "),s("div"),t._v(" "),s("div"),t._v(" "),s("pre",{staticClass:"title"},[s("h2",{attrs:{id:"tokenizer"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#tokenizer"}},[t._v("#")]),t._v(" Tokenizer "),s("Badge",{attrs:{text:"Class"}})],1),t._v("\n")]),t._v(" "),s("pre",{staticClass:"language-python"},[s("code",[t._v("\n"),s("span",{staticClass:"token keyword"},[t._v("class")]),t._v(" "),s("span",{staticClass:"ident"},[t._v("Tokenizer")]),t._v(" (config: "),s("a",{attrs:{title:"biome.text.configuration.TokenizerConfiguration",href:"configuration.html#biome.text.configuration.TokenizerConfiguration"}},[t._v("TokenizerConfiguration")]),t._v(")"),t._v("\n")]),t._v("\n")]),t._v(" "),s("p",[t._v("Pre-processes and tokenizes the input text")]),t._v(" "),s("p",[t._v("Transforms inputs (e.g., a text, a list of texts, etc.) into structures containing "),s("code",[t._v("allennlp.data.Token")]),t._v(" objects.")]),t._v(" "),s("p",[t._v("Use its arguments to configure the first stage of the pipeline (i.e., pre-processing a given set of text inputs.)")]),t._v(" "),s("p",[t._v("Use methods for tokenization depending on the shape of the inputs\n(e.g., records with multiple fields, sentences lists).")]),t._v(" "),s("h2",{attrs:{id:"parameters"}},[t._v("Parameters")]),t._v(" "),s("dl",[s("dt",[s("strong",[s("code",[t._v("config")])])]),t._v(" "),s("dd",[t._v("A "),s("code",[t._v("TokenizerConfiguration")]),t._v(" object")])]),t._v(" "),s("pre",{staticClass:"title"},[s("h3",{attrs:{id:"instance-variables"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#instance-variables"}},[t._v("#")]),t._v(" Instance variables")]),t._v("\n")]),t._v(" "),s("dl",[s("dt",{attrs:{id:"biome.text.tokenizer.Tokenizer.nlp"}},[s("code",{staticClass:"name"},[t._v("var "),s("span",{staticClass:"ident"},[t._v("nlp")]),t._v(" : spacy.language.Language")])]),t._v(" "),s("dd")]),t._v(" "),s("dl",[s("pre",{staticClass:"title"},[s("h3",{attrs:{id:"tokenize-text"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#tokenize-text"}},[t._v("#")]),t._v(" tokenize_text "),s("Badge",{attrs:{text:"Method"}})],1),t._v("\n")]),t._v(" "),s("dt",[s("div",{staticClass:"language-python extra-class"},[s("pre",{staticClass:"language-python"},[s("code",[t._v("\n"),s("span",{staticClass:"token keyword"},[t._v("def")]),t._v(" "),s("span",{staticClass:"ident"},[t._v("tokenize_text")]),t._v(" ("),t._v("\n  self,\n  text: str,\n)  -> List[List[allennlp.data.tokenizers.token.Token]]\n")]),t._v("\n")])])]),t._v(" "),s("dd",[s("p",[t._v("Tokenizes a text string applying sentence segmentation, if enabled")]),t._v(" "),s("h2",{attrs:{id:"parameters"}},[t._v("Parameters")]),t._v(" "),s("dl",[s("dt",[s("strong",[s("code",[t._v("text")])]),t._v(" : "),s("code",[t._v("str")])]),t._v(" "),s("dd",[t._v("The input text")])]),t._v(" "),s("h2",{attrs:{id:"returns"}},[t._v("Returns")]),t._v(" "),s("p",[t._v("A list of list of "),s("code",[t._v("Token")]),t._v(".")]),t._v(" "),s("dl",[s("dt",[s("code",[t._v("If no sentence segmentation is enabled,")]),t._v(" or "),s("code",[t._v("just one sentence is found in text")])]),t._v(" "),s("dd",[t._v(" ")])]),t._v(" "),s("p",[t._v("the first level list will contain just one element: the tokenized text.")])]),t._v(" "),s("pre",{staticClass:"title"},[s("h3",{attrs:{id:"tokenize-document"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#tokenize-document"}},[t._v("#")]),t._v(" tokenize_document "),s("Badge",{attrs:{text:"Method"}})],1),t._v("\n")]),t._v(" "),s("dt",[s("div",{staticClass:"language-python extra-class"},[s("pre",{staticClass:"language-python"},[s("code",[t._v("\n"),s("span",{staticClass:"token keyword"},[t._v("def")]),t._v(" "),s("span",{staticClass:"ident"},[t._v("tokenize_document")]),t._v(" ("),t._v("\n  self,\n  document: List[str],\n)  -> List[List[allennlp.data.tokenizers.token.Token]]\n")]),t._v("\n")])])]),t._v(" "),s("dd",[s("p",[t._v("Tokenizes a document-like structure containing lists of text inputs")]),t._v(" "),s("p",[t._v("Use this to account for hierarchical text structures (e.g., a paragraph)")]),t._v(" "),s("h2",{attrs:{id:"parameters"}},[t._v("Parameters")]),t._v(" "),s("dl",[s("dt",[s("strong",[s("code",[t._v("document")])]),t._v(" : "),s("code",[t._v("List[str]")])]),t._v(" "),s("dd",[t._v("A "),s("code",[t._v("List")]),t._v(" with text inputs, e.g., paragraphs")])]),t._v(" "),s("h2",{attrs:{id:"returns"}},[t._v("Returns")]),t._v(" "),s("dl",[s("dt",[s("strong",[s("code",[t._v("tokens")])]),t._v(" : "),s("code",[t._v("List[List[Token]]")])]),t._v(" "),s("dd",[t._v(" ")])])]),t._v(" "),s("pre",{staticClass:"title"},[s("h3",{attrs:{id:"tokenize-record"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#tokenize-record"}},[t._v("#")]),t._v(" tokenize_record "),s("Badge",{attrs:{text:"Method"}})],1),t._v("\n")]),t._v(" "),s("dt",[s("div",{staticClass:"language-python extra-class"},[s("pre",{staticClass:"language-python"},[s("code",[t._v("\n"),s("span",{staticClass:"token keyword"},[t._v("def")]),t._v(" "),s("span",{staticClass:"ident"},[t._v("tokenize_record")]),t._v(" ("),t._v("\n  self,\n  record: Dict[str, Any],\n  exclude_record_keys: bool,\n)  -> List[List[allennlp.data.tokenizers.token.Token]]\n")]),t._v("\n")])])]),t._v(" "),s("dd",[s("p",[t._v("Tokenizes a record-like structure containing text inputs")]),t._v(" "),s("p",[t._v("Use this to keep information about the record-like data structure as input features to the model.")]),t._v(" "),s("h2",{attrs:{id:"parameters"}},[t._v("Parameters")]),t._v(" "),s("dl",[s("dt",[s("strong",[s("code",[t._v("record")])]),t._v(" : "),s("code",[t._v("Dict[str, Any]")])]),t._v(" "),s("dd",[t._v("A "),s("code",[t._v("Dict")]),t._v(' with arbitrary "fields" containing text.')]),t._v(" "),s("dt",[s("strong",[s("code",[t._v("exclude_record_keys")])]),t._v(" : "),s("code",[t._v("bool")])]),t._v(" "),s("dd",[t._v("If enabled, exclude tokens related to record key text")])]),t._v(" "),s("h2",{attrs:{id:"returns"}},[t._v("Returns")]),t._v(" "),s("dl",[s("dt",[s("strong",[s("code",[t._v("tokens")])]),t._v(" : "),s("code",[t._v("List[List[Token]]")])]),t._v(" "),s("dd",[t._v("A list of tokenized fields as token list")])])])])])}),[],!1,null,null,null);e.default=a.exports}}]);