(window.webpackJsonp=window.webpackJsonp||[]).push([[48],{382:function(e,t,n){"use strict";n.r(t);var s=n(26),a=Object(s.a)({},(function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[n("h1",{attrs:{id:"biome-text-tokenizer"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#biome-text-tokenizer"}},[e._v("#")]),e._v(" biome.text.tokenizer "),n("Badge",{attrs:{text:"Module"}})],1),e._v(" "),n("div"),e._v(" "),n("div"),e._v(" "),n("pre",{staticClass:"title"},[n("h2",{attrs:{id:"tokenizer"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#tokenizer"}},[e._v("#")]),e._v(" Tokenizer "),n("Badge",{attrs:{text:"Class"}})],1),e._v("\n")]),e._v(" "),n("pre",{staticClass:"language-python"},[n("code",[e._v("\n"),n("span",{staticClass:"token keyword"},[e._v("class")]),e._v(" "),n("span",{staticClass:"ident"},[e._v("Tokenizer")]),e._v(" ("),e._v("\n    "),n("span",[e._v("lang: str = 'en'")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("skip_empty_tokens: bool = False")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("max_sequence_length: int = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("max_nr_of_sentences: int = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("text_cleaning: Union[biome.text.text_cleaning.TextCleaning, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("segment_sentences: Union[bool, allennlp.data.tokenizers.sentence_splitter.SentenceSplitter] = False")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("start_tokens: Union[List[str], NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("end_tokens: Union[List[str], NoneType] = None")]),n("span",[e._v(",")]),e._v("\n"),n("span",[e._v(")")]),e._v("\n")]),e._v("\n")]),e._v(" "),n("p",[e._v("Pre-processes and tokenizes input text")]),e._v(" "),n("p",[e._v("Transforms inputs (e.g., a text, a list of texts, etc.) into structures containing "),n("code",[e._v("allennlp.data.Token")]),e._v(" objects.")]),e._v(" "),n("p",[e._v("Use its arguments to configure the first stage of the pipeline (i.e., pre-processing a given set of text inputs.)")]),e._v(" "),n("p",[e._v("Use methods for tokenizing depending on the shape of inputs (e.g., records with multiple fields, sentences lists).")]),e._v(" "),n("h1",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),n("p",[e._v("lang: "),n("code",[e._v("str")]),e._v("\nThe "),n("code",[e._v("spaCy")]),e._v(" language to be used by the tokenizer (default is "),n("code",[e._v("en")]),e._v(")\nskip_empty_tokens: "),n("code",[e._v("bool")]),e._v("\nmax_sequence_length: "),n("code",[e._v("int")]),e._v("\nMaximum length in characters for input texts truncated with "),n("code",[e._v("[:max_sequence_length]")]),e._v(" after "),n("code",[e._v("TextCleaning")]),e._v(".\nmax_nr_of_sentences: "),n("code",[e._v("int")]),e._v("\nMaximum number of sentences to keep when using "),n("code",[e._v("segment_sentences")]),e._v(" truncated with "),n("code",[e._v("[:max_sequence_length]")]),e._v(".\ntext_cleaning: "),n("code",[e._v("Optional[TextCleaning]")]),e._v("\nA "),n("code",[e._v("TextCleaning")]),e._v(" configuration with pre-processing rules for cleaning up and transforming raw input text.\nsegment_sentences:\n"),n("code",[e._v("Union[bool, SentenceSplitter]")]),e._v("\nWhether to segment input texts in to sentences using the default "),n("code",[e._v("SentenceSplitter")]),e._v(" or a given splitter.\nstart_tokens: "),n("code",[e._v("Optional[List[str]]")]),e._v("\nA list of token strings to the sequence before tokenized input text.\nend_tokens: "),n("code",[e._v("Optional[List[str]]")]),e._v("\nA list of token strings to the sequence after tokenized input text.")]),e._v(" "),n("pre",{staticClass:"title"},[n("h3",{attrs:{id:"ancestors"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#ancestors"}},[e._v("#")]),e._v(" Ancestors")]),e._v("\n")]),e._v(" "),n("ul",{staticClass:"hlist"},[n("li",[e._v("allennlp.common.from_params.FromParams")])]),e._v(" "),n("dl",[n("pre",{staticClass:"title"},[n("h3",{attrs:{id:"tokenize-text"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#tokenize-text"}},[e._v("#")]),e._v(" tokenize_text "),n("Badge",{attrs:{text:"Method"}})],1),e._v("\n")]),e._v(" "),n("dt",[n("div",{staticClass:"language-python extra-class"},[n("pre",{staticClass:"language-python"},[n("code",[e._v("\n"),n("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),n("span",{staticClass:"ident"},[e._v("tokenize_text")]),e._v(" ("),e._v("\n  self,\n  text: str,\n)  -> List[List[allennlp.data.tokenizers.token.Token]]\n")]),e._v("\n")])])]),e._v(" "),n("dd",[n("p",[e._v("Tokenizes a text string applying sentence segmentation, if enabled")]),e._v(" "),n("h2",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),n("dl",[n("dt",[n("strong",[n("code",[e._v("text")])]),e._v(" : "),n("code",[e._v("str")])]),e._v(" "),n("dd",[e._v("The input text")])]),e._v(" "),n("h2",{attrs:{id:"returns"}},[e._v("Returns")]),e._v(" "),n("p",[e._v("A list of list of "),n("code",[e._v("Token")]),e._v(".")]),e._v(" "),n("dl",[n("dt",[n("code",[e._v("If no sentence segmentation is enabled,")]),e._v(" or "),n("code",[e._v("just one sentence is found in text")])]),e._v(" "),n("dd",[e._v(" ")])]),e._v(" "),n("p",[e._v("the first level list will contain just one element: the tokenized text.")])]),e._v(" "),n("pre",{staticClass:"title"},[n("h3",{attrs:{id:"tokenize-document"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#tokenize-document"}},[e._v("#")]),e._v(" tokenize_document "),n("Badge",{attrs:{text:"Method"}})],1),e._v("\n")]),e._v(" "),n("dt",[n("div",{staticClass:"language-python extra-class"},[n("pre",{staticClass:"language-python"},[n("code",[e._v("\n"),n("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),n("span",{staticClass:"ident"},[e._v("tokenize_document")]),e._v(" ("),e._v("\n  self,\n  document: List[str],\n)  -> List[List[allennlp.data.tokenizers.token.Token]]\n")]),e._v("\n")])])]),e._v(" "),n("dd",[n("p",[e._v("Tokenizes a document-like structure containing lists of text inputs")]),e._v(" "),n("p",[e._v("Use this to account for hierarchical text structures (e.g., a paragraph is a list of sentences)")]),e._v(" "),n("h2",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),n("dl",[n("dt",[n("strong",[n("code",[e._v("document")])]),e._v(" : "),n("code",[e._v("List[str]")])]),e._v(" "),n("dd",[e._v("A "),n("code",[e._v("List")]),e._v(" with text inputs, e.g., sentences")])]),e._v(" "),n("h2",{attrs:{id:"returns"}},[e._v("Returns")]),e._v(" "),n("dl",[n("dt",[n("strong",[n("code",[e._v("tokens")])]),e._v(" : "),n("code",[e._v("List[List[Token]]")])]),e._v(" "),n("dd",[e._v(" ")])])]),e._v(" "),n("pre",{staticClass:"title"},[n("h3",{attrs:{id:"tokenize-record"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#tokenize-record"}},[e._v("#")]),e._v(" tokenize_record "),n("Badge",{attrs:{text:"Method"}})],1),e._v("\n")]),e._v(" "),n("dt",[n("div",{staticClass:"language-python extra-class"},[n("pre",{staticClass:"language-python"},[n("code",[e._v("\n"),n("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),n("span",{staticClass:"ident"},[e._v("tokenize_record")]),e._v(" ("),e._v("\n  self,\n  record: Dict[str, Any],\n  exclude_record_keys: bool,\n)  -> List[List[allennlp.data.tokenizers.token.Token]]\n")]),e._v("\n")])])]),e._v(" "),n("dd",[n("p",[e._v("Tokenizes a record-like structure containing text inputs")]),e._v(" "),n("p",[e._v("Use this to keep information about the record-like data structure as input features to the model.")]),e._v(" "),n("h2",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),n("dl",[n("dt",[n("strong",[n("code",[e._v("record")])]),e._v(" : "),n("code",[e._v("Dict[str, Any]")])]),e._v(" "),n("dd",[e._v("A "),n("code",[e._v("Dict")]),e._v(' with arbitrary "fields" containing text.')]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("exclude_record_keys")])]),e._v(" : "),n("code",[e._v("bool")])]),e._v(" "),n("dd",[e._v("If enabled, exclude tokens related to record key text")])]),e._v(" "),n("h2",{attrs:{id:"returns"}},[e._v("Returns")]),e._v(" "),n("dl",[n("dt",[n("strong",[n("code",[e._v("tokens")])]),e._v(" : "),n("code",[e._v("List[List[Token]]")])]),e._v(" "),n("dd",[e._v("A list of tokenized fields as token list")])])])])])}),[],!1,null,null,null);t.default=a.exports}}]);