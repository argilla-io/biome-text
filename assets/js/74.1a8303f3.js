(window.webpackJsonp=window.webpackJsonp||[]).push([[74],{415:function(e,s,t){"use strict";t.r(s);var a=t(33),i=Object(a.a)({},(function(){var e=this,s=e.$createElement,t=e._self._c||s;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("h1",{attrs:{id:"biome-text-models-similarity-classifier"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#biome-text-models-similarity-classifier"}},[e._v("#")]),e._v(" biome.text.models.similarity_classifier "),t("Badge",{attrs:{text:"Module"}})],1),e._v(" "),t("dl",[t("h2",{attrs:{id:"biome.text.models.similarity_classifier.SimilarityClassifier"}},[e._v("SimilarityClassifier "),t("Badge",{attrs:{text:"Class"}})],1),e._v(" "),t("dt",[t("div",{staticClass:"language-python extra-class"},[t("pre",{staticClass:"language-python"},[e._v("    "),t("code",[e._v("\n"),t("span",{staticClass:"token keyword"},[e._v("class")]),e._v(" "),t("span",{staticClass:"ident"},[e._v("SimilarityClassifier")]),e._v(" ("),e._v("\n    "),t("span",[e._v("vocab: allennlp.data.vocabulary.Vocabulary")]),t("span",[e._v(",")]),e._v("\n    "),t("span",[e._v("text_field_embedder: allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder")]),t("span",[e._v(",")]),e._v("\n    "),t("span",[e._v("seq2vec_encoder: allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder")]),t("span",[e._v(",")]),e._v("\n    "),t("span",[e._v("seq2seq_encoder: Union[allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder, NoneType] = None")]),t("span",[e._v(",")]),e._v("\n    "),t("span",[e._v("multifield_seq2seq_encoder: Union[allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder, NoneType] = None")]),t("span",[e._v(",")]),e._v("\n    "),t("span",[e._v("multifield_seq2vec_encoder: Union[allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder, NoneType] = None")]),t("span",[e._v(",")]),e._v("\n    "),t("span",[e._v("feed_forward: Union[allennlp.modules.feedforward.FeedForward, NoneType] = None")]),t("span",[e._v(",")]),e._v("\n    "),t("span",[e._v("dropout: Union[float, NoneType] = None")]),t("span",[e._v(",")]),e._v("\n    "),t("span",[e._v("multifield_dropout: Union[float, NoneType] = None")]),t("span",[e._v(",")]),e._v("\n    "),t("span",[e._v("initializer: Union[allennlp.nn.initializers.InitializerApplicator, NoneType] = None")]),t("span",[e._v(",")]),e._v("\n    "),t("span",[e._v("regularizer: Union[allennlp.nn.regularizers.regularizer_applicator.RegularizerApplicator, NoneType] = None")]),t("span",[e._v(",")]),e._v("\n    "),t("span",[e._v("margin: float = 0.5")]),t("span",[e._v(",")]),e._v("\n    "),t("span",[e._v("verification_weight: float = 2.0")]),t("span",[e._v(",")]),e._v("\n"),t("span",[e._v(")")]),e._v("\n    ")])])])]),e._v(" "),t("dd",[t("div",{staticClass:"desc"},[t("p",[e._v("This "),t("code",[t("a",{attrs:{title:"biome.text.models.similarity_classifier.SimilarityClassifier",href:"#biome.text.models.similarity_classifier.SimilarityClassifier"}},[e._v("SimilarityClassifier")])]),e._v(" uses a siamese network architecture to perform a binary classification task:\nare two inputs similar or not?\nThe two input sequences are encoded with two single vectors, the resulting vectors are concatenated and fed to a\nlinear classification layer.")]),e._v(" "),t("p",[e._v("Apart from the CrossEntropy loss, this model includes a CosineEmbedding loss\n("),t("a",{attrs:{href:"https://pytorch.org/docs/stable/nn.html#cosineembeddingloss"}},[e._v("https://pytorch.org/docs/stable/nn.html#cosineembeddingloss")]),e._v(') that will drive the network to create\nvector clusters for each "class" in the data.\nMake sure that the label "same" is indexed as 0, and the label "different" as 1!!!\nMake sure that the dropout of the last Seq2Vec or the last FeedForward layer is set to 0!!!\n(Deep Learning Face Representation by Joint Identification-Verification, '),t("a",{attrs:{href:"https://arxiv.org/pdf/1406.4773.pdf"}},[e._v("https://arxiv.org/pdf/1406.4773.pdf")]),e._v(")")]),e._v(" "),t("h2",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),t("dl",[t("dt",[t("strong",[t("code",[e._v("kwargs")])])]),e._v(" "),t("dd",[e._v("See the "),t("code",[e._v("BaseModelClassifier")]),e._v(" for a description of the parameters.")]),e._v(" "),t("dt",[t("strong",[t("code",[e._v("margin")])])]),e._v(" "),t("dd",[e._v("This parameter is passed on to the CosineEmbedding loss. It provides a margin,\nat which dissimilar vectors are not driven further apart.\nCan be between -1 (always drive apart) and 1 (never drive apart).")]),e._v(" "),t("dt",[t("strong",[t("code",[e._v("verification_weight")])])]),e._v(" "),t("dd",[e._v("Defines the weight of the verification loss in the final loss sum:\nloss = CrossEntropy + w * CosineEmbedding")])]),e._v(" "),t("p",[e._v("Initializes internal Module state, shared by both nn.Module and ScriptModule.")])]),e._v(" "),t("h3",[e._v("Ancestors")]),e._v(" "),t("ul",{staticClass:"hlist"},[t("li",[t("a",{attrs:{title:"biome.text.models.sequence_classifier_base.SequenceClassifierBase",href:"sequence_classifier_base.html#biome.text.models.sequence_classifier_base.SequenceClassifierBase"}},[e._v("SequenceClassifierBase")])]),e._v(" "),t("li",[t("a",{attrs:{title:"biome.text.models.mixins.BiomeClassifierMixin",href:"mixins.html#biome.text.models.mixins.BiomeClassifierMixin"}},[e._v("BiomeClassifierMixin")])]),e._v(" "),t("li",[e._v("allennlp.models.model.Model")]),e._v(" "),t("li",[e._v("torch.nn.modules.module.Module")]),e._v(" "),t("li",[e._v("allennlp.common.registrable.Registrable")]),e._v(" "),t("li",[e._v("allennlp.common.from_params.FromParams")])]),e._v(" "),t("dl",[t("h3",{attrs:{id:"biome.text.models.similarity_classifier.SimilarityClassifier.forward"}},[e._v("forward "),t("Badge",{attrs:{text:"Method"}})],1),e._v(" "),t("dt",[t("div",{staticClass:"language-python extra-class"},[t("pre",{staticClass:"language-python"},[t("code",[e._v("\n"),t("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),t("span",{staticClass:"ident"},[e._v("forward")]),e._v(" ("),e._v("\n   self,\n   record1: Dict[str, torch.Tensor],\n   record2: Dict[str, torch.Tensor],\n   label: torch.Tensor = None,\n)  -> Dict[str, torch.Tensor]\n")]),e._v("\n        ")])])]),e._v(" "),t("dd",[t("div",{staticClass:"desc"},[t("p",[e._v("The architecture is basically:\nEmbedding -> Seq2Seq -> Seq2Vec -> Dropout -> (Optional: MultiField stuff) -> FeedForward\n-> Concatenation -> Classification layer")]),e._v(" "),t("h2",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),t("dl",[t("dt",[t("strong",[t("code",[e._v("record1")])])]),e._v(" "),t("dd",[e._v("The first input tokens.\nThe dictionary is the output of a "),t("code",[e._v("TextField.as_array()")]),e._v(". It gives names to the tensors created by\nthe "),t("code",[e._v("TokenIndexer")]),e._v("s.\nIn its most basic form, using a "),t("code",[e._v("SingleIdTokenIndexer")]),e._v(", the dictionary is composed of:\n"),t("code",[e._v('{"tokens": Tensor(batch_size, num_tokens)}')]),e._v(".\nThe keys of the dictionary are defined in the "),t("code",[e._v("model.yml")]),e._v(" input.\nThe dictionary is designed to be passed on directly to a "),t("code",[e._v("TextFieldEmbedder")]),e._v(", that has a\n"),t("code",[e._v("TokenEmbedder")]),e._v(" for each key in the dictionary (except you set "),t("code",[e._v("allow_unmatched_keys")]),e._v(" in the\n"),t("code",[e._v("TextFieldEmbedder")]),e._v(" to False) and knows how to combine different word/character representations into a\nsingle vector per token in your input.")]),e._v(" "),t("dt",[t("strong",[t("code",[e._v("record2")])])]),e._v(" "),t("dd",[e._v("The second input tokens.")]),e._v(" "),t("dt",[t("strong",[t("code",[e._v("label")])]),e._v(" : "),t("code",[e._v("torch.LongTensor")]),e._v(", optional "),t("code",[e._v("(default = None)")])]),e._v(" "),t("dd",[e._v("A torch tensor representing the sequence of integer gold class label of shape\n"),t("code",[e._v("(batch_size, num_classes)")]),e._v(".")])]),e._v(" "),t("h2",{attrs:{id:"returns"}},[e._v("Returns")]),e._v(" "),t("dl",[t("dt",[t("code",[e._v("An output dictionary consisting of:")])]),e._v(" "),t("dd",[e._v(" ")]),e._v(" "),t("dt",[t("strong",[t("code",[e._v("logits")])]),e._v(" : "),t("code",[e._v("torch.FloatTensor")])]),e._v(" "),t("dd",[e._v("A tensor of shape "),t("code",[e._v("(batch_size, num_tokens, tag_vocab_size)")]),e._v(" representing\nunnormalised log probabilities of the tag classes.")]),e._v(" "),t("dt",[t("strong",[t("code",[e._v("class_probabilities")])]),e._v(" : "),t("code",[e._v("torch.FloatTensor")])]),e._v(" "),t("dd",[e._v("A tensor of shape "),t("code",[e._v("(batch_size, num_tokens, tag_vocab_size)")]),e._v(" representing\na distribution of the tag classes per word.")]),e._v(" "),t("dt",[t("strong",[t("code",[e._v("loss")])]),e._v(" : "),t("code",[e._v("torch.FloatTensor")]),e._v(", optional")]),e._v(" "),t("dd",[e._v("A scalar loss to be optimised.")])])])])]),e._v(" "),t("h3",[e._v("Inherited members")]),e._v(" "),t("ul",{staticClass:"hlist"},[t("li",[t("code",[t("b",[t("a",{attrs:{title:"biome.text.models.sequence_classifier_base.SequenceClassifierBase",href:"sequence_classifier_base.html#biome.text.models.sequence_classifier_base.SequenceClassifierBase"}},[e._v("SequenceClassifierBase")])])]),e._v(":\n"),t("ul",{staticClass:"hlist"},[t("li",[t("code",[t("a",{attrs:{title:"biome.text.models.sequence_classifier_base.SequenceClassifierBase.decode",href:"mixins.html#biome.text.models.mixins.BiomeClassifierMixin.decode"}},[e._v("decode")])])]),e._v(" "),t("li",[t("code",[t("a",{attrs:{title:"biome.text.models.sequence_classifier_base.SequenceClassifierBase.extend_labels",href:"sequence_classifier_base.html#biome.text.models.sequence_classifier_base.SequenceClassifierBase.extend_labels"}},[e._v("extend_labels")])])]),e._v(" "),t("li",[t("code",[t("a",{attrs:{title:"biome.text.models.sequence_classifier_base.SequenceClassifierBase.forward_tokens",href:"sequence_classifier_base.html#biome.text.models.sequence_classifier_base.SequenceClassifierBase.forward_tokens"}},[e._v("forward_tokens")])])]),e._v(" "),t("li",[t("code",[t("a",{attrs:{title:"biome.text.models.sequence_classifier_base.SequenceClassifierBase.get_metrics",href:"mixins.html#biome.text.models.mixins.BiomeClassifierMixin.get_metrics"}},[e._v("get_metrics")])])]),e._v(" "),t("li",[t("code",[t("a",{attrs:{title:"biome.text.models.sequence_classifier_base.SequenceClassifierBase.label_for_index",href:"sequence_classifier_base.html#biome.text.models.sequence_classifier_base.SequenceClassifierBase.label_for_index"}},[e._v("label_for_index")])])]),e._v(" "),t("li",[t("code",[t("a",{attrs:{title:"biome.text.models.sequence_classifier_base.SequenceClassifierBase.n_inputs",href:"sequence_classifier_base.html#biome.text.models.sequence_classifier_base.SequenceClassifierBase.n_inputs"}},[e._v("n_inputs")])])]),e._v(" "),t("li",[t("code",[t("a",{attrs:{title:"biome.text.models.sequence_classifier_base.SequenceClassifierBase.num_classes",href:"sequence_classifier_base.html#biome.text.models.sequence_classifier_base.SequenceClassifierBase.num_classes"}},[e._v("num_classes")])])]),e._v(" "),t("li",[t("code",[t("a",{attrs:{title:"biome.text.models.sequence_classifier_base.SequenceClassifierBase.output_classes",href:"sequence_classifier_base.html#biome.text.models.sequence_classifier_base.SequenceClassifierBase.output_classes"}},[e._v("output_classes")])])]),e._v(" "),t("li",[t("code",[t("a",{attrs:{title:"biome.text.models.sequence_classifier_base.SequenceClassifierBase.output_layer",href:"sequence_classifier_base.html#biome.text.models.sequence_classifier_base.SequenceClassifierBase.output_layer"}},[e._v("output_layer")])])])])])])]),e._v(" "),t("h2",{attrs:{id:"biome.text.models.similarity_classifier.ContrastiveLoss"}},[e._v("ContrastiveLoss "),t("Badge",{attrs:{text:"Class"}})],1),e._v(" "),t("dt",[t("div",{staticClass:"language-python extra-class"},[t("pre",{staticClass:"language-python"},[e._v("    "),t("code",[e._v("\n"),t("span",{staticClass:"token keyword"},[e._v("class")]),e._v(" "),t("span",{staticClass:"ident"},[e._v("ContrastiveLoss")]),e._v(" ()"),e._v("\n    ")])])])]),e._v(" "),t("dd",[t("div",{staticClass:"desc"},[t("p",[e._v("Computes a contrastive loss given a distance.")]),e._v(" "),t("p",[e._v("We do not use it at the moment, i leave it here just in case.")]),e._v(" "),t("p",[e._v("Initializes internal Module state, shared by both nn.Module and ScriptModule.")])]),e._v(" "),t("h3",[e._v("Ancestors")]),e._v(" "),t("ul",{staticClass:"hlist"},[t("li",[e._v("torch.nn.modules.module.Module")])]),e._v(" "),t("dl",[t("h3",{attrs:{id:"biome.text.models.similarity_classifier.ContrastiveLoss.forward"}},[e._v("forward "),t("Badge",{attrs:{text:"Method"}})],1),e._v(" "),t("dt",[t("div",{staticClass:"language-python extra-class"},[t("pre",{staticClass:"language-python"},[t("code",[e._v("\n"),t("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),t("span",{staticClass:"ident"},[e._v("forward")]),e._v(" ("),e._v("\n   self,\n   distance,\n   label,\n   margin,\n) \n")]),e._v("\n        ")])])]),e._v(" "),t("dd",[t("div",{staticClass:"desc"},[t("p",[e._v("Compute the loss.")]),e._v(" "),t("p",[e._v("Important: Make sure label = 0 corresponds to the same case, label = 1 to the different case!")]),e._v(" "),t("h2",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),t("dl",[t("dt",[t("strong",[t("code",[e._v("distance")])])]),e._v(" "),t("dd",[e._v("Distance between the two input vectors")]),e._v(" "),t("dt",[t("strong",[t("code",[e._v("label")])])]),e._v(" "),t("dd",[e._v("Label if the two input vectors belong to the same or different class.")]),e._v(" "),t("dt",[t("strong",[t("code",[e._v("margin")])])]),e._v(" "),t("dd",[e._v("If the distance is larger than the margin, the distance of different class vectors\ndoes not contribute to the loss.")])]),e._v(" "),t("h2",{attrs:{id:"returns"}},[e._v("Returns")]),e._v(" "),t("dl",[t("dt",[t("code",[e._v("loss")])]),e._v(" "),t("dd",[e._v(" ")])])])])])])])])}),[],!1,null,null,null);s.default=i.exports}}]);