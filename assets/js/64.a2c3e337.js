(window.webpackJsonp=window.webpackJsonp||[]).push([[64],{442:function(t,a,s){"use strict";s.r(a);var n=s(26),e=Object(n.a)({},(function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h1",{attrs:{id:"using-transformers-in-biome-text"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#using-transformers-in-biome-text"}},[t._v("#")]),t._v(" Using Transformers in biome.text")]),t._v(" "),s("p",[s("a",{attrs:{target:"_blank",href:"https://www.recogn.ai/biome-text/documentation/tutorials/4-Using_Transformers_in_biome_text.html"}},[s("img",{staticClass:"icon",attrs:{src:"https://www.recogn.ai/biome-text/assets/img/biome-isotype.svg",width:"24"}})]),t._v(" "),s("a",{attrs:{href:"https://www.recogn.ai/biome-text/documentation/tutorials/4-Using_Transformers_in_biome_text.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("View on recogn.ai"),s("OutboundLink")],1)]),t._v(" "),s("p",[s("a",{attrs:{target:"_blank",href:"https://colab.research.google.com/github/recognai/biome-text/blob/master/docs/docs/documentation/tutorials/4-Using_Transformers_in_biome_text.ipynb"}},[s("img",{staticClass:"icon",attrs:{src:"https://www.tensorflow.org/images/colab_logo_32px.png",width:"24"}})]),t._v(" "),s("a",{attrs:{href:"https://colab.research.google.com/github/recognai/biome-text/blob/master/docs/docs/documentation/tutorials/4-Using_Transformers_in_biome_text.ipynb",target:"_blank",rel:"noopener noreferrer"}},[t._v("Run in Google Colab"),s("OutboundLink")],1)]),t._v(" "),s("p",[s("a",{attrs:{target:"_blank",href:"https://github.com/recognai/biome-text/blob/master/docs/docs/documentation/tutorials/4-Using_Transformers_in_biome_text.ipynb"}},[s("img",{staticClass:"icon",attrs:{src:"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png",width:"24"}})]),t._v(" "),s("a",{attrs:{href:"https://github.com/recognai/biome-text/blob/master/docs/docs/documentation/tutorials/4-Using_Transformers_in_biome_text.ipynb",target:"_blank",rel:"noopener noreferrer"}},[t._v("View source on GitHub"),s("OutboundLink")],1)]),t._v(" "),s("p",[t._v("When running this tutorial in Google Colab, make sure to install "),s("em",[t._v("biome.text")]),t._v(" first:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("!pip install "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("U pip\n!pip install git"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v("https"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("//")]),t._v("github"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("com"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("recognai"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("biome"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("git\nexit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Force restart of the runtime")]),t._v("\n")])])]),s("p",[s("em",[t._v("If")]),t._v(" you want to log your runs with "),s("a",{attrs:{href:"https://wandb.ai/home",target:"_blank",rel:"noopener noreferrer"}},[t._v("WandB"),s("OutboundLink")],1),t._v(", don't forget to install its client and log in.")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("!pip install wandb\n!wandb login\n")])])]),s("h2",{attrs:{id:"introduction"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#introduction"}},[t._v("#")]),t._v(" Introduction")]),t._v(" "),s("p",[t._v("In the last years we experienced a shift towards transfer learning as the standard approach to solve NLP problems. Before models were usually trained entirely from scratch, utilizing at most pretrained word embeddings. But nowadays it is very common to start with large pretrained language models as backbone of a system, and to set a task specific head on top of it. This new paradigm has made it easier to find state-of-the-art architectures for a great variety of NLP tasks.")]),t._v(" "),s("p",[t._v("Almost all current language models are based on the transformer architecture. The awesome "),s("a",{attrs:{href:"https://github.com/huggingface/transformers",target:"_blank",rel:"noopener noreferrer"}},[t._v("Hugging Face Transformers"),s("OutboundLink")],1),t._v(" library provides access to hundreds of such pretrained language models including state-of-the-art models such as infamous "),s("a",{attrs:{href:"https://github.com/google-research/bert",target:"_blank",rel:"noopener noreferrer"}},[t._v("BERT"),s("OutboundLink")],1),t._v(", as well as community driven models often covering a specific language type or resource requirements.")]),t._v(" "),s("p",[t._v("In this tutorial, we are going to classify "),s("a",{attrs:{href:"https://arxiv.org/",target:"_blank",rel:"noopener noreferrer"}},[t._v("arXiv"),s("OutboundLink")],1),t._v(" papers into "),s("a",{attrs:{href:"https://arxiv.org/category_taxonomy",target:"_blank",rel:"noopener noreferrer"}},[t._v("categories"),s("OutboundLink")],1),t._v(", analyzing the title of the paper and its abstract. We will use Hugging Face "),s("a",{attrs:{href:"https://medium.com/huggingface/distilbert-8cf3380435b5",target:"_blank",rel:"noopener noreferrer"}},[t._v("distilled"),s("OutboundLink")],1),t._v(" implementation of "),s("a",{attrs:{href:"https://ai.facebook.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/",target:"_blank",rel:"noopener noreferrer"}},[t._v("RoBERTa"),s("OutboundLink")],1),t._v(" and explore ways how to easily include pretrained transformers in a "),s("em",[t._v("biome.text")]),t._v(" pipeline.")]),t._v(" "),s("h3",{attrs:{id:"external-links-about-transformers"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#external-links-about-transformers"}},[t._v("#")]),t._v(" External links about transformers")]),t._v(" "),s("p",[t._v('If this is the first time you hear about "Transformers" not referring to giant robots, here is a small list of resources at which you might want to have a look first:')]),t._v(" "),s("ul",[s("li",[s("a",{attrs:{href:"https://arxiv.org/pdf/1706.03762.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Attention is all your need"),s("OutboundLink")],1),t._v(": paper that introduced the architecture.")]),t._v(" "),s("li",[s("a",{attrs:{href:"https://jalammar.github.io/illustrated-transformer/",target:"_blank",rel:"noopener noreferrer"}},[t._v("The Illustrated Transformer"),s("OutboundLink")],1),t._v(": 20-30 minute article covering how they work.")]),t._v(" "),s("li",[s("a",{attrs:{href:"https://youtu.be/4Bdc55j80l8",target:"_blank",rel:"noopener noreferrer"}},[t._v("Illustrated Guide to Transformer Neural Network: a step by step explanation"),s("OutboundLink")],1),t._v(": 15 minute long video covering how they work.")]),t._v(" "),s("li",[s("a",{attrs:{href:"https://www.youtube.com/watch?v=8Hg2UtQg6G4",target:"_blank",rel:"noopener noreferrer"}},[t._v("An Introduction To Transfer Learning In NLP and HuggingFace"),s("OutboundLink")],1),t._v(": 1 hour talk by Thomas Wolf")])]),t._v(" "),s("h3",{attrs:{id:"imports"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#imports"}},[t._v("#")]),t._v(" Imports")]),t._v(" "),s("p",[t._v("Let us first import all the stuff we need for this tutorial:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" biome"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Dataset"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Pipeline\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" biome"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("configuration "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" VocabularyConfiguration"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" TrainerConfiguration\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" biome"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("hpo "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" TuneExperiment\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" ray "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" tune\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" os\n")])])]),s("h2",{attrs:{id:"exploring-and-preparing-the-data"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#exploring-and-preparing-the-data"}},[t._v("#")]),t._v(" Exploring and preparing the data")]),t._v(" "),s("p",[t._v("For this tutorial we are going to use the "),s("a",{attrs:{href:"https://www.kaggle.com/Cornell-University/arxiv",target:"_blank",rel:"noopener noreferrer"}},[t._v("arXiv dataset"),s("OutboundLink")],1),t._v(" compiled by the Cornell University, which consists of metadata of scientific papers stored in "),s("a",{attrs:{href:"https://arxiv.org/",target:"_blank",rel:"noopener noreferrer"}},[t._v("arXiv"),s("OutboundLink")],1),t._v(".")]),t._v(" "),s("p",[t._v("We preprocessed the data in a separate "),s("a",{attrs:{href:"https://drive.google.com/file/d/1zUSz81x15RH5mL5GoN7i7xqiNGEqclU0/view?usp=sharing",target:"_blank",rel:"noopener noreferrer"}},[t._v("notebook"),s("OutboundLink")],1),t._v(" producing three csv files (train, validate and test datasets) that contain the title, the abstract and the category of the corresponding paper.")]),t._v(" "),s("p",[t._v("Our NLP task will be to classify the papers into the given categories based on the title and abstract. Below we download the preprocessed data and create our "),s("a",{attrs:{href:"https://www.recogn.ai/biome-text/api/biome/text/dataset.html#dataset",target:"_blank",rel:"noopener noreferrer"}},[t._v("Datasets"),s("OutboundLink")],1),t._v(" with it.")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Downloading the datasets")]),t._v("\n!curl "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("O https"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("//")]),t._v("biome"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("tutorials"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("s3"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("eu"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("west"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.")]),t._v("amazonaws"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("com"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("transformers_arxiv"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("classifier"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("arxiv"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("dataset"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("json\n!curl "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("O https"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("//")]),t._v("biome"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("tutorials"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("s3"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("eu"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("west"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.")]),t._v("amazonaws"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("com"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("transformers_arxiv"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("classifier"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("arxiv"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("dataset"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("validate"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("json\n!curl "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("O https"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("//")]),t._v("biome"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("tutorials"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("s3"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("eu"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("west"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.")]),t._v("amazonaws"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("com"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("transformers_arxiv"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("classifier"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("arxiv"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("dataset"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("test"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("json\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Loading from local")]),t._v("\ntrain_ds "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Dataset"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_json"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"arxiv-dataset-train.json"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nvalid_ds "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Dataset"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_json"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"arxiv-dataset-validate.json"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntest_ds "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Dataset"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_json"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"arxiv-dataset-test.json"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("Let's have a look at the first 10 examples of the train dataset.")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("train_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("head"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",[s("style",{attrs:{scoped:""}},[t._v('\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\x3c!--beforebegin--\x3e<div class="language- extra-class">\x3c!--afterbegin--\x3e<pre><code>.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n</code></pre>\n\x3c!--beforeend--\x3e</div>\x3c!--afterend--\x3e')]),t._v(" "),s("table",{staticClass:"dataframe",attrs:{border:"1"}},[s("thead",[s("tr",{staticStyle:{"text-align":"right"}},[s("th"),t._v(" "),s("th",[t._v("title")]),t._v(" "),s("th",[t._v("categories")]),t._v(" "),s("th",[t._v("abstract")])])]),t._v(" "),s("tbody",[s("tr",[s("th",[t._v("0")]),t._v(" "),s("td",[t._v("Coherent structures in the wake of a SAE squar...")]),t._v(" "),s("td",[t._v("physics.flu-dyn")]),t._v(" "),s("td",[t._v("The wake of a SAE squareback vehicle model i...")])]),t._v(" "),s("tr",[s("th",[t._v("1")]),t._v(" "),s("td",[t._v("On the interaction of precipitates and tensile...")]),t._v(" "),s("td",[t._v("cond-mat.mtrl-sci")]),t._v(" "),s("td",[t._v("Although magnesium alloys deform extensively...")])]),t._v(" "),s("tr",[s("th",[t._v("2")]),t._v(" "),s("td",[t._v("Multi-domain Spectral Collocation Method for V...")]),t._v(" "),s("td",[t._v("math.NA")]),t._v(" "),s("td",[t._v("Spectral and spectral element methods using ...")])]),t._v(" "),s("tr",[s("th",[t._v("3")]),t._v(" "),s("td",[t._v("The min-max edge q-coloring problem")]),t._v(" "),s("td",[t._v("cs.DS")]),t._v(" "),s("td",[t._v("In this paper we introduce and study a new p...")])]),t._v(" "),s("tr",[s("th",[t._v("4")]),t._v(" "),s("td",[t._v("A Simple Model for a Dual Non-Abelian Monopole...")]),t._v(" "),s("td",[t._v("hep-th")]),t._v(" "),s("td",[t._v("We investigate the flux-tube joining two equ...")])]),t._v(" "),s("tr",[s("th",[t._v("5")]),t._v(" "),s("td",[t._v("A second moment bound for critical points of p...")]),t._v(" "),s("td",[t._v("math.PR")]),t._v(" "),s("td",[t._v("We consider the number of critical points of...")])]),t._v(" "),s("tr",[s("th",[t._v("6")]),t._v(" "),s("td",[t._v("$\\Sigma^0$ production in proton nucleus collis...")]),t._v(" "),s("td",[t._v("nucl-ex")]),t._v(" "),s("td",[t._v("The production of $\\Sigma^{0}$ baryons in th...")])]),t._v(" "),s("tr",[s("th",[t._v("7")]),t._v(" "),s("td",[t._v("Zero-temperature glass transition in two dimen...")]),t._v(" "),s("td",[t._v("cond-mat.stat-mech")]),t._v(" "),s("td",[t._v("The nature of the glass transition is theore...")])]),t._v(" "),s("tr",[s("th",[t._v("8")]),t._v(" "),s("td",[t._v("Measurement of the $B^0_s\\to\\mu^+\\mu^-$ branch...")]),t._v(" "),s("td",[t._v("hep-ex")]),t._v(" "),s("td",[t._v("A search for the rare decays $B^0_s\\to\\mu^+\\...")])]),t._v(" "),s("tr",[s("th",[t._v("9")]),t._v(" "),s("td",[t._v("STAR inner tracking upgrade - A performance study")]),t._v(" "),s("td",[t._v("nucl-ex")]),t._v(" "),s("td",[t._v("Anisotropic flow measurements have demonstra...")])])])])]),t._v(" "),s("p",[t._v("Our pipeline defined in the next section, or to be more precise the "),s("code",[t._v("TaskClassification")]),t._v(" task "),s("a",{attrs:{href:"https://www.recogn.ai/biome-text/documentation/basics.html#head",target:"_blank",rel:"noopener noreferrer"}},[t._v("head"),s("OutboundLink")],1),t._v(", will expect a "),s("em",[t._v("text")]),t._v(" and "),s("em",[t._v("label")]),t._v(" column to be present in our data.\nTherefore, we need to map our input to these two columns:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Renaming the 'categories' column into 'label'")]),t._v("\ntrain_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("rename_column_"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"categories"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"label"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nvalid_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("rename_column_"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"categories"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"label"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntest_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("rename_column_"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"categories"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"label"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Combining 'title' and 'abstract' into a 'text' column, and remove them afterwards")]),t._v("\ntrain_ds "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" train_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"text"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"title"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('" "')]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"abstract"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" remove_columns"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"title"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"abstract"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nvalid_ds "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" valid_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"text"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"title"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('" "')]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"abstract"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" remove_columns"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"title"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"abstract"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntest_ds "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" test_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"text"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"title"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('" "')]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"abstract"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" remove_columns"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"title"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"abstract"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h2",{attrs:{id:"configuring-and-training-the-pipeline"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#configuring-and-training-the-pipeline"}},[t._v("#")]),t._v(" Configuring and training the pipeline")]),t._v(" "),s("p",[t._v("As we have seen in "),s("a",{attrs:{href:"https://www.recogn.ai/biome-text/documentation/tutorials/1-Training_a_text_classifier.html#explore-the-training-data",target:"_blank",rel:"noopener noreferrer"}},[t._v("previous tutorials"),s("OutboundLink")],1),t._v(", a "),s("em",[t._v("biome.text")]),t._v(" "),s("a",{attrs:{href:"https://www.recogn.ai/biome-text/documentation/basics.html#pipeline",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("Pipeline")]),s("OutboundLink")],1),t._v(" consists of tokenizing the input, extracting text features, applying a language encoding (optionally) and executing a task-specific head in the end. In "),s("em",[t._v("biome.text")]),t._v(" the pre-trained transformers by Hugging Face are treated as a text feature, just like the "),s("em",[t._v("word")]),t._v(" and "),s("em",[t._v("char")]),t._v(" feature.")]),t._v(" "),s("p",[t._v("In this section we will configure and train 3 different pipelines to showcase the usage of transformers in "),s("em",[t._v("biome.text")]),t._v(".")]),t._v(" "),s("h3",{attrs:{id:"fine-tuning-the-transformer"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#fine-tuning-the-transformer"}},[t._v("#")]),t._v(" Fine-tuning the transformer")]),t._v(" "),s("p",[t._v("In our first pipeline we follow the common approach to use pretrained transformers in classification tasks. It consists of fine-tuning the transformer weights and using a special token as pooler in the end. In our configuration the former step means setting the "),s("code",[t._v("trainable")]),t._v(" parameter in the "),s("em",[t._v("transformers")]),t._v(" features to "),s("code",[t._v("True")]),t._v(". The downside of fine-tuning is that most of the pre-trained transformers are relatively big and require dedicated hardware to be fine-tuned. For example, in this tutorial we will use "),s("code",[t._v("distilroberta-base")]),t._v(", a "),s("a",{attrs:{href:"https://github.com/huggingface/transformers/tree/master/examples/distillation",target:"_blank",rel:"noopener noreferrer"}},[t._v("distilled version"),s("OutboundLink")],1),t._v(" of RoBERTa with a total of ~80M parameters.")]),t._v(" "),s("p",[t._v("We also need to specify the maximum number of input tokens "),s("code",[t._v("max_length")]),t._v(" supported by the pretrained transformer. If you are sure that your input data does not exceed this limit, you can skip this parameter.")]),t._v(" "),s("p",[t._v("With BERT-like models, such as RoBERTa, a special [CLS] token is added as first token to each input. It is pretrained to effectively represent the entire input and can be used as pooler in the head component. Many BERT like models pass this token through a non-linear tanh activation layer that is part of the pretraining. If you want to use these pretrained weights you have to use the "),s("code",[t._v("bert_pooler")]),t._v(" together with the corresponding "),s("code",[t._v("pretrained_model")]),t._v(". We will fine-tune these weights as well (setting "),s("code",[t._v("require_grad")]),t._v(" to "),s("code",[t._v("True")]),t._v(") and add a little dropout.")]),t._v(" "),s("div",{staticClass:"custom-block tip"},[s("p",{staticClass:"custom-block-title"},[t._v("Tip")]),t._v(" "),s("p",[t._v("You can also use the [CLS] token directly without passing it through the non-linear layer by using the "),s("code",[t._v("cls_pooler")]),t._v(".")])]),t._v(" "),s("p",[t._v("The "),s("code",[t._v("TextClassification")]),t._v(" head automatically applies a linear layer with an output dimension corresponding to the number of labels in the end.")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("pipeline_dict_finetuning "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"name"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"arxiv_categories_classification"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"features"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"transformers"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"model_name"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"distilroberta-base"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"trainable"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# freeze the weights of the transformer")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"max_length"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("512")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"head"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"TextClassification"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"labels"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" train_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("unique"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"label"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"pooler"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"bert_pooler"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"pretrained_model"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"distilroberta-base"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"requires_grad"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"dropout"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# If you do not want to use the pre-trained activation layer for the CLS token (see text) ")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v('# "pooler": {')]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v('#     "type": "cls_pooler",')]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# }")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("pl "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Pipeline"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_config"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pipeline_dict_finetuning"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("In our trainer configuration we will use canonical values for the "),s("code",[t._v("batch_size")]),t._v(" and "),s("code",[t._v("lr")]),t._v(" taken from the Hugging Face transformers library. We also will apply a linearly decaying learning rate scheduler with 50 warm-up steps, which is recommended when fine-tuning a pretrained model. For now we will stick to two epochs to allow for a rapid iteration.")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("trainer "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TrainerConfiguration"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    optimizer"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"adamw"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"lr"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5e")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    learning_rate_scheduler"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"linear_with_warmup"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"num_epochs"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"num_steps_per_epoch"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1250")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"warmup_steps"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("50")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    batch_size"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    num_epochs"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("pl"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    output"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"output/fine_tuning"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    training"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("train_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    validation"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("valid_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    trainer"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("trainer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("After two epochs we achieve an accuracy of about 0.65, which is competetive looking at the corresponding "),s("a",{attrs:{href:"https://www.kaggle.com/Cornell-University/arxiv/notebooks",target:"_blank",rel:"noopener noreferrer"}},[t._v("Kaggle notebooks"),s("OutboundLink")],1),t._v(". Keep in mind that we did not optimize any of the training parameters.")]),t._v(" "),s("h3",{attrs:{id:"training-with-a-frozen-transformer"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#training-with-a-frozen-transformer"}},[t._v("#")]),t._v(" Training with a frozen transformer")]),t._v(" "),s("p",[t._v("In our second pipeline we keep the weights of the transformer frozen by setting "),s("code",[t._v("trainable: False")]),t._v(" and only train the pooler in the head component. In this setup the training will be significantly faster and does not necessarily require dedicated hardware.")]),t._v(" "),s("p",[t._v("As pooler we will use a bidirectional "),s("a",{attrs:{href:"https://en.wikipedia.org/wiki/Gated_recurrent_unit",target:"_blank",rel:"noopener noreferrer"}},[t._v("GRU"),s("OutboundLink")],1),t._v(" in the head.")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("pipeline_dict_frozen "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"name"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"arxiv_categories_classification"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"features"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"transformers"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"model_name"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"distilroberta-base"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"trainable"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"max_length"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("512")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"head"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"TextClassification"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"labels"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" train_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("unique"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"label"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"pooler"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"gru"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"num_layers"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"hidden_size"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"bidirectional"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("pl "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Pipeline"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_config"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pipeline_dict_frozen"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("In our training configuration we will use the same "),s("code",[t._v("batch_size")]),t._v(" as in the previous configuration but increase the learning rate to Pytorch's default value for the AdamW optimizer, in order to work well with the GRU. We also remove the learning rate scheduler with its warmup steps, since we do not modify the pretrained transformer weights.")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("trainer "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TrainerConfiguration"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    optimizer"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"adamw"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"lr"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.002")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    batch_size"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    num_epochs"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("pl"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    output"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"output/frozen_transformer"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    training"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("train_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    validation"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("valid_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    trainer"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("trainer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("The training is about 4 times faster compared with fine-tuning the transformer, and after two epochs we reach a respectable accuracy of about 0.60. Keep in mind that we did not optimize any of the training parameters.")]),t._v(" "),s("h3",{attrs:{id:"combining-text-features"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#combining-text-features"}},[t._v("#")]),t._v(" Combining text features")]),t._v(" "),s("p",[t._v("As mentioned earlier, the pretrained transformers are treated as a text feature in "),s("em",[t._v("biome.text")]),t._v(". We can easily combine them with other features, such as the "),s("em",[t._v("char")]),t._v(" feature for example, which encodes word tokens based on their characters.")]),t._v(" "),s("p",[t._v("Keep in mind that the "),s("em",[t._v("char")]),t._v(" feature provides a feature vector per word (spaCy) token, while the "),s("em",[t._v("transformers")]),t._v(" feature provides a contextualized feature vector per word piece. Therefore, we simply sum up the word piece vectors of the transformers feature, to end up with concatenated feature vectors per word token.")]),t._v(" "),s("div",{staticClass:"custom-block warning"},[s("p",{staticClass:"custom-block-title"},[t._v("Note")]),t._v(" "),s("p",[t._v("This also means that special transformer tokens, such as BERT's [CLS] token, are ignored when combining text features.")])]),t._v(" "),s("p",[t._v("As in the second configuration, we will pool the feature vectors with a "),s("em",[t._v("GRU")]),t._v(" in the "),s("em",[t._v("head")]),t._v(" component.")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("pipeline_dict_combining "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"name"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"arxiv_categories_classification"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"tokenizer"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"features"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"char"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"embedding_dim"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("32")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"lowercase_characters"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"encoder"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"gru"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"num_layers"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"hidden_size"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("32")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"bidirectional"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"dropout"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"transformers"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"model_name"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"distilroberta-base"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"trainable"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"max_length"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("512")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"head"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"TextClassification"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"labels"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" train_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("unique"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"label"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"pooler"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"gru"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"num_layers"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"hidden_size"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"bidirectional"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("pl "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Pipeline"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_config"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pipeline_dict_combining"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("For the "),s("em",[t._v("char")]),t._v(" feature we have to manually create the vocab before the training.")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("pl"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("create_vocabulary"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("VocabularyConfiguration"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sources"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("train_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" valid_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("We will use the same training configuration as in the frozen transformer section.")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("trainer "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TrainerConfiguration"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    optimizer"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"adamw"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"lr"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.001")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    batch_size"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    num_epochs"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("pl"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    output"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"output/combined_features"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    training"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("train_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    validation"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("valid_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    trainer"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("trainer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("With an accuracy of 0.55, combining features in this case seems to be counterproductive. The main reason is the exclusion of the special transformers tokens and the usage of feature vectors per word instead of word-pieces. Even when fine-tuning the transformer, those differences seem to significantly affect the performance as shown in our "),s("a",{attrs:{href:"https://wandb.ai/ignacioct/biome/reports/Exploring-Ways-to-use-Pretrained-Transformers-in-biome-text--VmlldzoyNzk2MTM",target:"_blank",rel:"noopener noreferrer"}},[t._v("WandB report"),s("OutboundLink")],1),t._v(".")]),t._v(" "),s("h3",{attrs:{id:"compare-performances-with-tensorboard-optional"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#compare-performances-with-tensorboard-optional"}},[t._v("#")]),t._v(" Compare performances with TensorBoard (optional)")]),t._v(" "),s("p",[t._v("In the output folder of the trainig we automatically log the results with "),s("a",{attrs:{href:"https://www.tensorflow.org/tensorboard/",target:"_blank",rel:"noopener noreferrer"}},[t._v("TensorBoard"),s("OutboundLink")],1),t._v(". This helps us to conveniently compare the three training runs from above. Alternatively, if you installed and logged in to WandB, the runs should have been logged automatically to the "),s("em",[t._v("biome")]),t._v(" project of your account.")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v("load_ext tensorboard\n"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v("tensorboard "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("logdir"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("output\n")])])]),s("h2",{attrs:{id:"optimizing-the-trainer-configuration"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#optimizing-the-trainer-configuration"}},[t._v("#")]),t._v(" Optimizing the trainer configuration")]),t._v(" "),s("p",[t._v("As described in the "),s("a",{attrs:{href:"https://www.recogn.ai/biome-text/documentation/tutorials/3-Hyperparameter_optimization_with_Ray_Tune.html#imports",target:"_blank",rel:"noopener noreferrer"}},[t._v("HPO tutorial"),s("OutboundLink")],1),t._v(", "),s("em",[t._v("biome.text")]),t._v(" relies on the "),s("a",{attrs:{href:"https://docs.ray.io/en/latest/tune.html#tune-index",target:"_blank",rel:"noopener noreferrer"}},[t._v("Ray Tune library"),s("OutboundLink")],1),t._v(" to perform hyperparameter optimization.\nWe recommend to go through that tutorial first, as we will be skipping most of the implementation details here.")]),t._v(" "),s("h3",{attrs:{id:"frozen-transformer"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#frozen-transformer"}},[t._v("#")]),t._v(" Frozen transformer")]),t._v(" "),s("p",[t._v("In this section we will first try to improve the performance of the frozen-transformer configuration by conducting a random search for three training parameters:")]),t._v(" "),s("ul",[s("li",[t._v("learning rate")]),t._v(" "),s("li",[t._v("weight decay")]),t._v(" "),s("li",[t._v("batch size")])]),t._v(" "),s("p",[t._v("We also set "),s("code",[t._v("num_serialized_models_to_keep")]),t._v(" to 0 to reduce the disk storage footprint.")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("trainer_dict "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"optimizer"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"adamw"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"lr"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" tune"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loguniform"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5e")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5e")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"weight_decay"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" tune"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loguniform"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"batch_size"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" tune"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("choice"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("16")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"num_epochs"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"num_serialized_models_to_keep"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),s("p",[t._v("Having defined the search space for our hyperparameters, we create a "),s("code",[t._v("TuneExperiment")]),t._v(" where we specify the number of samples to be dranw from our search space, the "),s("code",[t._v("local_dir")]),t._v(" for our HPO output and the computing resources we want Ray Tune to have access to.")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("tune_exp "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TuneExperiment"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    pipeline_config"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("pipeline_dict_frozen"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n    trainer_config"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("trainer_dict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    train_dataset"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("train_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    valid_dataset"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("valid_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    name"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"frozen_transformer_sweep"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# parameters for tune.run")]),t._v("\n    num_samples"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("50")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    local_dir"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"tune_runs"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    resources_per_trial"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"gpu"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"cpu"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("With our TuneExperiment object at hand, we simply have to pass it on to the "),s("a",{attrs:{href:"https://docs.ray.io/en/master/tune/api_docs/execution.html#tune-run",target:"_blank",rel:"noopener noreferrer"}},[s("code",[t._v("tune.run")]),s("OutboundLink")],1),t._v(" function to start our random search.")]),t._v(" "),s("p",[t._v("To speed things up we will use the "),s("a",{attrs:{href:"https://blog.ml.cmu.edu/2018/12/12/massively-parallel-hyperparameter-optimization/",target:"_blank",rel:"noopener noreferrer"}},[t._v("ASHA"),s("OutboundLink")],1),t._v(" trial scheduler that terminates low performing trials early. In our case we take the "),s("em",[t._v("validation_accuracy")]),t._v(" as a meassure of the models performance.")]),t._v(" "),s("p",[t._v("In Google Colab with a GPU backend this random search should not take more than about 1.5 hours and we recommend following the progress via WandB. Alternatively, you could follow the progress via "),s("a",{attrs:{href:"https://www.tensorflow.org/tensorboard/",target:"_blank",rel:"noopener noreferrer"}},[t._v("TensorBoard"),s("OutboundLink")],1),t._v(" by launching a TensorBoard instance before starting the random search, and pointing it to the "),s("em",[t._v("local_dir")]),t._v(" output:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v("tensorboard "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("logdir"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("tune_runs\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("analysis_frozen "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tune"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("run"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    tune_exp"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    scheduler"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("tune"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("schedulers"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ASHAScheduler"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n    metric"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"validation_accuracy"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    mode"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"max"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    progress_reporter"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("tune"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("JupyterNotebookReporter"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("overwrite"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("The best configuration in our random search achieved an accuracy of about 0.63 with following parameters:")]),t._v(" "),s("ul",[s("li",[t._v("learning rate: 0.002541")]),t._v(" "),s("li",[t._v("batch size: 16")]),t._v(" "),s("li",[t._v("weight decay: 0.04194")])]),t._v(" "),s("h3",{attrs:{id:"fine-tuning-the-transformer-2"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#fine-tuning-the-transformer-2"}},[t._v("#")]),t._v(" Fine-tuning the transformer")]),t._v(" "),s("p",[t._v("We will also try to optimize the training parameters for a fine-tuning of the transformer. Since this is computationally much more expensive, we will take only a subset of our training data for the random search.")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("train_1000 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" train_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shuffle"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("seed"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("43")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("select"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1000")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("The training parameters we are going to tune are the following:")]),t._v(" "),s("ul",[s("li",[t._v("learning rate")]),t._v(" "),s("li",[t._v("weight decay")]),t._v(" "),s("li",[t._v("warmup steps")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("trainer_dict "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"optimizer"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"adamw"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"lr"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" tune"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loguniform"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"weight_decay"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" tune"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loguniform"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"learning_rate_scheduler"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"linear_with_warmup"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"num_epochs"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"num_steps_per_epoch"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("125")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"warmup_steps"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" tune"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("choice"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("101")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"batch_size"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"num_epochs"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"num_serialized_models_to_keep"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),s("p",[t._v("After having defined the search space, we create a "),s("code",[t._v("TuneExperiment")]),t._v(" providing this time the subset of the training data.")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("tune_exp "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TuneExperiment"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    pipeline_config"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("pipeline_dict_finetuning"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n    trainer_config"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("trainer_dict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    train_dataset"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("train_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    valid_dataset"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("valid_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    name"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"finetuning_sweep3"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# parameters for tune.run")]),t._v("\n    num_samples"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("50")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    local_dir"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"tune_runs"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    resources_per_trial"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"gpu"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"cpu"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("Again, we will use the "),s("a",{attrs:{href:"https://blog.ml.cmu.edu/2018/12/12/massively-parallel-hyperparameter-optimization/",target:"_blank",rel:"noopener noreferrer"}},[t._v("ASHA"),s("OutboundLink")],1),t._v(" trial scheduler and maximize the "),s("em",[t._v("validation_accuracy")]),t._v(".")]),t._v(" "),s("p",[t._v("In Google Colab with a GPU backend, this random search should not take longer than 1.5 hours.")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("analysis_finetuning "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tune"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("run"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    tune_exp"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    scheduler"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("tune"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("schedulers"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ASHAScheduler"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    metric"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"validation_accuracy"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    mode"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"max"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    progress_reporter"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("tune"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("JupyterNotebookReporter"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("overwrite"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("We now take the configuration that yielded the best "),s("em",[t._v("validation accuracy")]),t._v(" and train the pipeline on the full training set. In our random search the best configuration was following:")]),t._v(" "),s("ul",[s("li",[t._v("learning rate: 0.0000453")]),t._v(" "),s("li",[t._v("warmup steps: 45")]),t._v(" "),s("li",[t._v("weight decay: 0.003197")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("pl "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Pipeline"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_config"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pipeline_dict_finetuning"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("trainer "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TrainerConfiguration"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    optimizer"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"adamw"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"lr"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.0000453")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"weight_decay"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.003197")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    learning_rate_scheduler"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"linear_with_warmup"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"num_epochs"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"num_steps_per_epoch"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1250")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"warmup_steps"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("45")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    batch_size"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    num_epochs"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("pl"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    output"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"output/transformer_final_model/"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    training"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("train_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    validation"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("valid_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    trainer"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("trainer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("With the optimized training parameters we achieve an accuracy of about 0.67.")]),t._v(" "),s("h3",{attrs:{id:"evaluating-with-a-test-set"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#evaluating-with-a-test-set"}},[t._v("#")]),t._v(" Evaluating with a test set")]),t._v(" "),s("p",[t._v("Having optimized the training parameters of both models, we will now evaluate them on an independent test set.")]),t._v(" "),s("p",[t._v("For the frozen-transformer configuration we can use the "),s("code",[t._v("analysis_frozen")]),t._v(" object of the random search to directly access the best performing model:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("best_model_path "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" os"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("join"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("analysis_frozen"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get_best_logdir"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"training"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"model.tar.gz"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\npl_frozen "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Pipeline"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_pretrained"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("best_model_path"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("With the best performing pipeline at hand we will call its evaluate method together with the test data set. By default the evaluation will be done with a batch size of 16 and on a CUDA device if one is available. You can also specify a "),s("code",[t._v("predictions_output_file")]),t._v(" argument to save all the batch predictions made during evaluation.")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("pl_frozen"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("evaluate"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("test_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" predictions_output_file"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"predictions_from_evaluation.txt"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("On the test set we achieve an accuracy of about 0.65, which is a bit better than the 0.63 on our validation set.")]),t._v(" "),s("p",[t._v("Let us also quickly check the accuracy of our best fine-tuned model:")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("pl_finetuned "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Pipeline"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_pretrained"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"output/transformer_final_model/model.tar.gz"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\npl_finetuned"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("evaluate"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("test_ds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("Here we achieve roughly the same accuracy of 0.67 as with the validation data set. So it seems both models generalized well during the random search and there is no strong bias towards the validation data set.")]),t._v(" "),s("h2",{attrs:{id:"making-predictions"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#making-predictions"}},[t._v("#")]),t._v(" Making predictions")]),t._v(" "),s("p",[t._v("Let's quickly recap what we have learnt so far:")]),t._v(" "),s("ul",[s("li",[t._v("Freezing the pretrained transformer and optimizing a GRU pooler in the head can be valid option if computing resources are limited;")]),t._v(" "),s("li",[t._v('However, fine-tuning the transformer at word-piece level and using the CLS token as "pooler" works best;')]),t._v(" "),s("li",[t._v("A quick HPO of the training parameters improved the accuracies by ~0.03.")])]),t._v(" "),s("p",[t._v("With our best model at hand we will finally make a simple prediction. We can call the "),s("code",[t._v("predict")]),t._v(" method of our pipeline that outputs a dictionary with a labels and probabilities key containing a list of labels and their corresponding probabilities, ordered from most to less likely.")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("pl_finetuned"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("text"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"This is a title of a super intelligent Natural Language Processing system"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v('The most likely category predicted is the "'),s("em",[t._v("cs.CL")]),t._v('" category, which seems fitting according to this '),s("a",{attrs:{href:"https://arxiv.org/category_taxonomy",target:"_blank",rel:"noopener noreferrer"}},[t._v("list of arxiv categories and their meanings"),s("OutboundLink")],1),t._v(".")])])}),[],!1,null,null,null);a.default=e.exports}}]);