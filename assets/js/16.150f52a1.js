(window.webpackJsonp=window.webpackJsonp||[]).push([[16],{402:function(e,t,n){"use strict";n.r(t);var a=n(26),o=Object(a.a)({},(function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[n("h1",{attrs:{id:"biome-text-configuration"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#biome-text-configuration"}},[e._v("#")]),e._v(" biome.text.configuration "),n("Badge",{attrs:{text:"Module"}})],1),e._v(" "),n("dl",[n("h2",{attrs:{id:"biome.text.configuration.FeaturesConfiguration"}},[e._v("FeaturesConfiguration "),n("Badge",{attrs:{text:"Class"}})],1),e._v(" "),n("dt",[n("div",{staticClass:"language-python extra-class"},[n("pre",{staticClass:"language-python"},[e._v("    "),n("code",[e._v("\n"),n("span",{staticClass:"token keyword"},[e._v("class")]),e._v(" "),n("span",{staticClass:"ident"},[e._v("FeaturesConfiguration")]),e._v(" ("),e._v("\n    "),n("span",[e._v("words: Union[Dict[str, Any], NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("chars: Union[Dict[str, Any], NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("**extra_params")]),n("span",[e._v(",")]),e._v("\n"),n("span",[e._v(")")]),e._v("\n    ")])])])]),e._v(" "),n("dd",[n("div",{staticClass:"desc"},[n("p",[e._v("Creates a input featurizer configuration")]),e._v(" "),n("p",[e._v("This class will create a configuration for the features of the "),n("code",[e._v("Pipeline")]),e._v(".")]),e._v(" "),n("p",[e._v("Use this for defining the main features to be used by the model, namely word and character embeddings.")]),e._v(" "),n("p",[e._v(":::tip\nIf you do not pass "),n("code",[e._v("words")]),e._v(" and "),n("code",[e._v("chars")]),e._v(" your pipeline will be setup with default word features (embedding_dim=50).\n:::")]),e._v(" "),n("p",[e._v("Example:")]),e._v(" "),n("pre",[n("code",{staticClass:"python"},[e._v("words = {'embedding_dim': 100}\nchars = {'embedding_dim': 16, 'encoder': {'type': 'gru'}}\nconfig = FeaturesConfiguration(words,chars)\n")])]),e._v(" "),n("h2",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),n("dl",[n("dt",[n("strong",[n("code",[e._v("words")])]),e._v(" : "),n("code",[e._v("Dict[str, Any]")])]),e._v(" "),n("dd",[e._v(" ")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("chars")])]),e._v(" : "),n("code",[e._v("Dict[str, Any]")])]),e._v(" "),n("dd",[e._v(" ")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("extra_params")])])]),e._v(" "),n("dd",[e._v(" ")])])]),e._v(" "),n("h3",[e._v("Ancestors")]),e._v(" "),n("ul",{staticClass:"hlist"},[n("li",[e._v("allennlp.common.from_params.FromParams")])]),e._v(" "),n("dl",[n("h3",{attrs:{id:"biome.text.configuration.FeaturesConfiguration.from_params"}},[e._v("from_params "),n("Badge",{attrs:{text:"Static method"}})],1),e._v(" "),n("dt",[n("div",{staticClass:"language-python extra-class"},[n("pre",{staticClass:"language-python"},[n("code",[e._v("\n"),n("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),n("span",{staticClass:"ident"},[e._v("from_params")]),e._v(" ("),e._v("\n   params: allennlp.common.params.Params,\n   **extras,\n)  -> "),n("a",{attrs:{title:"biome.text.configuration.FeaturesConfiguration",href:"#biome.text.configuration.FeaturesConfiguration"}},[e._v("FeaturesConfiguration")]),e._v("\n")]),e._v("\n        ")])])]),e._v(" "),n("dd",[n("div",{staticClass:"desc"},[n("p",[e._v("This is the automatic implementation of "),n("code",[e._v("from_params")]),e._v(". Any class that subclasses "),n("code",[e._v("FromParams")]),e._v("\n(or "),n("code",[e._v("Registrable")]),e._v(", which itself subclasses "),n("code",[e._v("FromParams")]),e._v(') gets this implementation for free.\nIf you want your class to be instantiated from params in the "obvious" way – pop off parameters\nand hand them to your constructor with the same names – this provides that functionality.')]),e._v(" "),n("p",[e._v("If you need more complex logic in your from "),n("code",[e._v("from_params")]),e._v(" method, you'll have to implement\nyour own method that overrides this one.")])])])]),e._v(" "),n("dl",[n("h3",{attrs:{id:"biome.text.configuration.FeaturesConfiguration.compile"}},[e._v("compile "),n("Badge",{attrs:{text:"Method"}})],1),e._v(" "),n("dt",[n("div",{staticClass:"language-python extra-class"},[n("pre",{staticClass:"language-python"},[n("code",[e._v("\n"),n("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),n("span",{staticClass:"ident"},[e._v("compile")]),e._v("("),n("span",[e._v("self) -> "),n("a",{attrs:{title:"biome.text.featurizer.InputFeaturizer",href:"featurizer.html#biome.text.featurizer.InputFeaturizer"}},[e._v("InputFeaturizer")])]),e._v("\n")]),e._v("\n        ")])])]),e._v(" "),n("dd",[n("div",{staticClass:"desc"},[n("p",[e._v("Creates a featurizer from the configuration object")]),e._v(" "),n("p",[e._v(":::tip")]),e._v(" "),n("p",[e._v("If you are creating configurations programmatically use this method to check that your config object contains\na valid configuration.")]),e._v(" "),n("p",[e._v(":::")]),e._v(" "),n("h2",{attrs:{id:"returns"}},[e._v("Returns")]),e._v(" "),n("p",[e._v("The configured "),n("code",[e._v("InputFeaturizer")])])])])])]),e._v(" "),n("h2",{attrs:{id:"biome.text.configuration.TokenizerConfiguration"}},[e._v("TokenizerConfiguration "),n("Badge",{attrs:{text:"Class"}})],1),e._v(" "),n("dt",[n("div",{staticClass:"language-python extra-class"},[n("pre",{staticClass:"language-python"},[e._v("    "),n("code",[e._v("\n"),n("span",{staticClass:"token keyword"},[e._v("class")]),e._v(" "),n("span",{staticClass:"ident"},[e._v("TokenizerConfiguration")]),e._v(" ("),e._v("\n    "),n("span",[e._v("lang: str = 'en'")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("skip_empty_tokens: bool = False")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("max_sequence_length: int = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("max_nr_of_sentences: int = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("text_cleaning: Union[Dict[str, Any], NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("segment_sentences: Union[bool, Dict[str, Any]] = False")]),n("span",[e._v(",")]),e._v("\n"),n("span",[e._v(")")]),e._v("\n    ")])])])]),e._v(" "),n("dd",[n("div",{staticClass:"desc"},[n("p",[e._v("Creates a "),n("code",[e._v("Tokenizer")]),e._v(" configuration")]),e._v(" "),n("h2",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),n("dl",[n("dt",[n("strong",[n("code",[e._v("lang")])])]),e._v(" "),n("dd",[e._v(" ")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("skip_empty_tokens")])])]),e._v(" "),n("dd",[e._v(" ")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("max_sequence_length")])])]),e._v(" "),n("dd",[e._v(" ")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("max_nr_of_sentences")])])]),e._v(" "),n("dd",[e._v(" ")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("text_cleaning")])])]),e._v(" "),n("dd",[e._v(" ")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("segment_sentences")])])]),e._v(" "),n("dd",[e._v(" ")])])]),e._v(" "),n("h3",[e._v("Ancestors")]),e._v(" "),n("ul",{staticClass:"hlist"},[n("li",[e._v("allennlp.common.from_params.FromParams")])]),e._v(" "),n("dl",[n("h3",{attrs:{id:"biome.text.configuration.TokenizerConfiguration.compile"}},[e._v("compile "),n("Badge",{attrs:{text:"Method"}})],1),e._v(" "),n("dt",[n("div",{staticClass:"language-python extra-class"},[n("pre",{staticClass:"language-python"},[n("code",[e._v("\n"),n("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),n("span",{staticClass:"ident"},[e._v("compile")]),e._v("("),n("span",[e._v("self) -> "),n("a",{attrs:{title:"biome.text.tokenizer.Tokenizer",href:"tokenizer.html#biome.text.tokenizer.Tokenizer"}},[e._v("Tokenizer")])]),e._v("\n")]),e._v("\n        ")])])]),e._v(" "),n("dd",[n("div",{staticClass:"desc"},[n("p",[e._v("Build tokenizer object from its configuration")])])])])]),e._v(" "),n("h2",{attrs:{id:"biome.text.configuration.PipelineConfiguration"}},[e._v("PipelineConfiguration "),n("Badge",{attrs:{text:"Class"}})],1),e._v(" "),n("dt",[n("div",{staticClass:"language-python extra-class"},[n("pre",{staticClass:"language-python"},[e._v("    "),n("code",[e._v("\n"),n("span",{staticClass:"token keyword"},[e._v("class")]),e._v(" "),n("span",{staticClass:"ident"},[e._v("PipelineConfiguration")]),e._v(" ("),e._v("\n    "),n("span",[e._v("name: str")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("features: "),n("a",{attrs:{title:"biome.text.configuration.FeaturesConfiguration",href:"#biome.text.configuration.FeaturesConfiguration"}},[e._v("FeaturesConfiguration")])]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("head: "),n("a",{attrs:{title:"biome.text.modules.heads.defs.TaskHeadSpec",href:"modules/heads/defs.html#biome.text.modules.heads.defs.TaskHeadSpec"}},[e._v("TaskHeadSpec")])]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("tokenizer: Union[biome.text.configuration.TokenizerConfiguration, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("encoder: Union[biome.text.modules.specs.allennlp_specs.Seq2SeqEncoderSpec, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n"),n("span",[e._v(")")]),e._v("\n    ")])])])]),e._v(" "),n("dd",[n("div",{staticClass:"desc"},[n("p",[e._v('"Creates a '),n("code",[e._v("Pipeline")]),e._v(" configuration")]),e._v(" "),n("h2",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),n("dl",[n("dt",[n("strong",[n("code",[e._v("name")])]),e._v(" : "),n("code",[e._v("str")])]),e._v(" "),n("dd",[e._v("The "),n("code",[e._v("name")]),e._v(" for our pipeline")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("features")])]),e._v(" : "),n("code",[n("a",{attrs:{title:"biome.text.configuration.FeaturesConfiguration",href:"#biome.text.configuration.FeaturesConfiguration"}},[e._v("FeaturesConfiguration")])])]),e._v(" "),n("dd",[e._v("The input "),n("code",[e._v("features")]),e._v(" to be used by the model pipeline. We define this using a "),n("code",[n("a",{attrs:{title:"biome.text.configuration.FeaturesConfiguration",href:"#biome.text.configuration.FeaturesConfiguration"}},[e._v("FeaturesConfiguration")])]),e._v(" object.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("head")])]),e._v(" : "),n("code",[e._v("TaskHeadSpec")])]),e._v(" "),n("dd",[e._v("The "),n("code",[e._v("head")]),e._v(" for the task, e.g., a LanguageModelling task, using a "),n("code",[e._v("TaskHeadSpec")]),e._v(" object.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("tokenizer")])]),e._v(" : "),n("code",[e._v("TokenizerConfiguration")]),e._v(", optional")]),e._v(" "),n("dd",[e._v("The "),n("code",[e._v("tokenizer")]),e._v(" defined with a "),n("code",[n("a",{attrs:{title:"biome.text.configuration.TokenizerConfiguration",href:"#biome.text.configuration.TokenizerConfiguration"}},[e._v("TokenizerConfiguration")])]),e._v(" object.")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("encoder")])]),e._v(" : "),n("code",[e._v("Seq2SeqEncoderSpec")])]),e._v(" "),n("dd",[e._v("The core text seq2seq "),n("code",[e._v("encoder")]),e._v(" of our model using a "),n("code",[e._v("Seq2SeqEncoderSpec")])])])]),e._v(" "),n("h3",[e._v("Ancestors")]),e._v(" "),n("ul",{staticClass:"hlist"},[n("li",[e._v("allennlp.common.from_params.FromParams")])]),e._v(" "),n("dl",[n("h3",{attrs:{id:"biome.text.configuration.PipelineConfiguration.as_dict"}},[e._v("as_dict "),n("Badge",{attrs:{text:"Method"}})],1),e._v(" "),n("dt",[n("div",{staticClass:"language-python extra-class"},[n("pre",{staticClass:"language-python"},[n("code",[e._v("\n"),n("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),n("span",{staticClass:"ident"},[e._v("as_dict")]),e._v("("),n("span",[e._v("self) -> Dict[str, Any]")]),e._v("\n")]),e._v("\n        ")])])]),e._v(" "),n("dd",[n("div",{staticClass:"desc"})])])]),e._v(" "),n("h2",{attrs:{id:"biome.text.configuration.TrainerConfiguration"}},[e._v("TrainerConfiguration "),n("Badge",{attrs:{text:"Class"}})],1),e._v(" "),n("dt",[n("div",{staticClass:"language-python extra-class"},[n("pre",{staticClass:"language-python"},[e._v("    "),n("code",[e._v("\n"),n("span",{staticClass:"token keyword"},[e._v("class")]),e._v(" "),n("span",{staticClass:"ident"},[e._v("TrainerConfiguration")]),e._v(" ("),e._v("\n    "),n("span",[e._v("optimizer: Dict[str, Any]")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("validation_metric: str = '-loss'")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("patience: Union[int, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("shuffle: bool = True")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("num_epochs: int = 20")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("cuda_device: int = -1")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("grad_norm: Union[float, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("grad_clipping: Union[float, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("learning_rate_scheduler: Union[Dict[str, Any], NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("momentum_scheduler: Union[Dict[str, Any], NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("moving_average: Union[Dict[str, Any], NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("batch_size: Union[int, NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("cache_instances: bool = True")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("in_memory_batches: int = 2")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("data_bucketing: bool = True")]),n("span",[e._v(",")]),e._v("\n"),n("span",[e._v(")")]),e._v("\n    ")])])])]),e._v(" "),n("dd",[n("div",{staticClass:"desc"},[n("p",[e._v("Creates a "),n("code",[n("a",{attrs:{title:"biome.text.configuration.TrainerConfiguration",href:"#biome.text.configuration.TrainerConfiguration"}},[e._v("TrainerConfiguration")])])]),e._v(" "),n("h2",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),n("dl",[n("dt",[n("strong",[n("code",[e._v("optimizer")])])]),e._v(" "),n("dd",[e._v(" ")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("validation_metric")])])]),e._v(" "),n("dd",[e._v(" ")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("patience")])])]),e._v(" "),n("dd",[e._v(" ")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("shuffle")])])]),e._v(" "),n("dd",[e._v(" ")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("num_epochs")])])]),e._v(" "),n("dd",[e._v(" ")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("cuda_device")])])]),e._v(" "),n("dd",[e._v(" ")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("grad_norm")])])]),e._v(" "),n("dd",[e._v(" ")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("grad_clipping")])])]),e._v(" "),n("dd",[e._v(" ")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("learning_rate_scheduler")])])]),e._v(" "),n("dd",[e._v(" ")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("momentum_scheduler")])])]),e._v(" "),n("dd",[e._v(" ")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("moving_average")])])]),e._v(" "),n("dd",[e._v(" ")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("batch_size")])])]),e._v(" "),n("dd",[e._v(" ")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("cache_instances")])])]),e._v(" "),n("dd",[e._v(" ")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("in_memory_batches")])])]),e._v(" "),n("dd",[e._v(" ")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("data_bucketing")])])]),e._v(" "),n("dd",[e._v(" ")])])])]),e._v(" "),n("h2",{attrs:{id:"biome.text.configuration.VocabularyConfiguration"}},[e._v("VocabularyConfiguration "),n("Badge",{attrs:{text:"Class"}})],1),e._v(" "),n("dt",[n("div",{staticClass:"language-python extra-class"},[n("pre",{staticClass:"language-python"},[e._v("    "),n("code",[e._v("\n"),n("span",{staticClass:"token keyword"},[e._v("class")]),e._v(" "),n("span",{staticClass:"ident"},[e._v("VocabularyConfiguration")]),e._v(" ("),e._v("\n    "),n("span",[e._v("sources: List[str]")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("min_count: Dict[str, int] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("max_vocab_size: Union[int, Dict[str, int]] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("pretrained_files: Union[Dict[str, str], NoneType] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("only_include_pretrained_words: bool = False")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("tokens_to_add: Dict[str, List[str]] = None")]),n("span",[e._v(",")]),e._v("\n    "),n("span",[e._v("min_pretrained_embeddings: Dict[str, int] = None")]),n("span",[e._v(",")]),e._v("\n"),n("span",[e._v(")")]),e._v("\n    ")])])])]),e._v(" "),n("dd",[n("div",{staticClass:"desc"},[n("p",[e._v("Configures a "),n("code",[e._v("Vocabulary")]),e._v(" before it gets created from data")]),e._v(" "),n("p",[e._v("Use this to configure a Vocabulary using specific arguments from `allennlp.data.Vocabulary``")]),e._v(" "),n("p",[e._v("See "),n("a",{attrs:{href:"https://docs.allennlp.org/master/api/data/vocabulary/#vocabulary]"}},[e._v("AllenNLP Vocabulary docs")])]),e._v(" "),n("h2",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),n("dl",[n("dt",[n("strong",[n("code",[e._v("sources")])]),e._v(" : "),n("code",[e._v("List[str]")])]),e._v(" "),n("dd",[e._v("Datasource paths to be used for data creation")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("min_count")])]),e._v(" : "),n("code",[e._v("Dict[str, int]")]),e._v(", optional "),n("code",[e._v("(default=None)")])]),e._v(" "),n("dd",[e._v("Minimum number of appearances of a token to be included in the vocabulary")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("max_vocab_size")])]),e._v(" : "),n("code"),e._v("Union[int, Dict[str, int]]"),n("code",[n("code",[e._v(", optional </code>(default=<code>None</code>)")])])]),e._v(" "),n("dd",[e._v("Maximum number of tokens of the vocabulary")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("pretrained_files")])]),e._v(" : "),n("code",[e._v("Optional[Dict[str, str]]")]),e._v(", optional")]),e._v(" "),n("dd",[e._v("Pretrained files with word vectors")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("only_include_pretrained_words")])]),e._v(" : "),n("code",[e._v("bool")]),e._v(", optional "),n("code",[e._v("(default=False)")])]),e._v(" "),n("dd",[e._v("Only include tokens present in pretrained_files")]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("tokens_to_add")])]),e._v(" : "),n("code",[e._v("Dict[str, int]")]),e._v(", optional")]),e._v(" "),n("dd",[e._v("A list of tokens to add to the vocabulary, even if they are not present in the "),n("code",[e._v("sources")])]),e._v(" "),n("dt",[n("strong",[n("code",[e._v("min_pretrained_embeddings")])]),e._v(" : "),n("code",[e._v("Dict[str, int]")]),e._v(", optional")]),e._v(" "),n("dd",[e._v("Minimum number of lines to keep from pretrained_files, even for tokens not appearing in the sources.")])])])])])])}),[],!1,null,null,null);t.default=o.exports}}]);