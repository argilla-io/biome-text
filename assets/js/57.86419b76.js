(window.webpackJsonp=window.webpackJsonp||[]).push([[57],{398:function(t,a,e){"use strict";e.r(a);var s=e(26),n=Object(s.a)({},(function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("h1",{attrs:{id:"training-a-short-text-classifier-of-german-business-names"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#training-a-short-text-classifier-of-german-business-names"}},[t._v("#")]),t._v(" Training a short text classifier of German business names")]),t._v(" "),e("table",[e("td",[e("a",{attrs:{target:"_blank",href:"https://www.recogn.ai/biome-text/documentation/tutorials/Training_a_text_classifier.html"}},[e("img",{attrs:{src:"https://www.recogn.ai/biome-text/assets/img/biome-isotype.svg",width:"32"}}),t._v("View on recogn.ai")])]),t._v(" "),e("td",[e("a",{attrs:{target:"_blank",href:"https://colab.research.google.com/github/recognai/biome-text/blob/master/docs/docs/documentation/tutorials/Training_a_text_classifier.ipynb"}},[e("img",{attrs:{src:"https://www.tensorflow.org/images/colab_logo_32px.png"}}),t._v("Run in Google Colab")])]),t._v(" "),e("td",[e("a",{attrs:{target:"_blank",href:"https://github.com/recognai/biome-text/blob/master/docs/docs/documentation/tutorials/Training_a_text_classifier.ipynb"}},[e("img",{attrs:{src:"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png",width:"32"}}),t._v("View source on GitHub")])])]),t._v(" "),e("p",[t._v("In this tutorial we will train a basic short-text classifier for predicting the sector of a business based only on its business name.\nFor this we will use a training dataset with business names and business categories in German.")]),t._v(" "),e("p",[t._v("When running this tutorial in Google Colab, make sure to install "),e("em",[t._v("biome.text")]),t._v(" first:")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("!pip install "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("U git"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v("https"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("//")]),t._v("github"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("com"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("recognai"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("biome"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("text"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("git\n")])])]),e("p",[t._v("Ignore warnings and don't forget to restart your runtime afterwards ("),e("em",[t._v("Runtime -> Restart runtime")]),t._v(").")]),t._v(" "),e("h2",{attrs:{id:"explore-the-training-data"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#explore-the-training-data"}},[t._v("#")]),t._v(" Explore the training data")]),t._v(" "),e("p",[t._v("Let's take a look at the data we will use for training. For this we create a "),e("code",[t._v("DataSource")]),t._v(" instance providing a path to our data.")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" biome"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("data "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" DataSource\n")])])]),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("train_ds "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" DataSource"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"https://biome-tutorials-data.s3-eu-west-1.amazonaws.com/text_classifier/business.cat.train.csv"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntrain_ds"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("head"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("div",[e("style",{attrs:{scoped:""}},[t._v('\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\x3c!--beforebegin--\x3e<div class="language- extra-class">\x3c!--afterbegin--\x3e<pre><code>.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n</code></pre>\n\x3c!--beforeend--\x3e</div>\x3c!--afterend--\x3e')]),t._v(" "),e("table",{staticClass:"dataframe",attrs:{border:"1"}},[e("thead",[e("tr",{staticStyle:{"text-align":"right"}},[e("th"),t._v(" "),e("th",[t._v("label")]),t._v(" "),e("th",[t._v("text")]),t._v(" "),e("th",[t._v("path")])])]),t._v(" "),e("tbody",[e("tr",[e("th",[t._v("0")]),t._v(" "),e("td",[t._v("Edv")]),t._v(" "),e("td",[t._v("Cse Gmbh Computer Edv-service Bürobedarf")]),t._v(" "),e("td",[t._v("https://biome-tutorials-data.s3-eu-west-1.amaz...")])]),t._v(" "),e("tr",[e("th",[t._v("1")]),t._v(" "),e("td",[t._v("Maler")]),t._v(" "),e("td",[t._v("Malerfachbetrieb U. Nee")]),t._v(" "),e("td",[t._v("https://biome-tutorials-data.s3-eu-west-1.amaz...")])]),t._v(" "),e("tr",[e("th",[t._v("2")]),t._v(" "),e("td",[t._v("Gebrauchtwagen")]),t._v(" "),e("td",[t._v("Sippl Automobilverkäufer Hausmann")]),t._v(" "),e("td",[t._v("https://biome-tutorials-data.s3-eu-west-1.amaz...")])]),t._v(" "),e("tr",[e("th",[t._v("3")]),t._v(" "),e("td",[t._v("Handelsvermittler Und -vertreter")]),t._v(" "),e("td",[t._v("Strenge Handelsagentur Werth")]),t._v(" "),e("td",[t._v("https://biome-tutorials-data.s3-eu-west-1.amaz...")])]),t._v(" "),e("tr",[e("th",[t._v("4")]),t._v(" "),e("td",[t._v("Gebrauchtwagen")]),t._v(" "),e("td",[t._v("Dzengel Autohaus Gordemitz Rusch")]),t._v(" "),e("td",[t._v("https://biome-tutorials-data.s3-eu-west-1.amaz...")])]),t._v(" "),e("tr",[e("th",[t._v("5")]),t._v(" "),e("td",[t._v("Apotheken")]),t._v(" "),e("td",[t._v("Schinkel-apotheke Bitzer")]),t._v(" "),e("td",[t._v("https://biome-tutorials-data.s3-eu-west-1.amaz...")])]),t._v(" "),e("tr",[e("th",[t._v("6")]),t._v(" "),e("td",[t._v("Tiefbau")]),t._v(" "),e("td",[t._v("Franz Möbius Mehrings-bau-hude Und Stigge")]),t._v(" "),e("td",[t._v("https://biome-tutorials-data.s3-eu-west-1.amaz...")])]),t._v(" "),e("tr",[e("th",[t._v("7")]),t._v(" "),e("td",[t._v("Handelsvermittler Und -vertreter")]),t._v(" "),e("td",[t._v("Kontze Hdl.vertr. Lau")]),t._v(" "),e("td",[t._v("https://biome-tutorials-data.s3-eu-west-1.amaz...")])]),t._v(" "),e("tr",[e("th",[t._v("8")]),t._v(" "),e("td",[t._v("Autowerkstätten")]),t._v(" "),e("td",[t._v("Keßler Kfz-handel")]),t._v(" "),e("td",[t._v("https://biome-tutorials-data.s3-eu-west-1.amaz...")])]),t._v(" "),e("tr",[e("th",[t._v("9")]),t._v(" "),e("td",[t._v("Gebrauchtwagen")]),t._v(" "),e("td",[t._v("Diko Lack Und Schrift Betriebsteil Der Autocen...")]),t._v(" "),e("td",[t._v("https://biome-tutorials-data.s3-eu-west-1.amaz...")])])])])]),t._v(" "),e("p",[t._v("As we can see we have two relevant columns "),e("em",[t._v("label")]),t._v(" and "),e("em",[t._v("text")]),t._v(". The "),e("em",[t._v("path")]),t._v(" column is added automatically by the "),e("code",[t._v("DataSource")]),t._v(" class to keep track of the source file.")]),t._v(" "),e("p",[t._v("Our classifier will be trained to predict the "),e("em",[t._v("label")]),t._v(" given a "),e("em",[t._v("text")]),t._v(".")]),t._v(" "),e("p",[t._v("The "),e("code",[t._v("DataSource")]),t._v(" class stores the data in an underlying "),e("a",{attrs:{href:"https://docs.dask.org/en/latest/dataframe.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("Dask DataFrame"),e("OutboundLink")],1),t._v(" that you can easily access.\nFor example, let's check the size of our training data:")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train_ds"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_dataframe"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("div",{staticClass:"language- extra-class"},[e("pre",[e("code",[t._v("8000\n")])])]),e("p",[t._v("Or let's check the distribution of our labels:")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("labels "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" train_ds"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_dataframe"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("label"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("compute"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nlabels"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("value_counts"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("div",{staticClass:"language- extra-class"},[e("pre",[e("code",[t._v("Unternehmensberatungen              632\nFriseure                            564\nTiefbau                             508\nDienstleistungen                    503\nGebrauchtwagen                      449\nElektriker                          430\nRestaurants                         422\nArchitekturbüros                    417\nVereine                             384\nVersicherungsvermittler             358\nMaler                               330\nSanitärinstallationen               323\nEdv                                 318\nWerbeagenturen                      294\nApotheken                           289\nPhysiotherapie                      286\nVermittlungen                       277\nHotels                              274\nAutowerkstätten                     263\nElektrotechnik                      261\nAllgemeinärzte                      216\nHandelsvermittler Und -vertreter    202\nName: label, dtype: int64\n")])])]),e("div",{staticClass:"custom-block tip"},[e("p",{staticClass:"custom-block-title"},[t._v("TIP")]),t._v(" "),e("p",[t._v("The "),e("RouterLink",{attrs:{to:"/api/biome/text/modules/heads/task_head.html#taskhead"}},[t._v("TaskHead")]),t._v(" of our model will expect a "),e("em",[t._v("text")]),t._v(" and a "),e("em",[t._v("label")]),t._v(" column to be present in the dataframe.\nSince they are already present, there is no need for a "),e("RouterLink",{attrs:{to:"/api/biome/text/data/datasource.html#datasource"}},[t._v("mapping")]),t._v(" in the "),e("code",[t._v("DataSource")]),t._v(".")],1)]),t._v(" "),e("h2",{attrs:{id:"configure-your-biome-text-pipeline"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#configure-your-biome-text-pipeline"}},[t._v("#")]),t._v(" Configure your "),e("code",[t._v("biome.text")]),t._v(" Pipeline")]),t._v(" "),e("p",[t._v("A typical "),e("RouterLink",{attrs:{to:"/api/biome/text/pipeline.html#pipeline"}},[t._v("Pipeline")]),t._v(" consists of tokenizing the input, extracting features, applying a language encoding (optionally) and executing a task-specific head in the end.")],1),t._v(" "),e("p",[t._v("After training a pipeline, you can use it to make predictions or explore the underlying model via the "),e("RouterLink",{attrs:{to:"/documentation/user-guides/02.explore.html"}},[t._v("UI")]),t._v(".")],1),t._v(" "),e("p",[t._v("As a first step we must define a configuration for our pipeline.\nIn this tutorial we will create a configuration dictionary and use the "),e("code",[t._v("Pipeline.from_config()")]),t._v(" method to create our pipeline, but there are "),e("RouterLink",{attrs:{to:"/api/biome/text/pipeline.html#pipeline"}},[t._v("other ways")]),t._v(".")],1),t._v(" "),e("p",[t._v("A "),e("code",[t._v("biome.text")]),t._v(" pipeline has the following main components:")]),t._v(" "),e("div",{staticClass:"language-yaml extra-class"},[e("pre",{pre:!0,attrs:{class:"language-yaml"}},[e("code",[e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("name")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# a descriptive name of your pipeline")]),t._v("\n\n"),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("tokenizer")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# how to tokenize the input")]),t._v("\n\n"),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("features")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# input features of the model")]),t._v("\n\n"),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("encoder")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# the language encoder")]),t._v("\n\n"),e("span",{pre:!0,attrs:{class:"token key atrule"}},[t._v("head")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# your task configuration")]),t._v("\n\n")])])]),e("p",[t._v("See the "),e("RouterLink",{attrs:{to:"/documentation/user-guides/05.configuration.html"}},[t._v("Configuration section")]),t._v(" for a detailed description of how these main components can be configured.")],1),t._v(" "),e("p",[t._v("Our complete configuration for this tutorial will be following:")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("pipeline_dict "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"name"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"german_business_names"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"tokenizer"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"text_cleaning"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"rules"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"strip_spaces"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"features"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"word"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"embedding_dim"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("64")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"lowercase_tokens"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"char"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"embedding_dim"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("32")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"lowercase_characters"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"encoder"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n                "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"gru"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"num_layers"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"hidden_size"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("32")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"bidirectional"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"dropout"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"head"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"TextClassification"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"labels"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("labels"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("value_counts"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("index"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"pooler"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"gru"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"num_layers"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"hidden_size"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("32")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"bidirectional"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"feedforward"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"num_layers"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"hidden_dims"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("32")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"activations"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"relu"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"dropout"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.0")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("       \n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),e("p",[t._v("With this dictionary we can now create a "),e("code",[t._v("Pipeline")]),t._v(":")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" biome"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Pipeline\n")])])]),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("pl "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Pipeline"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_config"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pipeline_dict"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("h2",{attrs:{id:"create-a-vocabulary"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#create-a-vocabulary"}},[t._v("#")]),t._v(" Create a vocabulary")]),t._v(" "),e("p",[t._v("Before we can start the training we need to create the vocabulary for our model.\nFor this we define a "),e("code",[t._v("VocabularyConfiguration")]),t._v(".")]),t._v(" "),e("p",[t._v('In our business name classifier we only want to include words with a general meaning to our word feature vocabulary (like "Computer" or "Autohaus", for example), and want to exclude specific names that will not help to generally classify the kind of business.\nThis can be achieved by including only the most frequent words in our training set via the '),e("code",[t._v("min_count")]),t._v(" argument. For a complete list of available arguments see the "),e("RouterLink",{attrs:{to:"/api/biome/text/configuration.html#vocabularyconfiguration"}},[t._v("VocabularyConfiguration API")]),t._v(".")],1),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" biome"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("configuration "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" VocabularyConfiguration"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" WordFeatures\n")])])]),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("vocab_config "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" VocabularyConfiguration"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sources"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("train_ds"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" min_count"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("WordFeatures"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("namespace"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("p",[t._v("We then pass this configuration to our "),e("code",[t._v("Pipeline")]),t._v(" to create the vocabulary:")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("pl"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("create_vocabulary"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("vocab_config"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("div",{staticClass:"language- extra-class"},[e("pre",[e("code",[t._v("HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))\n")])])]),e("p",[t._v("After creating the vocabulary we can check the size of our entire model in terms of trainable parameters:")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("pl"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("trainable_parameters\n")])])]),e("div",{staticClass:"language- extra-class"},[e("pre",[e("code",[t._v("60566\n")])])]),e("h2",{attrs:{id:"configure-the-trainer"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#configure-the-trainer"}},[t._v("#")]),t._v(" Configure the trainer")]),t._v(" "),e("p",[t._v("As a next step we have to configure the "),e("em",[t._v("trainer")]),t._v(".")]),t._v(" "),e("p",[t._v("The default trainer has sensible defaults and should work alright for most of your cases.\nIn this tutorial, however, we want to tune a bit the learning rate and limit the training time to three epochs only.\nFor a complete list of available arguments see the "),e("RouterLink",{attrs:{to:"/api/biome/text/configuration.html#trainerconfiguration"}},[t._v("TrainerConfiguration API")]),t._v(".")],1),t._v(" "),e("div",{staticClass:"custom-block tip"},[e("p",{staticClass:"custom-block-title"},[t._v("TIP")]),t._v(" "),e("p",[t._v("In case you have a cuda device available, you also specify it here.")])]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" biome"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("configuration "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" TrainerConfiguration\n")])])]),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("trainer_config "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TrainerConfiguration"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    optimizer"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"adam"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"lr"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.01")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    num_epochs"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# cuda_device=0,")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("h2",{attrs:{id:"train-your-model"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#train-your-model"}},[t._v("#")]),t._v(" Train your model")]),t._v(" "),e("p",[t._v("Now we have everything ready to start the training of our model:")]),t._v(" "),e("ul",[e("li",[t._v("training data set")]),t._v(" "),e("li",[t._v("vocabulary")]),t._v(" "),e("li",[t._v("trainer")])]),t._v(" "),e("p",[t._v("Optionally we can provide a validation data set to estimate the generalization error.\nFor this we will create another "),e("code",[t._v("DataSource")]),t._v(" pointing to our validation data.")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("valid_ds "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" DataSource"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"https://biome-tutorials-data.s3-eu-west-1.amazonaws.com/text_classifier/business.cat.valid.csv"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("p",[t._v("The training output will be saved in a folder specified by the "),e("code",[t._v("output")]),t._v(" argument. It contains the trained model weights and the metrics, as well as the vocabulary and a "),e("em",[t._v("log")]),t._v(" folder for visualizing the training process with "),e("a",{attrs:{href:"https://www.tensorflow.org/tensorboard/",target:"_blank",rel:"noopener noreferrer"}},[t._v("tensorboard"),e("OutboundLink")],1),t._v(".")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("pl"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    output"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"output"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    training"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("train_ds"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    validation"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("valid_ds"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    trainer"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("trainer_config"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("div",{staticClass:"custom-block tip"},[e("p",{staticClass:"custom-block-title"},[t._v("TIP")]),t._v(" "),e("p",[t._v("If for some reason the training gets interrupted, you can continue where you left off by setting the "),e("code",[t._v("restore")]),t._v(" argument in the "),e("code",[t._v("Pipeline.train()")]),t._v(" method to "),e("code",[t._v("True")]),t._v(".\nIf you want to train your model for a few more epochs, you can also use the "),e("code",[t._v("restore")]),t._v(" argument, but you have to modify the "),e("code",[t._v("epochs")]),t._v(" argument in your "),e("code",[t._v("TrainerConfiguration")]),t._v(" to reflect the total amount of epochs you aim for.")])]),t._v(" "),e("h2",{attrs:{id:"make-your-first-predictions"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#make-your-first-predictions"}},[t._v("#")]),t._v(" Make your first predictions")]),t._v(" "),e("p",[t._v("Now that we trained our model we can go on to make our first predictions.\nFirst we must load our trained model into a new "),e("code",[t._v("Pipeline")]),t._v(":")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("pl_trained "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Pipeline"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_pretrained"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"output/model.tar.gz"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("div",{staticClass:"language- extra-class"},[e("pre",[e("code",[t._v("INFO:filelock:Lock 140475560757072 acquired on /tmp/tmpwhw1ojkf/vocabulary/.lock\nINFO:filelock:Lock 140475560757072 released on /tmp/tmpwhw1ojkf/vocabulary/.lock\n")])])]),e("p",[t._v("We then provide the input expected by our "),e("code",[t._v("TaskHead")]),t._v(" of the model to the "),e("code",[t._v("Pipeline.predict()")]),t._v(" method.\nIn our case it is a "),e("code",[t._v("TextClassification")]),t._v(" head that classifies a "),e("code",[t._v("text")]),t._v(" input:")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("pl_trained"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("text"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Autohaus biome.text"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("div",{staticClass:"language- extra-class"},[e("pre",[e("code",[t._v("{'logits': array([ -6.2526927, -12.3927355,  -4.7647996,  -3.067424 ,   7.4543405,\n         -2.3797615,  -4.6205163,  -8.315695 , -22.648367 , -13.5710745,\n        -17.335678 ,  -2.408677 , -13.462036 ,  -4.141186 , -14.408431 ,\n        -16.615704 ,  -4.739997 ,  -5.768703 ,   1.5712477,  -4.3752007,\n        -20.959383 ,  -7.2523   ], dtype=float32),\n 'probs': array([1.1112966e-06, 2.3946556e-09, 4.9205510e-06, 2.6864234e-05,\n        9.9705434e-01, 5.3434618e-05, 5.6842759e-06, 1.4121464e-07,\n        8.4193645e-14, 7.3704992e-10, 1.7082473e-11, 5.1911611e-05,\n        8.2196266e-10, 9.1800612e-06, 3.1903444e-10, 3.5093907e-11,\n        5.0441176e-06, 1.8031176e-06, 2.7779476e-03, 7.2646444e-06,\n        4.5582245e-13, 4.0898382e-07], dtype=float32),\n 'classes': {'Gebrauchtwagen': tensor(0.9971),\n  'Autowerkstätten': tensor(0.0028),\n  'Elektriker': tensor(5.3435e-05),\n  'Sanitärinstallationen': tensor(5.1912e-05),\n  'Dienstleistungen': tensor(2.6864e-05),\n  'Werbeagenturen': tensor(9.1801e-06),\n  'Elektrotechnik': tensor(7.2646e-06),\n  'Restaurants': tensor(5.6843e-06),\n  'Vermittlungen': tensor(5.0441e-06),\n  'Tiefbau': tensor(4.9206e-06),\n  'Hotels': tensor(1.8031e-06),\n  'Unternehmensberatungen': tensor(1.1113e-06),\n  'Handelsvermittler Und -vertreter': tensor(4.0898e-07),\n  'Architekturbüros': tensor(1.4121e-07),\n  'Friseure': tensor(2.3947e-09),\n  'Edv': tensor(8.2196e-10),\n  'Versicherungsvermittler': tensor(7.3705e-10),\n  'Apotheken': tensor(3.1903e-10),\n  'Physiotherapie': tensor(3.5094e-11),\n  'Maler': tensor(1.7082e-11),\n  'Allgemeinärzte': tensor(4.5582e-13),\n  'Vereine': tensor(8.4194e-14)},\n 'max_class': 'Gebrauchtwagen',\n 'max_class_prob': tensor(0.9971),\n 'label': 'Gebrauchtwagen',\n 'prob': tensor(0.9971)}\n")])])]),e("p",[t._v("The returned dictionary contains the logits and probabilities of all labels (classes).\nThe label with the highest probability is stored under the "),e("code",[t._v("label")]),t._v(" key, together with its probability under the "),e("code",[t._v("prob")]),t._v(" key.")]),t._v(" "),e("div",{staticClass:"custom-block tip"},[e("p",{staticClass:"custom-block-title"},[t._v("TIP")]),t._v(" "),e("p",[t._v("When configuring the pipeline in the first place, we recommend to check that it is correctly setup by using the "),e("code",[t._v("predict")]),t._v(" method.\nSince the pipeline is still not trained at that moment, the predictions will be arbitrary.")])]),t._v(" "),e("h2",{attrs:{id:"explore-the-model-s-predictions"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#explore-the-model-s-predictions"}},[t._v("#")]),t._v(" Explore the model's predictions")]),t._v(" "),e("p",[t._v("To check and understand the predictions of the model, we can use the "),e("strong",[t._v("biome.text explore UI")]),t._v(".\nJust calling the "),e("RouterLink",{attrs:{to:"/api/biome/text/pipeline.html#explore"}},[t._v("Pipeline.predict")]),t._v(" method will open the UI in the output of our cell.\nWe will set the "),e("code",[t._v("explain")]),t._v(" argument to true, which automatically visualizes the attribution of each token by means of "),e("a",{attrs:{href:"https://arxiv.org/abs/1703.01365",target:"_blank",rel:"noopener noreferrer"}},[t._v("integrated gradients"),e("OutboundLink")],1),t._v(".")],1),t._v(" "),e("p",[t._v("The usage of the explore UI should be largely intuitive, but if you are interested in more details see this "),e("RouterLink",{attrs:{to:"/documentation/user-guides/02.explore.html"}},[t._v("user guide")]),t._v(".")],1),t._v(" "),e("div",{staticClass:"custom-block warning"},[e("p",{staticClass:"custom-block-title"},[t._v("WARNING")]),t._v(" "),e("p",[t._v("For the UI to work you need a running "),e("a",{attrs:{href:"https://www.elastic.co/elasticsearch/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Elasticsearch"),e("OutboundLink")],1),t._v(" instance.")])]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("pl_trained"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("explore"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("valid_ds"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" explain"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("p",[t._v('Exploring our model we could take advantage of the F1 scores of each label to figure out which labels to prioritize when gathering new training data.\nFor example, although "Allgemeinärzte" is the second rarest label in our training data, it still seems relatively easy to classify for our model due to the distinctive words "Dr." and "Allgemeinmedizin".')])])}),[],!1,null,null,null);a.default=n.exports}}]);