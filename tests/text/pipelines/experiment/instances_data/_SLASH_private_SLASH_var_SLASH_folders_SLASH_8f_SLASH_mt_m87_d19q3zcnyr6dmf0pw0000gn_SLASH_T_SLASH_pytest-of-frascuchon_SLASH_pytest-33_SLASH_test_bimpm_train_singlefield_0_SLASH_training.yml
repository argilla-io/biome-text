{"py/object": "allennlp.data.instance.Instance", "fields": {"record1": {"py/object": "allennlp.data.fields.text_field.TextField", "tokens": [{"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["record1_1", 0, "record1_1", "", "", "", "", null]}, "py/seq": ["record1_1", 0, "record1_1", "", "", "", "", null]}, {"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["record1_1", 0, "record1_1", "", "", "", "", null]}, "py/seq": ["record1_1", 0, "record1_1", "", "", "", "", null]}, {"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["record1_2", 0, "record1_2", "", "", "", "", null]}, "py/seq": ["record1_2", 0, "record1_2", "", "", "", "", null]}, {"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["record1_2", 0, "record1_2", "", "", "", "", null]}, "py/seq": ["record1_2", 0, "record1_2", "", "", "", "", null]}, {"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["record1_3", 0, "record1_3", "", "", "", "", null]}, "py/seq": ["record1_3", 0, "record1_3", "", "", "", "", null]}, {"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["record1_3", 0, "record1_3", "", "", "", "", null]}, "py/seq": ["record1_3", 0, "record1_3", "", "", "", "", null]}], "_token_indexers": {"words": {"py/object": "allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer", "_token_min_padding_length": 0, "namespace": "words", "lowercase_tokens": true, "_start_tokens": [], "_end_tokens": []}, "chars": {"py/object": "allennlp.data.token_indexers.token_characters_indexer.TokenCharactersIndexer", "_token_min_padding_length": 0, "_min_padding_length": 0, "_namespace": "chars", "_character_tokenizer": {"py/object": "allennlp.data.tokenizers.character_tokenizer.CharacterTokenizer", "_byte_encoding": null, "_lowercase_characters": true, "_start_tokens": [], "_end_tokens": []}, "_start_tokens": [], "_end_tokens": []}}, "_indexed_tokens": null, "_indexer_name_to_indexed_token": null, "_token_index_to_indexer_name": null}, "record2": {"py/object": "allennlp.data.fields.text_field.TextField", "tokens": [{"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["record2_1", 0, "record2_1", "", "", "", "", null]}, "py/seq": ["record2_1", 0, "record2_1", "", "", "", "", null]}, {"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["record2_1", 0, "record2_1", "", "", "", "", null]}, "py/seq": ["record2_1", 0, "record2_1", "", "", "", "", null]}, {"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["record2_2", 0, "record2_2", "", "", "", "", null]}, "py/seq": ["record2_2", 0, "record2_2", "", "", "", "", null]}, {"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["record2_2", 0, "record2_2", "", "", "", "", null]}, "py/seq": ["record2_2", 0, "record2_2", "", "", "", "", null]}], "_token_indexers": {"words": {"py/id": 9}, "chars": {"py/id": 12}}, "_indexed_tokens": null, "_indexer_name_to_indexed_token": null, "_token_index_to_indexer_name": null}, "label": {"py/object": "allennlp.data.fields.label_field.LabelField", "label": "1", "_label_namespace": "gold_labels", "_label_id": null, "_skip_indexing": false}}, "indexed": false}
{"py/object": "allennlp.data.instance.Instance", "fields": {"record1": {"py/object": "allennlp.data.fields.text_field.TextField", "tokens": [{"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["record1_1", 0, "record1_1", "", "", "", "", null]}, "py/seq": ["record1_1", 0, "record1_1", "", "", "", "", null]}, {"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["record1_1", 0, "record1_1", "", "", "", "", null]}, "py/seq": ["record1_1", 0, "record1_1", "", "", "", "", null]}, {"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["test", 10, "test", "", "", "", "", null]}, "py/seq": ["test", 10, "test", "", "", "", "", null]}, {"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["record1_2", 0, "record1_2", "", "", "", "", null]}, "py/seq": ["record1_2", 0, "record1_2", "", "", "", "", null]}, {"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["record1_2", 0, "record1_2", "", "", "", "", null]}, "py/seq": ["record1_2", 0, "record1_2", "", "", "", "", null]}, {"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["test", 10, "test", "", "", "", "", null]}, "py/seq": ["test", 10, "test", "", "", "", "", null]}, {"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["record1_3", 0, "record1_3", "", "", "", "", null]}, "py/seq": ["record1_3", 0, "record1_3", "", "", "", "", null]}, {"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["record1_3", 0, "record1_3", "", "", "", "", null]}, "py/seq": ["record1_3", 0, "record1_3", "", "", "", "", null]}, {"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["test", 10, "test", "", "", "", "", null]}, "py/seq": ["test", 10, "test", "", "", "", "", null]}], "_token_indexers": {"words": {"py/object": "allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer", "_token_min_padding_length": 0, "namespace": "words", "lowercase_tokens": true, "_start_tokens": [], "_end_tokens": []}, "chars": {"py/object": "allennlp.data.token_indexers.token_characters_indexer.TokenCharactersIndexer", "_token_min_padding_length": 0, "_min_padding_length": 0, "_namespace": "chars", "_character_tokenizer": {"py/object": "allennlp.data.tokenizers.character_tokenizer.CharacterTokenizer", "_byte_encoding": null, "_lowercase_characters": true, "_start_tokens": [], "_end_tokens": []}, "_start_tokens": [], "_end_tokens": []}}, "_indexed_tokens": null, "_indexer_name_to_indexed_token": null, "_token_index_to_indexer_name": null}, "record2": {"py/object": "allennlp.data.fields.text_field.TextField", "tokens": [{"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["record2_1", 0, "record2_1", "", "", "", "", null]}, "py/seq": ["record2_1", 0, "record2_1", "", "", "", "", null]}, {"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["record2_1", 0, "record2_1", "", "", "", "", null]}, "py/seq": ["record2_1", 0, "record2_1", "", "", "", "", null]}, {"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["test", 10, "test", "", "", "", "", null]}, "py/seq": ["test", 10, "test", "", "", "", "", null]}, {"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["record2_2", 0, "record2_2", "", "", "", "", null]}, "py/seq": ["record2_2", 0, "record2_2", "", "", "", "", null]}, {"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["record2_2", 0, "record2_2", "", "", "", "", null]}, "py/seq": ["record2_2", 0, "record2_2", "", "", "", "", null]}, {"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["test", 10, "test", "", "", "", "", null]}, "py/seq": ["test", 10, "test", "", "", "", "", null]}], "_token_indexers": {"words": {"py/id": 12}, "chars": {"py/id": 15}}, "_indexed_tokens": null, "_indexer_name_to_indexed_token": null, "_token_index_to_indexer_name": null}, "label": {"py/object": "allennlp.data.fields.label_field.LabelField", "label": "0", "_label_namespace": "gold_labels", "_label_id": null, "_skip_indexing": false}}, "indexed": false}
{"py/object": "allennlp.data.instance.Instance", "fields": {"record1": {"py/object": "allennlp.data.fields.text_field.TextField", "tokens": [{"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["record1_1", 0, "record1_1", "", "", "", "", null]}, "py/seq": ["record1_1", 0, "record1_1", "", "", "", "", null]}, {"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["record1_1", 0, "record1_1", "", "", "", "", null]}, "py/seq": ["record1_1", 0, "record1_1", "", "", "", "", null]}, {"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["test", 10, "test", "", "", "", "", null]}, "py/seq": ["test", 10, "test", "", "", "", "", null]}, {"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["this", 15, "this", "", "", "", "", null]}, "py/seq": ["this", 15, "this", "", "", "", "", null]}, {"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["record1_2", 0, "record1_2", "", "", "", "", null]}, "py/seq": ["record1_2", 0, "record1_2", "", "", "", "", null]}, {"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["record1_2", 0, "record1_2", "", "", "", "", null]}, "py/seq": ["record1_2", 0, "record1_2", "", "", "", "", null]}, {"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["test", 10, "test", "", "", "", "", null]}, "py/seq": ["test", 10, "test", "", "", "", "", null]}, {"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["this", 15, "this", "", "", "", "", null]}, "py/seq": ["this", 15, "this", "", "", "", "", null]}, {"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["record1_3", 0, "record1_3", "", "", "", "", null]}, "py/seq": ["record1_3", 0, "record1_3", "", "", "", "", null]}, {"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["record1_3", 0, "record1_3", "", "", "", "", null]}, "py/seq": ["record1_3", 0, "record1_3", "", "", "", "", null]}, {"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["test", 10, "test", "", "", "", "", null]}, "py/seq": ["test", 10, "test", "", "", "", "", null]}, {"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["this", 15, "this", "", "", "", "", null]}, "py/seq": ["this", 15, "this", "", "", "", "", null]}], "_token_indexers": {"words": {"py/object": "allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer", "_token_min_padding_length": 0, "namespace": "words", "lowercase_tokens": true, "_start_tokens": [], "_end_tokens": []}, "chars": {"py/object": "allennlp.data.token_indexers.token_characters_indexer.TokenCharactersIndexer", "_token_min_padding_length": 0, "_min_padding_length": 0, "_namespace": "chars", "_character_tokenizer": {"py/object": "allennlp.data.tokenizers.character_tokenizer.CharacterTokenizer", "_byte_encoding": null, "_lowercase_characters": true, "_start_tokens": [], "_end_tokens": []}, "_start_tokens": [], "_end_tokens": []}}, "_indexed_tokens": null, "_indexer_name_to_indexed_token": null, "_token_index_to_indexer_name": null}, "record2": {"py/object": "allennlp.data.fields.text_field.TextField", "tokens": [{"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["record2_1", 0, "record2_1", "", "", "", "", null]}, "py/seq": ["record2_1", 0, "record2_1", "", "", "", "", null]}, {"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["record2_1", 0, "record2_1", "", "", "", "", null]}, "py/seq": ["record2_1", 0, "record2_1", "", "", "", "", null]}, {"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["test", 10, "test", "", "", "", "", null]}, "py/seq": ["test", 10, "test", "", "", "", "", null]}, {"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["this", 15, "this", "", "", "", "", null]}, "py/seq": ["this", 15, "this", "", "", "", "", null]}, {"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["record2_2", 0, "record2_2", "", "", "", "", null]}, "py/seq": ["record2_2", 0, "record2_2", "", "", "", "", null]}, {"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["record2_2", 0, "record2_2", "", "", "", "", null]}, "py/seq": ["record2_2", 0, "record2_2", "", "", "", "", null]}, {"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["test", 10, "test", "", "", "", "", null]}, "py/seq": ["test", 10, "test", "", "", "", "", null]}, {"py/object": "allennlp.data.tokenizers.token.Token", "py/newargs": {"py/tuple": ["this", 15, "this", "", "", "", "", null]}, "py/seq": ["this", 15, "this", "", "", "", "", null]}], "_token_indexers": {"words": {"py/id": 15}, "chars": {"py/id": 18}}, "_indexed_tokens": null, "_indexer_name_to_indexed_token": null, "_token_index_to_indexer_name": null}, "label": {"py/object": "allennlp.data.fields.label_field.LabelField", "label": "1", "_label_namespace": "gold_labels", "_label_id": null, "_skip_indexing": false}}, "indexed": false}
