<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Using Transformers in biome.text | biome.text</title>
    <meta name="generator" content="VuePress 1.8.2">
    <link rel="shortcut icon" href="/biome-text/v2.0.1rc1/favicon.ico">
    <meta name="description" content="biome.text practical NLP open source library.">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta property="og:image" content="https://www.recogn.ai/images/biome_og.png">
    
    <link rel="preload" href="/biome-text/v2.0.1rc1/assets/css/0.styles.88faedca.css" as="style"><link rel="preload" href="/biome-text/v2.0.1rc1/assets/js/app.da9c7cc4.js" as="script"><link rel="preload" href="/biome-text/v2.0.1rc1/assets/js/4.654db8cc.js" as="script"><link rel="preload" href="/biome-text/v2.0.1rc1/assets/js/3.ec9f5f07.js" as="script"><link rel="preload" href="/biome-text/v2.0.1rc1/assets/js/66.50b0b792.js" as="script"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/10.9e7e294d.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/11.36448be8.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/12.9ea6506f.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/13.b6d2d3e7.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/14.55ff4e9e.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/15.d76cc660.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/16.74fe881c.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/17.97e2862a.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/18.9ad89835.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/19.e964aa40.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/20.dbecf425.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/21.fb069313.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/22.31ce58c7.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/23.845171fe.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/24.bc59e034.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/25.37af71c6.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/26.a51a1372.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/27.f3e37d69.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/28.f90d0fec.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/29.b93aea55.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/30.9c7120ad.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/31.d3c1886a.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/32.7a384541.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/33.2690d2a6.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/34.1054646e.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/35.fb3fa72a.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/36.9545dbe0.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/37.5af6f82f.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/38.6fff958f.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/39.93ef2f18.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/40.150470d0.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/41.3d7099a3.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/42.dfc49d4f.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/43.bf946bf9.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/44.6464c4cb.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/45.168fe3f1.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/46.319cd4f4.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/47.49eb2f83.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/48.4d1a8e79.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/49.2203d8a7.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/5.c2b67978.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/50.ed58659f.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/51.38a913ea.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/52.d6d2ee01.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/53.a3b775e8.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/54.64224be1.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/55.322098bd.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/56.252f52f4.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/57.944aa8b7.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/58.b6aa355a.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/59.9a8b83d3.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/6.788d81cc.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/60.b8607c9f.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/61.0eea3e87.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/62.98d94a8e.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/63.2e4d096b.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/64.817daf53.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/65.1050b90f.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/67.cd8aebca.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/68.808338cc.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/69.95dc1e66.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/7.76338fbb.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/8.b70c8b51.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/9.023f49be.js"><link rel="prefetch" href="/biome-text/v2.0.1rc1/assets/js/vendors~docsearch.6a549fca.js">
    <link rel="stylesheet" href="/biome-text/v2.0.1rc1/assets/css/0.styles.88faedca.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container" data-v-348088ed><header class="navbar" data-v-348088ed><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/biome-text/v2.0.1rc1/" class="home-link router-link-active"><!----> <span class="site-name">biome<span>.text</span></span></a> <div class="links"><form id="search-form" role="search" class="algolia-search-wrapper search-box"><input id="algolia-search-input" class="search-query"></form> <nav class="nav-links can-hide"><div class="nav-item"><a href="/biome-text/v2.0.1rc1/api/" class="nav-link">
  API
</a></div><div class="nav-item"><a href="/biome-text/v2.0.1rc1/documentation/" class="nav-link router-link-active">
  Documentation
</a></div><div class="nav-item"><a href="https://github.com/recognai/biome-text" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span class="external__icon"><vp-icon color="#4A4A4A" name="blank" size="12px"></vp-icon></span></a></div><div class="nav-item"><a href="https://recogn.ai" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Recognai
  <span class="external__icon"><vp-icon color="#4A4A4A" name="blank" size="12px"></vp-icon></span></a></div> <!----></nav></div></header> <div class="sidebar-mask" data-v-348088ed></div> <aside class="sidebar" data-v-348088ed><div class="sidebar__link"><a href="/biome-text/v2.0.1rc1/"><img src="/biome-text/v2.0.1rc1/assets/img/biome.svg" class="sidebar__img"></a></div> <!----> <nav class="nav-links"><div class="nav-item"><a href="/biome-text/v2.0.1rc1/api/" class="nav-link">
  API
</a></div><div class="nav-item"><a href="/biome-text/v2.0.1rc1/documentation/" class="nav-link router-link-active">
  Documentation
</a></div><div class="nav-item"><a href="https://github.com/recognai/biome-text" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span class="external__icon"><vp-icon color="#4A4A4A" name="blank" size="12px"></vp-icon></span></a></div><div class="nav-item"><a href="https://recogn.ai" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Recognai
  <span class="external__icon"><vp-icon color="#4A4A4A" name="blank" size="12px"></vp-icon></span></a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group depth-0"><p class="sidebar-heading"><span>Get started</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/biome-text/v2.0.1rc1/documentation/" aria-current="page" class="sidebar-link">Installation</a></li><li><a href="/biome-text/v2.0.1rc1/documentation/basics.html" class="sidebar-link">The basics</a></li></ul></section></li><li><section class="sidebar-group depth-0"><p class="sidebar-heading open"><span>Tutorials</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/biome-text/v2.0.1rc1/documentation/tutorials/1-Training_a_text_classifier.html" class="sidebar-link">Training a short text classifier of German business names</a></li><li><a href="/biome-text/v2.0.1rc1/documentation/tutorials/2-Training_a_sequence_tagger_for_Slot_Filling.html" class="sidebar-link">Training a sequence tagger for Slot Filling</a></li><li><a href="/biome-text/v2.0.1rc1/documentation/tutorials/3-Hyperparameter_optimization_with_Ray_Tune.html" class="sidebar-link">Hyperparameter optimization with Ray Tune</a></li><li><a href="/biome-text/v2.0.1rc1/documentation/tutorials/4-Using_Transformers_in_biome_text.html" aria-current="page" class="active sidebar-link">Using Transformers in biome.text</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/biome-text/v2.0.1rc1/documentation/tutorials/4-Using_Transformers_in_biome_text.html#introduction" class="sidebar-link">Introduction</a></li><li class="sidebar-sub-header"><a href="/biome-text/v2.0.1rc1/documentation/tutorials/4-Using_Transformers_in_biome_text.html#exploring-and-preparing-the-data" class="sidebar-link">Exploring and preparing the data</a></li><li class="sidebar-sub-header"><a href="/biome-text/v2.0.1rc1/documentation/tutorials/4-Using_Transformers_in_biome_text.html#configuring-and-training-the-pipeline" class="sidebar-link">Configuring and training the pipeline</a></li><li class="sidebar-sub-header"><a href="/biome-text/v2.0.1rc1/documentation/tutorials/4-Using_Transformers_in_biome_text.html#optimizing-the-trainer-configuration" class="sidebar-link">Optimizing the trainer configuration</a></li><li class="sidebar-sub-header"><a href="/biome-text/v2.0.1rc1/documentation/tutorials/4-Using_Transformers_in_biome_text.html#making-predictions" class="sidebar-link">Making predictions</a></li></ul></li></ul></section></li><li><section class="sidebar-group depth-0"><p class="sidebar-heading"><span>User Guides</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/biome-text/v2.0.1rc1/documentation/user-guides/1-nlp-tasks.html" class="sidebar-link">NLP Tasks</a></li><li><a href="/biome-text/v2.0.1rc1/documentation/user-guides/2-configuration.html" class="sidebar-link">Configurations</a></li><li><a href="/biome-text/v2.0.1rc1/documentation/user-guides/3-deployment.html" class="sidebar-link">Deployment</a></li></ul></section></li><li><section class="sidebar-group depth-0"><p class="sidebar-heading"><span>Community</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/biome-text/v2.0.1rc1/documentation/community/1-contributing.html" class="sidebar-link">Contributing</a></li><li><a href="/biome-text/v2.0.1rc1/documentation/community/2-get_help.html" class="sidebar-link">Getting help</a></li><li><a href="/biome-text/v2.0.1rc1/documentation/community/3-developer_guides.html" class="sidebar-link">Developer guides</a></li></ul></section></li></ul> </aside> <main class="page" data-v-348088ed> <div class="theme-default-content content__default"><h1 id="using-transformers-in-biome-text"><a href="#using-transformers-in-biome-text" class="header-anchor">#</a> Using Transformers in biome.text</h1> <p><a target="_blank" href="https://www.recogn.ai/biome-text/v2.0.1rc1/documentation/tutorials/4-Using_Transformers_in_biome_text.html"><img src="https://www.recogn.ai/biome-text/v2.0.1rc1/assets/img/biome-isotype.svg" width="24" class="icon"></a> <a href="https://www.recogn.ai/biome-text/v2.0.1rc1/documentation/tutorials/4-Using_Transformers_in_biome_text.html" target="_blank" rel="noopener noreferrer">View on recogn.ai<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p><a target="_blank" href="https://colab.research.google.com/github/recognai/biome-text/blob/v2.0.1rc1/docs/docs/documentation/tutorials/4-Using_Transformers_in_biome_text.ipynb"><img src="https://www.tensorflow.org/images/colab_logo_32px.png" width="24" class="icon"></a> <a href="https://colab.research.google.com/github/recognai/biome-text/blob/v2.0.1rc1/docs/docs/documentation/tutorials/4-Using_Transformers_in_biome_text.ipynb" target="_blank" rel="noopener noreferrer">Run in Google Colab<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p><a target="_blank" href="https://github.com/recognai/biome-text/blob/v2.0.1rc1/docs/docs/documentation/tutorials/4-Using_Transformers_in_biome_text.ipynb"><img src="https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png" width="24" class="icon"></a> <a href="https://github.com/recognai/biome-text/blob/v2.0.1rc1/docs/docs/documentation/tutorials/4-Using_Transformers_in_biome_text.ipynb" target="_blank" rel="noopener noreferrer">View source on GitHub<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p>When running this tutorial in Google Colab, make sure to install <em>biome.text</em> first:</p> <div class="language-python extra-class"><pre class="language-python"><code>!pip install <span class="token operator">-</span>U pip
!pip install <span class="token operator">-</span>U biome<span class="token operator">-</span>text
exit<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>  <span class="token comment"># Force restart of the runtime</span>
</code></pre></div><p><em>If</em> you want to log your runs with <a href="https://wandb.ai/home" target="_blank" rel="noopener noreferrer">WandB<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>, don't forget to install its client and log in.</p> <div class="language-python extra-class"><pre class="language-python"><code>!pip install wandb
!wandb login
</code></pre></div><h2 id="introduction"><a href="#introduction" class="header-anchor">#</a> Introduction</h2> <p>In the last years we experienced a shift towards transfer learning as the standard approach to solve NLP problems. Before models were usually trained entirely from scratch, utilizing at most pretrained word embeddings. But nowadays it is very common to start with large pretrained language models as backbone of a system, and to set a task specific head on top of it. This new paradigm has made it easier to find state-of-the-art architectures for a great variety of NLP tasks.</p> <p>Almost all current language models are based on the transformer architecture. The awesome <a href="https://github.com/huggingface/transformers" target="_blank" rel="noopener noreferrer">Hugging Face Transformers<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> library provides access to hundreds of such pretrained language models including state-of-the-art models such as infamous <a href="https://github.com/google-research/bert" target="_blank" rel="noopener noreferrer">BERT<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>, as well as community driven models often covering a specific language type or resource requirements.</p> <p>In this tutorial, we are going to classify <a href="https://arxiv.org/" target="_blank" rel="noopener noreferrer">arXiv<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> papers into <a href="https://arxiv.org/category_taxonomy" target="_blank" rel="noopener noreferrer">categories<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>, analyzing the title of the paper and its abstract. We will use Hugging Face <a href="https://medium.com/huggingface/distilbert-8cf3380435b5" target="_blank" rel="noopener noreferrer">distilled<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> implementation of <a href="https://ai.facebook.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/" target="_blank" rel="noopener noreferrer">RoBERTa<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> and explore ways how to easily include pretrained transformers in a <em>biome.text</em> pipeline.</p> <h3 id="external-links-about-transformers"><a href="#external-links-about-transformers" class="header-anchor">#</a> External links about transformers</h3> <p>If this is the first time you hear about &quot;Transformers&quot; not referring to giant robots, here is a small list of resources at which you might want to have a look first:</p> <ul><li><a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener noreferrer">Attention is all your need<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>: paper that introduced the architecture.</li> <li><a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener noreferrer">The Illustrated Transformer<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>: 20-30 minute article covering how they work.</li> <li><a href="https://youtu.be/4Bdc55j80l8" target="_blank" rel="noopener noreferrer">Illustrated Guide to Transformer Neural Network: a step by step explanation<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>: 15 minute long video covering how they work.</li> <li><a href="https://www.youtube.com/watch?v=8Hg2UtQg6G4" target="_blank" rel="noopener noreferrer">An Introduction To Transfer Learning In NLP and HuggingFace<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>: 1 hour talk by Thomas Wolf</li></ul> <h3 id="imports"><a href="#imports" class="header-anchor">#</a> Imports</h3> <p>Let us first import all the stuff we need for this tutorial:</p> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">from</span> biome<span class="token punctuation">.</span>text <span class="token keyword">import</span> Dataset<span class="token punctuation">,</span> Pipeline
<span class="token keyword">from</span> biome<span class="token punctuation">.</span>text<span class="token punctuation">.</span>configuration <span class="token keyword">import</span> VocabularyConfiguration<span class="token punctuation">,</span> TrainerConfiguration
<span class="token keyword">from</span> biome<span class="token punctuation">.</span>text<span class="token punctuation">.</span>hpo <span class="token keyword">import</span> TuneExperiment
<span class="token keyword">from</span> ray <span class="token keyword">import</span> tune
<span class="token keyword">import</span> os
</code></pre></div><h2 id="exploring-and-preparing-the-data"><a href="#exploring-and-preparing-the-data" class="header-anchor">#</a> Exploring and preparing the data</h2> <p>For this tutorial we are going to use the <a href="https://www.kaggle.com/Cornell-University/arxiv" target="_blank" rel="noopener noreferrer">arXiv dataset<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> compiled by the Cornell University, which consists of metadata of scientific papers stored in <a href="https://arxiv.org/" target="_blank" rel="noopener noreferrer">arXiv<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>.</p> <p>We preprocessed the data in a separate <a href="https://drive.google.com/file/d/1zUSz81x15RH5mL5GoN7i7xqiNGEqclU0/view?usp=sharing" target="_blank" rel="noopener noreferrer">notebook<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> producing three csv files (train, validate and test datasets) that contain the title, the abstract and the category of the corresponding paper.</p> <p>Our NLP task will be to classify the papers into the given categories based on the title and abstract. Below we download the preprocessed data and create our <a href="https://www.recogn.ai/biome-text/v2.0.1rc1/api/biome/text/dataset.html#dataset" target="_blank" rel="noopener noreferrer">Datasets<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> with it.</p> <div class="language-python extra-class"><pre class="language-python"><code><span class="token comment"># Downloading the datasets</span>
!curl <span class="token operator">-</span>O https<span class="token punctuation">:</span><span class="token operator">//</span>biome<span class="token operator">-</span>tutorials<span class="token operator">-</span>data<span class="token punctuation">.</span>s3<span class="token operator">-</span>eu<span class="token operator">-</span>west<span class="token operator">-</span><span class="token number">1.</span>amazonaws<span class="token punctuation">.</span>com<span class="token operator">/</span>transformers_arxiv<span class="token operator">-</span>classifier<span class="token operator">/</span>arxiv<span class="token operator">-</span>dataset<span class="token operator">-</span>train<span class="token punctuation">.</span>json
!curl <span class="token operator">-</span>O https<span class="token punctuation">:</span><span class="token operator">//</span>biome<span class="token operator">-</span>tutorials<span class="token operator">-</span>data<span class="token punctuation">.</span>s3<span class="token operator">-</span>eu<span class="token operator">-</span>west<span class="token operator">-</span><span class="token number">1.</span>amazonaws<span class="token punctuation">.</span>com<span class="token operator">/</span>transformers_arxiv<span class="token operator">-</span>classifier<span class="token operator">/</span>arxiv<span class="token operator">-</span>dataset<span class="token operator">-</span>validate<span class="token punctuation">.</span>json
!curl <span class="token operator">-</span>O https<span class="token punctuation">:</span><span class="token operator">//</span>biome<span class="token operator">-</span>tutorials<span class="token operator">-</span>data<span class="token punctuation">.</span>s3<span class="token operator">-</span>eu<span class="token operator">-</span>west<span class="token operator">-</span><span class="token number">1.</span>amazonaws<span class="token punctuation">.</span>com<span class="token operator">/</span>transformers_arxiv<span class="token operator">-</span>classifier<span class="token operator">/</span>arxiv<span class="token operator">-</span>dataset<span class="token operator">-</span>test<span class="token punctuation">.</span>json
</code></pre></div><div class="language-python extra-class"><pre class="language-python"><code><span class="token comment"># Loading from local</span>
train_ds <span class="token operator">=</span> Dataset<span class="token punctuation">.</span>from_json<span class="token punctuation">(</span><span class="token string">&quot;arxiv-dataset-train.json&quot;</span><span class="token punctuation">)</span>
valid_ds <span class="token operator">=</span> Dataset<span class="token punctuation">.</span>from_json<span class="token punctuation">(</span><span class="token string">&quot;arxiv-dataset-validate.json&quot;</span><span class="token punctuation">)</span>
test_ds <span class="token operator">=</span> Dataset<span class="token punctuation">.</span>from_json<span class="token punctuation">(</span><span class="token string">&quot;arxiv-dataset-test.json&quot;</span><span class="token punctuation">)</span>
</code></pre></div><p>Let's have a look at the first 10 examples of the train dataset.</p> <div class="language-python extra-class"><pre class="language-python"><code>train_ds<span class="token punctuation">.</span>head<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre></div><div><style scoped="scoped">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;!--beforebegin--&gt;&lt;div class=&quot;language- extra-class&quot;&gt;&lt;!--afterbegin--&gt;&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;!--beforeend--&gt;&lt;/div&gt;&lt;!--afterend--&gt;</style> <table border="1" class="dataframe"><thead><tr style="text-align:right;"><th></th> <th>title</th> <th>categories</th> <th>abstract</th></tr></thead> <tbody><tr><th>0</th> <td>Coherent structures in the wake of a SAE squar...</td> <td>physics.flu-dyn</td> <td>The wake of a SAE squareback vehicle model i...</td></tr> <tr><th>1</th> <td>On the interaction of precipitates and tensile...</td> <td>cond-mat.mtrl-sci</td> <td>Although magnesium alloys deform extensively...</td></tr> <tr><th>2</th> <td>Multi-domain Spectral Collocation Method for V...</td> <td>math.NA</td> <td>Spectral and spectral element methods using ...</td></tr> <tr><th>3</th> <td>The min-max edge q-coloring problem</td> <td>cs.DS</td> <td>In this paper we introduce and study a new p...</td></tr> <tr><th>4</th> <td>A Simple Model for a Dual Non-Abelian Monopole...</td> <td>hep-th</td> <td>We investigate the flux-tube joining two equ...</td></tr> <tr><th>5</th> <td>A second moment bound for critical points of p...</td> <td>math.PR</td> <td>We consider the number of critical points of...</td></tr> <tr><th>6</th> <td>$\Sigma^0$ production in proton nucleus collis...</td> <td>nucl-ex</td> <td>The production of $\Sigma^{0}$ baryons in th...</td></tr> <tr><th>7</th> <td>Zero-temperature glass transition in two dimen...</td> <td>cond-mat.stat-mech</td> <td>The nature of the glass transition is theore...</td></tr> <tr><th>8</th> <td>Measurement of the $B^0_s\to\mu^+\mu^-$ branch...</td> <td>hep-ex</td> <td>A search for the rare decays $B^0_s\to\mu^+\...</td></tr> <tr><th>9</th> <td>STAR inner tracking upgrade - A performance study</td> <td>nucl-ex</td> <td>Anisotropic flow measurements have demonstra...</td></tr></tbody></table></div> <p>Our pipeline defined in the next section, or to be more precise the <code>TaskClassification</code> task <a href="https://www.recogn.ai/biome-text/v2.0.1rc1/documentation/basics.html#head" target="_blank" rel="noopener noreferrer">head<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>, will expect a <em>text</em> and <em>label</em> column to be present in our data.
Therefore, we need to map our input to these two columns:</p> <div class="language-python extra-class"><pre class="language-python"><code><span class="token comment"># Renaming the 'categories' column into 'label'</span>
train_ds<span class="token punctuation">.</span>rename_column_<span class="token punctuation">(</span><span class="token string">&quot;categories&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;label&quot;</span><span class="token punctuation">)</span>
valid_ds<span class="token punctuation">.</span>rename_column_<span class="token punctuation">(</span><span class="token string">&quot;categories&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;label&quot;</span><span class="token punctuation">)</span>
test_ds<span class="token punctuation">.</span>rename_column_<span class="token punctuation">(</span><span class="token string">&quot;categories&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;label&quot;</span><span class="token punctuation">)</span>

<span class="token comment"># Combining 'title' and 'abstract' into a 'text' column, and remove them afterwards</span>
train_ds <span class="token operator">=</span> train_ds<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token string">&quot;text&quot;</span><span class="token punctuation">:</span> x<span class="token punctuation">[</span><span class="token string">&quot;title&quot;</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token string">&quot; &quot;</span> <span class="token operator">+</span> x<span class="token punctuation">[</span><span class="token string">&quot;abstract&quot;</span><span class="token punctuation">]</span><span class="token punctuation">}</span><span class="token punctuation">,</span> remove_columns<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">&quot;title&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;abstract&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
valid_ds <span class="token operator">=</span> valid_ds<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token string">&quot;text&quot;</span><span class="token punctuation">:</span> x<span class="token punctuation">[</span><span class="token string">&quot;title&quot;</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token string">&quot; &quot;</span> <span class="token operator">+</span> x<span class="token punctuation">[</span><span class="token string">&quot;abstract&quot;</span><span class="token punctuation">]</span><span class="token punctuation">}</span><span class="token punctuation">,</span> remove_columns<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">&quot;title&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;abstract&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
test_ds <span class="token operator">=</span> test_ds<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token string">&quot;text&quot;</span><span class="token punctuation">:</span> x<span class="token punctuation">[</span><span class="token string">&quot;title&quot;</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token string">&quot; &quot;</span> <span class="token operator">+</span> x<span class="token punctuation">[</span><span class="token string">&quot;abstract&quot;</span><span class="token punctuation">]</span><span class="token punctuation">}</span><span class="token punctuation">,</span> remove_columns<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">&quot;title&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;abstract&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre></div><h2 id="configuring-and-training-the-pipeline"><a href="#configuring-and-training-the-pipeline" class="header-anchor">#</a> Configuring and training the pipeline</h2> <p>As we have seen in <a href="https://www.recogn.ai/biome-text/v2.0.1rc1/documentation/tutorials/1-Training_a_text_classifier.html#explore-the-training-data" target="_blank" rel="noopener noreferrer">previous tutorials<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>, a <em>biome.text</em> <a href="https://www.recogn.ai/biome-text/v2.0.1rc1/documentation/basics.html#pipeline" target="_blank" rel="noopener noreferrer"><code>Pipeline</code><span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> consists of tokenizing the input, extracting text features, applying a language encoding (optionally) and executing a task-specific head in the end. In <em>biome.text</em> the pre-trained transformers by Hugging Face are treated as a text feature, just like the <em>word</em> and <em>char</em> feature.</p> <p>In this section we will configure and train 3 different pipelines to showcase the usage of transformers in <em>biome.text</em>.</p> <h3 id="fine-tuning-the-transformer"><a href="#fine-tuning-the-transformer" class="header-anchor">#</a> Fine-tuning the transformer</h3> <p>In our first pipeline we follow the common approach to use pretrained transformers in classification tasks. It consists of fine-tuning the transformer weights and using a special token as pooler in the end. In our configuration the former step means setting the <code>trainable</code> parameter in the <em>transformers</em> features to <code>True</code>. The downside of fine-tuning is that most of the pre-trained transformers are relatively big and require dedicated hardware to be fine-tuned. For example, in this tutorial we will use <code>distilroberta-base</code>, a <a href="https://github.com/huggingface/transformers/tree/master/examples/distillation" target="_blank" rel="noopener noreferrer">distilled version<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> of RoBERTa with a total of ~80M parameters.</p> <p>We also need to specify the maximum number of input tokens <code>max_length</code> supported by the pretrained transformer. If you are sure that your input data does not exceed this limit, you can skip this parameter.</p> <p>With BERT-like models, such as RoBERTa, a special [CLS] token is added as first token to each input. It is pretrained to effectively represent the entire input and can be used as pooler in the head component. Many BERT like models pass this token through a non-linear tanh activation layer that is part of the pretraining. If you want to use these pretrained weights you have to use the <code>bert_pooler</code> together with the corresponding <code>pretrained_model</code>. We will fine-tune these weights as well (setting <code>require_grad</code> to <code>True</code>) and add a little dropout.</p> <div class="custom-block tip"><p class="custom-block-title">Tip</p> <p>You can also use the [CLS] token directly without passing it through the non-linear layer by using the <code>cls_pooler</code>.</p></div> <p>The <code>TextClassification</code> head automatically applies a linear layer with an output dimension corresponding to the number of labels in the end.</p> <div class="language-python extra-class"><pre class="language-python"><code>pipeline_dict_finetuning <span class="token operator">=</span> <span class="token punctuation">{</span>
    <span class="token string">&quot;name&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;arxiv_categories_classification&quot;</span><span class="token punctuation">,</span>
    <span class="token string">&quot;features&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">{</span>
        <span class="token string">&quot;transformers&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">{</span>
            <span class="token string">&quot;model_name&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;distilroberta-base&quot;</span><span class="token punctuation">,</span>
            <span class="token string">&quot;trainable&quot;</span><span class="token punctuation">:</span> <span class="token boolean">True</span><span class="token punctuation">,</span>  <span class="token comment"># freeze the weights of the transformer</span>
            <span class="token string">&quot;max_length&quot;</span><span class="token punctuation">:</span> <span class="token number">512</span><span class="token punctuation">,</span>
        <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token string">&quot;head&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">{</span>
        <span class="token string">&quot;type&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;TextClassification&quot;</span><span class="token punctuation">,</span>
        <span class="token string">&quot;labels&quot;</span><span class="token punctuation">:</span> train_ds<span class="token punctuation">.</span>unique<span class="token punctuation">(</span><span class="token string">&quot;label&quot;</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token string">&quot;pooler&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">{</span>
            <span class="token string">&quot;type&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;bert_pooler&quot;</span><span class="token punctuation">,</span>
            <span class="token string">&quot;pretrained_model&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;distilroberta-base&quot;</span><span class="token punctuation">,</span>
            <span class="token string">&quot;requires_grad&quot;</span><span class="token punctuation">:</span> <span class="token boolean">True</span><span class="token punctuation">,</span>
            <span class="token string">&quot;dropout&quot;</span><span class="token punctuation">:</span> <span class="token number">0.1</span><span class="token punctuation">,</span>
        <span class="token punctuation">}</span><span class="token punctuation">,</span>
        <span class="token comment"># If you do not want to use the pre-trained activation layer for the CLS token (see text) </span>
        <span class="token comment"># &quot;pooler&quot;: {</span>
        <span class="token comment">#     &quot;type&quot;: &quot;cls_pooler&quot;,</span>
        <span class="token comment"># }</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
<span class="token punctuation">}</span>
</code></pre></div><div class="language-python extra-class"><pre class="language-python"><code>pl <span class="token operator">=</span> Pipeline<span class="token punctuation">.</span>from_config<span class="token punctuation">(</span>pipeline_dict_finetuning<span class="token punctuation">)</span>
</code></pre></div><p>In our trainer configuration we will use canonical values for the <code>batch_size</code> and <code>lr</code> taken from the Hugging Face transformers library. We also will apply a linearly decaying learning rate scheduler with 50 warm-up steps, which is recommended when fine-tuning a pretrained model. For now we will stick to two epochs to allow for a rapid iteration.</p> <div class="language-python extra-class"><pre class="language-python"><code>trainer <span class="token operator">=</span> TrainerConfiguration<span class="token punctuation">(</span>
    optimizer<span class="token operator">=</span><span class="token punctuation">{</span>
        <span class="token string">&quot;type&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;adamw&quot;</span><span class="token punctuation">,</span>
        <span class="token string">&quot;lr&quot;</span><span class="token punctuation">:</span> <span class="token number">5e</span><span class="token operator">-</span><span class="token number">5</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    learning_rate_scheduler<span class="token operator">=</span><span class="token punctuation">{</span>
        <span class="token string">&quot;type&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;linear_with_warmup&quot;</span><span class="token punctuation">,</span>
        <span class="token string">&quot;num_epochs&quot;</span><span class="token punctuation">:</span> <span class="token number">2</span><span class="token punctuation">,</span>
        <span class="token string">&quot;num_steps_per_epoch&quot;</span><span class="token punctuation">:</span> <span class="token number">1250</span><span class="token punctuation">,</span>
        <span class="token string">&quot;warmup_steps&quot;</span><span class="token punctuation">:</span> <span class="token number">50</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    batch_size<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span>
    num_epochs<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
</code></pre></div><div class="language-python extra-class"><pre class="language-python"><code>pl<span class="token punctuation">.</span>train<span class="token punctuation">(</span>
    output<span class="token operator">=</span><span class="token string">&quot;output/fine_tuning&quot;</span><span class="token punctuation">,</span>
    training<span class="token operator">=</span>train_ds<span class="token punctuation">,</span>
    validation<span class="token operator">=</span>valid_ds<span class="token punctuation">,</span>
    trainer<span class="token operator">=</span>trainer<span class="token punctuation">,</span>
<span class="token punctuation">)</span>
</code></pre></div><p>After two epochs we achieve an accuracy of about 0.65, which is competetive looking at the corresponding <a href="https://www.kaggle.com/Cornell-University/arxiv/notebooks" target="_blank" rel="noopener noreferrer">Kaggle notebooks<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>. Keep in mind that we did not optimize any of the training parameters.</p> <h3 id="training-with-a-frozen-transformer"><a href="#training-with-a-frozen-transformer" class="header-anchor">#</a> Training with a frozen transformer</h3> <p>In our second pipeline we keep the weights of the transformer frozen by setting <code>trainable: False</code> and only train the pooler in the head component. In this setup the training will be significantly faster and does not necessarily require dedicated hardware.</p> <p>As pooler we will use a bidirectional <a href="https://en.wikipedia.org/wiki/Gated_recurrent_unit" target="_blank" rel="noopener noreferrer">GRU<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> in the head.</p> <div class="language-python extra-class"><pre class="language-python"><code>pipeline_dict_frozen <span class="token operator">=</span> <span class="token punctuation">{</span>
    <span class="token string">&quot;name&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;arxiv_categories_classification&quot;</span><span class="token punctuation">,</span>
    <span class="token string">&quot;features&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">{</span>
        <span class="token string">&quot;transformers&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">{</span>
            <span class="token string">&quot;model_name&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;distilroberta-base&quot;</span><span class="token punctuation">,</span>
            <span class="token string">&quot;trainable&quot;</span><span class="token punctuation">:</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
            <span class="token string">&quot;max_length&quot;</span><span class="token punctuation">:</span> <span class="token number">512</span><span class="token punctuation">,</span>
        <span class="token punctuation">}</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token string">&quot;head&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">{</span>
        <span class="token string">&quot;type&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;TextClassification&quot;</span><span class="token punctuation">,</span>
        <span class="token string">&quot;labels&quot;</span><span class="token punctuation">:</span> train_ds<span class="token punctuation">.</span>unique<span class="token punctuation">(</span><span class="token string">&quot;label&quot;</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token string">&quot;pooler&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">{</span>
            <span class="token string">&quot;type&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;gru&quot;</span><span class="token punctuation">,</span>
            <span class="token string">&quot;num_layers&quot;</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span>
            <span class="token string">&quot;hidden_size&quot;</span><span class="token punctuation">:</span> <span class="token number">128</span><span class="token punctuation">,</span>
            <span class="token string">&quot;bidirectional&quot;</span><span class="token punctuation">:</span> <span class="token boolean">True</span><span class="token punctuation">,</span>
        <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
<span class="token punctuation">}</span>
</code></pre></div><div class="language-python extra-class"><pre class="language-python"><code>pl <span class="token operator">=</span> Pipeline<span class="token punctuation">.</span>from_config<span class="token punctuation">(</span>pipeline_dict_frozen<span class="token punctuation">)</span>
</code></pre></div><p>In our training configuration we will use the same <code>batch_size</code> as in the previous configuration but increase the learning rate to Pytorch's default value for the AdamW optimizer, in order to work well with the GRU. We also remove the learning rate scheduler with its warmup steps, since we do not modify the pretrained transformer weights.</p> <div class="language-python extra-class"><pre class="language-python"><code>trainer <span class="token operator">=</span> TrainerConfiguration<span class="token punctuation">(</span>
    optimizer<span class="token operator">=</span><span class="token punctuation">{</span>
        <span class="token string">&quot;type&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;adamw&quot;</span><span class="token punctuation">,</span>
        <span class="token string">&quot;lr&quot;</span><span class="token punctuation">:</span> <span class="token number">0.002</span><span class="token punctuation">,</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    batch_size<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span>
    num_epochs<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
</code></pre></div><div class="language-python extra-class"><pre class="language-python"><code>pl<span class="token punctuation">.</span>train<span class="token punctuation">(</span>
    output<span class="token operator">=</span><span class="token string">&quot;output/frozen_transformer&quot;</span><span class="token punctuation">,</span>
    training<span class="token operator">=</span>train_ds<span class="token punctuation">,</span>
    validation<span class="token operator">=</span>valid_ds<span class="token punctuation">,</span>
    trainer<span class="token operator">=</span>trainer<span class="token punctuation">,</span>
<span class="token punctuation">)</span>
</code></pre></div><p>The training is about 4 times faster compared with fine-tuning the transformer, and after two epochs we reach a respectable accuracy of about 0.60. Keep in mind that we did not optimize any of the training parameters.</p> <h3 id="combining-text-features"><a href="#combining-text-features" class="header-anchor">#</a> Combining text features</h3> <p>As mentioned earlier, the pretrained transformers are treated as a text feature in <em>biome.text</em>. We can easily combine them with other features, such as the <em>char</em> feature for example, which encodes word tokens based on their characters.</p> <p>Keep in mind that the <em>char</em> feature provides a feature vector per word (spaCy) token, while the <em>transformers</em> feature provides a contextualized feature vector per word piece. Therefore, we simply sum up the word piece vectors of the transformers feature, to end up with concatenated feature vectors per word token.</p> <div class="custom-block warning"><p class="custom-block-title">Note</p> <p>This also means that special transformer tokens, such as BERT's [CLS] token, are ignored when combining text features.</p></div> <p>As in the second configuration, we will pool the feature vectors with a <em>GRU</em> in the <em>head</em> component.</p> <div class="language-python extra-class"><pre class="language-python"><code>pipeline_dict_combining <span class="token operator">=</span> <span class="token punctuation">{</span>
    <span class="token string">&quot;name&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;arxiv_categories_classification&quot;</span><span class="token punctuation">,</span>
    <span class="token string">&quot;tokenizer&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token string">&quot;features&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">{</span>
        <span class="token string">&quot;char&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">{</span>
            <span class="token string">&quot;embedding_dim&quot;</span><span class="token punctuation">:</span> <span class="token number">32</span><span class="token punctuation">,</span>
            <span class="token string">&quot;lowercase_characters&quot;</span><span class="token punctuation">:</span> <span class="token boolean">True</span><span class="token punctuation">,</span>
            <span class="token string">&quot;encoder&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">{</span>
                <span class="token string">&quot;type&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;gru&quot;</span><span class="token punctuation">,</span>
                <span class="token string">&quot;num_layers&quot;</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span>
                <span class="token string">&quot;hidden_size&quot;</span><span class="token punctuation">:</span> <span class="token number">32</span><span class="token punctuation">,</span>
                <span class="token string">&quot;bidirectional&quot;</span><span class="token punctuation">:</span> <span class="token boolean">True</span><span class="token punctuation">,</span>
            <span class="token punctuation">}</span><span class="token punctuation">,</span>
            <span class="token string">&quot;dropout&quot;</span><span class="token punctuation">:</span> <span class="token number">0.1</span><span class="token punctuation">,</span>
        <span class="token punctuation">}</span><span class="token punctuation">,</span>
        <span class="token string">&quot;transformers&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">{</span>
            <span class="token string">&quot;model_name&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;distilroberta-base&quot;</span><span class="token punctuation">,</span>
            <span class="token string">&quot;trainable&quot;</span><span class="token punctuation">:</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
            <span class="token string">&quot;max_length&quot;</span><span class="token punctuation">:</span> <span class="token number">512</span><span class="token punctuation">,</span>
        <span class="token punctuation">}</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token string">&quot;head&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">{</span>
        <span class="token string">&quot;type&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;TextClassification&quot;</span><span class="token punctuation">,</span>
        <span class="token string">&quot;labels&quot;</span><span class="token punctuation">:</span> train_ds<span class="token punctuation">.</span>unique<span class="token punctuation">(</span><span class="token string">&quot;label&quot;</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token string">&quot;pooler&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">{</span>
            <span class="token string">&quot;type&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;gru&quot;</span><span class="token punctuation">,</span>
            <span class="token string">&quot;num_layers&quot;</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span>
            <span class="token string">&quot;hidden_size&quot;</span><span class="token punctuation">:</span> <span class="token number">128</span><span class="token punctuation">,</span>
            <span class="token string">&quot;bidirectional&quot;</span><span class="token punctuation">:</span> <span class="token boolean">True</span><span class="token punctuation">,</span>
        <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
<span class="token punctuation">}</span>
</code></pre></div><div class="language-python extra-class"><pre class="language-python"><code>pl <span class="token operator">=</span> Pipeline<span class="token punctuation">.</span>from_config<span class="token punctuation">(</span>pipeline_dict_combining<span class="token punctuation">)</span>
</code></pre></div><p>We will use the same training configuration as in the frozen transformer section.</p> <div class="language-python extra-class"><pre class="language-python"><code>trainer <span class="token operator">=</span> TrainerConfiguration<span class="token punctuation">(</span>
    optimizer<span class="token operator">=</span><span class="token punctuation">{</span>
        <span class="token string">&quot;type&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;adamw&quot;</span><span class="token punctuation">,</span>
        <span class="token string">&quot;lr&quot;</span><span class="token punctuation">:</span> <span class="token number">0.001</span><span class="token punctuation">,</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    batch_size<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span>
    num_epochs<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
</code></pre></div><div class="language-python extra-class"><pre class="language-python"><code>pl<span class="token punctuation">.</span>train<span class="token punctuation">(</span>
    output<span class="token operator">=</span><span class="token string">&quot;output/combined_features&quot;</span><span class="token punctuation">,</span>
    training<span class="token operator">=</span>train_ds<span class="token punctuation">,</span>
    validation<span class="token operator">=</span>valid_ds<span class="token punctuation">,</span>
    trainer<span class="token operator">=</span>trainer<span class="token punctuation">,</span>
<span class="token punctuation">)</span>
</code></pre></div><p>With an accuracy of 0.55, combining features in this case seems to be counterproductive. The main reason is the exclusion of the special transformers tokens and the usage of feature vectors per word instead of word-pieces. Even when fine-tuning the transformer, those differences seem to significantly affect the performance as shown in our <a href="https://wandb.ai/ignacioct/biome/reports/Exploring-Ways-to-use-Pretrained-Transformers-in-biome-text--VmlldzoyNzk2MTM" target="_blank" rel="noopener noreferrer">WandB report<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>.</p> <h3 id="compare-performances-with-tensorboard-optional"><a href="#compare-performances-with-tensorboard-optional" class="header-anchor">#</a> Compare performances with TensorBoard (optional)</h3> <p>In the output folder of the trainig we automatically log the results with <a href="https://www.tensorflow.org/tensorboard/" target="_blank" rel="noopener noreferrer">TensorBoard<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>. This helps us to conveniently compare the three training runs from above. Alternatively, if you installed and logged in to WandB, the runs should have been logged automatically to the <em>biome</em> project of your account.</p> <div class="language-python extra-class"><pre class="language-python"><code><span class="token operator">%</span>load_ext tensorboard
<span class="token operator">%</span>tensorboard <span class="token operator">-</span><span class="token operator">-</span>logdir<span class="token operator">=</span>output
</code></pre></div><h2 id="optimizing-the-trainer-configuration"><a href="#optimizing-the-trainer-configuration" class="header-anchor">#</a> Optimizing the trainer configuration</h2> <p>As described in the <a href="https://www.recogn.ai/biome-text/v2.0.1rc1/documentation/tutorials/3-Hyperparameter_optimization_with_Ray_Tune.html#imports" target="_blank" rel="noopener noreferrer">HPO tutorial<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>, <em>biome.text</em> relies on the <a href="https://docs.ray.io/en/latest/tune.html#tune-index" target="_blank" rel="noopener noreferrer">Ray Tune library<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> to perform hyperparameter optimization.
We recommend to go through that tutorial first, as we will be skipping most of the implementation details here.</p> <h3 id="frozen-transformer"><a href="#frozen-transformer" class="header-anchor">#</a> Frozen transformer</h3> <p>In this section we will first try to improve the performance of the frozen-transformer configuration by conducting a random search for three training parameters:</p> <ul><li>learning rate</li> <li>weight decay</li> <li>batch size</li></ul> <p>We also set <code>num_serialized_models_to_keep</code> to 0 to reduce the disk storage footprint.</p> <div class="language-python extra-class"><pre class="language-python"><code>trainer_dict <span class="token operator">=</span> <span class="token punctuation">{</span>
    <span class="token string">&quot;optimizer&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">{</span>
        <span class="token string">&quot;type&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;adamw&quot;</span><span class="token punctuation">,</span>
        <span class="token string">&quot;lr&quot;</span><span class="token punctuation">:</span> tune<span class="token punctuation">.</span>loguniform<span class="token punctuation">(</span><span class="token number">5e</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">5e</span><span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token string">&quot;weight_decay&quot;</span><span class="token punctuation">:</span> tune<span class="token punctuation">.</span>loguniform<span class="token punctuation">(</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">)</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token string">&quot;batch_size&quot;</span><span class="token punctuation">:</span> tune<span class="token punctuation">.</span>choice<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token string">&quot;num_epochs&quot;</span><span class="token punctuation">:</span> <span class="token number">2</span><span class="token punctuation">,</span>
    <span class="token string">&quot;num_serialized_models_to_keep&quot;</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span>
<span class="token punctuation">}</span>
</code></pre></div><p>Having defined the search space for our hyperparameters, we create a <code>TuneExperiment</code> where we specify the number of samples to be dranw from our search space, the <code>local_dir</code> for our HPO output and the computing resources we want Ray Tune to have access to.</p> <div class="language-python extra-class"><pre class="language-python"><code>tune_exp <span class="token operator">=</span> TuneExperiment<span class="token punctuation">(</span>
    pipeline_config<span class="token operator">=</span>pipeline_dict_frozen<span class="token punctuation">,</span> 
    trainer_config<span class="token operator">=</span>trainer_dict<span class="token punctuation">,</span>
    train_dataset<span class="token operator">=</span>train_ds<span class="token punctuation">,</span>
    valid_dataset<span class="token operator">=</span>valid_ds<span class="token punctuation">,</span>
    name<span class="token operator">=</span><span class="token string">&quot;frozen_transformer_sweep&quot;</span><span class="token punctuation">,</span>
    <span class="token comment"># parameters for tune.run</span>
    num_samples<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span>
    local_dir<span class="token operator">=</span><span class="token string">&quot;tune_runs&quot;</span><span class="token punctuation">,</span>
    resources_per_trial<span class="token operator">=</span><span class="token punctuation">{</span><span class="token string">&quot;gpu&quot;</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">&quot;cpu&quot;</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
</code></pre></div><p>With our TuneExperiment object at hand, we simply have to pass it on to the <a href="https://docs.ray.io/en/master/tune/api_docs/execution.html#tune-run" target="_blank" rel="noopener noreferrer"><code>tune.run</code><span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> function to start our random search.</p> <p>To speed things up we will use the <a href="https://blog.ml.cmu.edu/2018/12/12/massively-parallel-hyperparameter-optimization/" target="_blank" rel="noopener noreferrer">ASHA<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> trial scheduler that terminates low performing trials early. In our case we take the <em>validation_accuracy</em> as a meassure of the models performance.</p> <p>In Google Colab with a GPU backend this random search should not take more than about 1.5 hours and we recommend following the progress via WandB. Alternatively, you could follow the progress via <a href="https://www.tensorflow.org/tensorboard/" target="_blank" rel="noopener noreferrer">TensorBoard<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> by launching a TensorBoard instance before starting the random search, and pointing it to the <em>local_dir</em> output:</p> <div class="language-python extra-class"><pre class="language-python"><code><span class="token operator">%</span>tensorboard <span class="token operator">-</span><span class="token operator">-</span>logdir<span class="token operator">=</span>tune_runs
</code></pre></div><div class="language-python extra-class"><pre class="language-python"><code>analysis_frozen <span class="token operator">=</span> tune<span class="token punctuation">.</span>run<span class="token punctuation">(</span>
    tune_exp<span class="token punctuation">,</span>
    scheduler<span class="token operator">=</span>tune<span class="token punctuation">.</span>schedulers<span class="token punctuation">.</span>ASHAScheduler<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
    metric<span class="token operator">=</span><span class="token string">&quot;validation_accuracy&quot;</span><span class="token punctuation">,</span>
    mode<span class="token operator">=</span><span class="token string">&quot;max&quot;</span><span class="token punctuation">,</span>
    progress_reporter<span class="token operator">=</span>tune<span class="token punctuation">.</span>JupyterNotebookReporter<span class="token punctuation">(</span>overwrite<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token punctuation">)</span>
</code></pre></div><p>The best configuration in our random search achieved an accuracy of about 0.63 with following parameters:</p> <ul><li>learning rate: 0.002541</li> <li>batch size: 16</li> <li>weight decay: 0.04194</li></ul> <h3 id="fine-tuning-the-transformer-2"><a href="#fine-tuning-the-transformer-2" class="header-anchor">#</a> Fine-tuning the transformer</h3> <p>We will also try to optimize the training parameters for a fine-tuning of the transformer. Since this is computationally much more expensive, we will take only a subset of our training data for the random search.</p> <div class="language-python extra-class"><pre class="language-python"><code>train_1000 <span class="token operator">=</span> train_ds<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>seed<span class="token operator">=</span><span class="token number">43</span><span class="token punctuation">)</span><span class="token punctuation">.</span>select<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre></div><p>The training parameters we are going to tune are the following:</p> <ul><li>learning rate</li> <li>weight decay</li> <li>warmup steps</li></ul> <div class="language-python extra-class"><pre class="language-python"><code>trainer_dict <span class="token operator">=</span> <span class="token punctuation">{</span>
    <span class="token string">&quot;optimizer&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">{</span>
        <span class="token string">&quot;type&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;adamw&quot;</span><span class="token punctuation">,</span>
        <span class="token string">&quot;lr&quot;</span><span class="token punctuation">:</span> tune<span class="token punctuation">.</span>loguniform<span class="token punctuation">(</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token string">&quot;weight_decay&quot;</span><span class="token punctuation">:</span> tune<span class="token punctuation">.</span>loguniform<span class="token punctuation">(</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">)</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token string">&quot;learning_rate_scheduler&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">{</span>
        <span class="token string">&quot;type&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;linear_with_warmup&quot;</span><span class="token punctuation">,</span>
        <span class="token string">&quot;num_epochs&quot;</span><span class="token punctuation">:</span> <span class="token number">2</span><span class="token punctuation">,</span>
        <span class="token string">&quot;num_steps_per_epoch&quot;</span><span class="token punctuation">:</span> <span class="token number">125</span><span class="token punctuation">,</span>
        <span class="token string">&quot;warmup_steps&quot;</span><span class="token punctuation">:</span> tune<span class="token punctuation">.</span>choice<span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">101</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token string">&quot;batch_size&quot;</span><span class="token punctuation">:</span> <span class="token number">8</span><span class="token punctuation">,</span>
    <span class="token string">&quot;num_epochs&quot;</span><span class="token punctuation">:</span> <span class="token number">2</span><span class="token punctuation">,</span>
    <span class="token string">&quot;num_serialized_models_to_keep&quot;</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span>
<span class="token punctuation">}</span>
</code></pre></div><p>After having defined the search space, we create a <code>TuneExperiment</code> providing this time the subset of the training data.</p> <div class="language-python extra-class"><pre class="language-python"><code>tune_exp <span class="token operator">=</span> TuneExperiment<span class="token punctuation">(</span>
    pipeline_config<span class="token operator">=</span>pipeline_dict_finetuning<span class="token punctuation">,</span> 
    trainer_config<span class="token operator">=</span>trainer_dict<span class="token punctuation">,</span>
    train_dataset<span class="token operator">=</span>train_ds<span class="token punctuation">,</span>
    valid_dataset<span class="token operator">=</span>valid_ds<span class="token punctuation">,</span>
    name<span class="token operator">=</span><span class="token string">&quot;finetuning_sweep3&quot;</span><span class="token punctuation">,</span>
    <span class="token comment"># parameters for tune.run</span>
    num_samples<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span>
    local_dir<span class="token operator">=</span><span class="token string">&quot;tune_runs&quot;</span><span class="token punctuation">,</span>
    resources_per_trial<span class="token operator">=</span><span class="token punctuation">{</span><span class="token string">&quot;gpu&quot;</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token string">&quot;cpu&quot;</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
</code></pre></div><p>Again, we will use the <a href="https://blog.ml.cmu.edu/2018/12/12/massively-parallel-hyperparameter-optimization/" target="_blank" rel="noopener noreferrer">ASHA<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> trial scheduler and maximize the <em>validation_accuracy</em>.</p> <p>In Google Colab with a GPU backend, this random search should not take longer than 1.5 hours.</p> <div class="language-python extra-class"><pre class="language-python"><code>analysis_finetuning <span class="token operator">=</span> tune<span class="token punctuation">.</span>run<span class="token punctuation">(</span>
    tune_exp<span class="token punctuation">,</span>
    scheduler<span class="token operator">=</span>tune<span class="token punctuation">.</span>schedulers<span class="token punctuation">.</span>ASHAScheduler<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    metric<span class="token operator">=</span><span class="token string">&quot;validation_accuracy&quot;</span><span class="token punctuation">,</span>
    mode<span class="token operator">=</span><span class="token string">&quot;max&quot;</span><span class="token punctuation">,</span>
    progress_reporter<span class="token operator">=</span>tune<span class="token punctuation">.</span>JupyterNotebookReporter<span class="token punctuation">(</span>overwrite<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
</code></pre></div><p>We now take the configuration that yielded the best <em>validation accuracy</em> and train the pipeline on the full training set. In our random search the best configuration was following:</p> <ul><li>learning rate: 0.0000453</li> <li>warmup steps: 45</li> <li>weight decay: 0.003197</li></ul> <div class="language-python extra-class"><pre class="language-python"><code>pl <span class="token operator">=</span> Pipeline<span class="token punctuation">.</span>from_config<span class="token punctuation">(</span>pipeline_dict_finetuning<span class="token punctuation">)</span>
</code></pre></div><div class="language-python extra-class"><pre class="language-python"><code>trainer <span class="token operator">=</span> TrainerConfiguration<span class="token punctuation">(</span>
    optimizer<span class="token operator">=</span><span class="token punctuation">{</span>
        <span class="token string">&quot;type&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;adamw&quot;</span><span class="token punctuation">,</span>
        <span class="token string">&quot;lr&quot;</span><span class="token punctuation">:</span> <span class="token number">0.0000453</span><span class="token punctuation">,</span>
        <span class="token string">&quot;weight_decay&quot;</span><span class="token punctuation">:</span> <span class="token number">0.003197</span><span class="token punctuation">,</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    learning_rate_scheduler<span class="token operator">=</span><span class="token punctuation">{</span>
        <span class="token string">&quot;type&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;linear_with_warmup&quot;</span><span class="token punctuation">,</span>
        <span class="token string">&quot;num_epochs&quot;</span><span class="token punctuation">:</span> <span class="token number">2</span><span class="token punctuation">,</span>
        <span class="token string">&quot;num_steps_per_epoch&quot;</span><span class="token punctuation">:</span> <span class="token number">1250</span><span class="token punctuation">,</span>
        <span class="token string">&quot;warmup_steps&quot;</span><span class="token punctuation">:</span> <span class="token number">45</span><span class="token punctuation">,</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    batch_size<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span>
    num_epochs<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
</code></pre></div><div class="language-python extra-class"><pre class="language-python"><code>pl<span class="token punctuation">.</span>train<span class="token punctuation">(</span>
    output<span class="token operator">=</span><span class="token string">&quot;output/transformer_final_model/&quot;</span><span class="token punctuation">,</span>
    training<span class="token operator">=</span>train_ds<span class="token punctuation">,</span>
    validation<span class="token operator">=</span>valid_ds<span class="token punctuation">,</span>
    trainer<span class="token operator">=</span>trainer<span class="token punctuation">,</span>
<span class="token punctuation">)</span>
</code></pre></div><p>With the optimized training parameters we achieve an accuracy of about 0.67.</p> <h3 id="evaluating-with-a-test-set"><a href="#evaluating-with-a-test-set" class="header-anchor">#</a> Evaluating with a test set</h3> <p>Having optimized the training parameters of both models, we will now evaluate them on an independent test set.</p> <p>For the frozen-transformer configuration we can use the <code>analysis_frozen</code> object of the random search to directly access the best performing model:</p> <div class="language-python extra-class"><pre class="language-python"><code>best_model_path <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>analysis_frozen<span class="token punctuation">.</span>get_best_logdir<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">&quot;training&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;model.tar.gz&quot;</span><span class="token punctuation">)</span>

pl_frozen <span class="token operator">=</span> Pipeline<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>best_model_path<span class="token punctuation">)</span>
</code></pre></div><p>With the best performing pipeline at hand we will call its evaluate method together with the test data set. By default the evaluation will be done with a batch size of 16 and on a CUDA device if one is available. You can also specify a <code>predictions_output_file</code> argument to save all the batch predictions made during evaluation.</p> <div class="language-python extra-class"><pre class="language-python"><code>pl_frozen<span class="token punctuation">.</span>evaluate<span class="token punctuation">(</span>test_ds<span class="token punctuation">,</span> predictions_output_file<span class="token operator">=</span><span class="token string">&quot;predictions_from_evaluation.txt&quot;</span><span class="token punctuation">)</span>
</code></pre></div><p>On the test set we achieve an accuracy of about 0.65, which is a bit better than the 0.63 on our validation set.</p> <p>Let us also quickly check the accuracy of our best fine-tuned model:</p> <div class="language-python extra-class"><pre class="language-python"><code>pl_finetuned <span class="token operator">=</span> Pipeline<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">&quot;output/transformer_final_model/model.tar.gz&quot;</span><span class="token punctuation">)</span>

pl_finetuned<span class="token punctuation">.</span>evaluate<span class="token punctuation">(</span>test_ds<span class="token punctuation">)</span>
</code></pre></div><p>Here we achieve roughly the same accuracy of 0.67 as with the validation data set. So it seems both models generalized well during the random search and there is no strong bias towards the validation data set.</p> <h2 id="making-predictions"><a href="#making-predictions" class="header-anchor">#</a> Making predictions</h2> <p>Let's quickly recap what we have learnt so far:</p> <ul><li>Freezing the pretrained transformer and optimizing a GRU pooler in the head can be valid option if computing resources are limited;</li> <li>However, fine-tuning the transformer at word-piece level and using the CLS token as &quot;pooler&quot; works best;</li> <li>A quick HPO of the training parameters improved the accuracies by ~0.03.</li></ul> <p>With our best model at hand we will finally make a simple prediction. We can call the <code>predict</code> method of our pipeline that outputs a dictionary with a labels and probabilities key containing a list of labels and their corresponding probabilities, ordered from most to less likely.</p> <div class="language-python extra-class"><pre class="language-python"><code>pl_finetuned<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>text<span class="token operator">=</span><span class="token string">&quot;This is a title of a super intelligent Natural Language Processing system&quot;</span><span class="token punctuation">)</span>
</code></pre></div><p>The most likely category predicted is the &quot;<em>cs.CL</em>&quot; category, which seems fitting according to this <a href="https://arxiv.org/category_taxonomy" target="_blank" rel="noopener noreferrer">list of arxiv categories and their meanings<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>.</p></div> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><span class="page-nav__button prev"><a href="/biome-text/v2.0.1rc1/documentation/tutorials/3-Hyperparameter_optimization_with_Ray_Tune.html" class="prev"><span class="page-nav__button__icon"><vp-icon color="#4A4A4A" name="chev-left" size="18px"></vp-icon></span> <span class="page-nav__button__text">
          Hyperparameter optimization with Ray Tune
        </span></a></span> <span class="page-nav__button next"><a href="/biome-text/v2.0.1rc1/documentation/user-guides/1-nlp-tasks.html"><span class="page-nav__button__text">
          NLP Tasks
        </span> <span class="page-nav__button__icon"><vp-icon color="#4A4A4A" name="chev-right" size="18px"></vp-icon></span></a></span></p></div> <footer class="footer" data-v-348088ed><div data-v-348088ed>
          Maintained by
          <a href="https://www.recogn.ai/" target="_blank" data-v-348088ed><img width="70px" src="/biome-text/v2.0.1rc1/assets/img/recognai.png" class="footer__img" data-v-348088ed></a></div></footer> </main></div><div class="global-ui"><!----></div></div>
    <script src="/biome-text/v2.0.1rc1/assets/js/app.da9c7cc4.js" defer></script><script src="/biome-text/v2.0.1rc1/assets/js/4.654db8cc.js" defer></script><script src="/biome-text/v2.0.1rc1/assets/js/3.ec9f5f07.js" defer></script><script src="/biome-text/v2.0.1rc1/assets/js/66.50b0b792.js" defer></script>
  </body>
</html>
