(window.webpackJsonp=window.webpackJsonp||[]).push([[67],{470:function(t,e,n){"use strict";n.r(e);var a=n(26),s=Object(a.a)({},(function(){var t=this,e=t.$createElement,n=t._self._c||e;return n("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[n("h1",{attrs:{id:"using-transformers-in-biome-text"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#using-transformers-in-biome-text"}},[t._v("#")]),t._v(" Using Transformers in biome.text")]),t._v(" "),n("p",[n("a",{attrs:{target:"_blank",href:"https://www.recogn.ai/biome-text/documentation/tutorials/4-Using_Transformers_in_biome_text.html"}},[n("img",{staticClass:"icon",attrs:{src:"https://www.recogn.ai/biome-text/assets/img/biome-isotype.svg",width:"24"}})]),t._v(" "),n("a",{attrs:{href:"https://www.recogn.ai/biome-text/documentation/tutorials/4-Using_Transformers_in_biome_text.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("View on recogn.ai"),n("OutboundLink")],1)]),t._v(" "),n("p",[n("a",{attrs:{target:"_blank",href:"https://colab.research.google.com/github/recognai/biome-text/blob/master/docs/docs/documentation/tutorials/4-Using_Transformers_in_biome_text.ipynb"}},[n("img",{staticClass:"icon",attrs:{src:"https://www.tensorflow.org/images/colab_logo_32px.png",width:"24"}})]),t._v(" "),n("a",{attrs:{href:"https://colab.research.google.com/github/recognai/biome-text/blob/master/docs/docs/documentation/tutorials/4-Using_Transformers_in_biome_text.ipynb",target:"_blank",rel:"noopener noreferrer"}},[t._v("Run in Google Colab"),n("OutboundLink")],1)]),t._v(" "),n("p",[n("a",{attrs:{target:"_blank",href:"https://github.com/recognai/biome-text/blob/master/docs/docs/documentation/tutorials/4-Using_Transformers_in_biome_text.ipynb"}},[n("img",{staticClass:"icon",attrs:{src:"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png",width:"24"}})]),t._v(" "),n("a",{attrs:{href:"https://github.com/recognai/biome-text/blob/master/docs/docs/documentation/tutorials/4-Using_Transformers_in_biome_text.ipynb",target:"_blank",rel:"noopener noreferrer"}},[t._v("View source on GitHub"),n("OutboundLink")],1)]),t._v(" "),n("p",[t._v("When running this tutorial in Google Colab, make sure to install "),n("em",[t._v("biome.text")]),t._v(" first:")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("!pip install "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("U pip\n!pip install "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("use"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("feature"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2020")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("resolver git"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v("https"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("//")]),t._v("github"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("com"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("recognai"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("biome"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("text"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("git\n")])])]),n("p",[t._v("Ignore warnings and don't forget to restart your runtime afterwards "),n("em",[t._v("(Runtime -> Restart runtime)")]),t._v(" .")]),t._v(" "),n("p",[n("em",[t._v("If")]),t._v(" you want to log your runs with "),n("a",{attrs:{href:"https://wandb.ai/home",target:"_blank",rel:"noopener noreferrer"}},[t._v("WandB"),n("OutboundLink")],1),t._v(", don't forget to install its client and log in.")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("!pip install wandb\n!wandb login\n")])])]),n("h2",{attrs:{id:"introduction"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#introduction"}},[t._v("#")]),t._v(" Introduction")]),t._v(" "),n("p",[t._v("In the last years we experienced a shift towards transfer learning as the standard approach to solve NLP problems. Before models were usually trained entirely from scratch, utilizing at most pretrained word embeddings. But nowadays it is very common to start with large pretrained language models as backbone of a system, and to set a task specific head on top of it. This new paradigm has made it easier to find state-of-the-art architectures for a great variety of NLP tasks.")]),t._v(" "),n("p",[t._v("Almost all current language models are based on the transformer architecture. The awesome "),n("a",{attrs:{href:"https://github.com/huggingface/transformers",target:"_blank",rel:"noopener noreferrer"}},[t._v("Hugging Face Transformers"),n("OutboundLink")],1),t._v(" library provides access to hundreds of such pretrained language models including state-of-the-art models such as infamous "),n("a",{attrs:{href:"https://github.com/google-research/bert",target:"_blank",rel:"noopener noreferrer"}},[t._v("BERT"),n("OutboundLink")],1),t._v(", as well as community driven models often covering a specific language type or resource requirements.")]),t._v(" "),n("p",[t._v("In this tutorial, we are going to classify "),n("a",{attrs:{href:"https://arxiv.org/",target:"_blank",rel:"noopener noreferrer"}},[t._v("arXiv"),n("OutboundLink")],1),t._v(" papers into "),n("a",{attrs:{href:"https://arxiv.org/category_taxonomy",target:"_blank",rel:"noopener noreferrer"}},[t._v("categories"),n("OutboundLink")],1),t._v(", analyzing the title of the paper and its abstract. We will use Hugging Face "),n("a",{attrs:{href:"https://medium.com/huggingface/distilbert-8cf3380435b5",target:"_blank",rel:"noopener noreferrer"}},[t._v("distilled"),n("OutboundLink")],1),t._v(" implementation of "),n("a",{attrs:{href:"https://ai.facebook.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/",target:"_blank",rel:"noopener noreferrer"}},[t._v("RoBERTa"),n("OutboundLink")],1),t._v(" and explore ways how to easily include pretrained transformers in a "),n("em",[t._v("biome.text")]),t._v(" pipeline.")]),t._v(" "),n("h3",{attrs:{id:"external-links-about-transformers"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#external-links-about-transformers"}},[t._v("#")]),t._v(" External links about transformers")]),t._v(" "),n("p",[t._v('If this is the first time you hear about "Transformers" not referring to giant robots, here is a small list of resources at which you might want to have a look first:')]),t._v(" "),n("ul",[n("li",[n("a",{attrs:{href:"https://arxiv.org/pdf/1706.03762.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Attention is all your need"),n("OutboundLink")],1),t._v(": paper that introduced the architecture.")]),t._v(" "),n("li",[n("a",{attrs:{href:"https://jalammar.github.io/illustrated-transformer/",target:"_blank",rel:"noopener noreferrer"}},[t._v("The Illustrated Transformer"),n("OutboundLink")],1),t._v(": 20-30 minute article covering how they work.")]),t._v(" "),n("li",[n("a",{attrs:{href:"https://youtu.be/4Bdc55j80l8",target:"_blank",rel:"noopener noreferrer"}},[t._v("Illustrated Guide to Transformer Neural Network: a step by step explanation"),n("OutboundLink")],1),t._v(": 15 minute long video covering how they work.")]),t._v(" "),n("li",[n("a",{attrs:{href:"https://www.youtube.com/watch?v=8Hg2UtQg6G4",target:"_blank",rel:"noopener noreferrer"}},[t._v("An Introduction To Transfer Learning In NLP and HuggingFace"),n("OutboundLink")],1),t._v(": 1 hour talk by Thomas Wolf")])]),t._v(" "),n("h3",{attrs:{id:"imports"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#imports"}},[t._v("#")]),t._v(" Imports")]),t._v(" "),n("p",[t._v("Let us first import all the stuff we need for this tutorial:")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" biome"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Dataset"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Pipeline\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" biome"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("configuration "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" VocabularyConfiguration"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" TrainerConfiguration\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" biome"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("text"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("hpo "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" TuneExperiment\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" ray "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" tune\n")])])]),n("div",{staticClass:"language- extra-class"},[n("pre",[n("code",[t._v("/home/david/miniconda3/envs/biome/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n  return f(*args, **kwds)\n/home/david/miniconda3/envs/biome/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n  return f(*args, **kwds)\n/home/david/miniconda3/envs/biome/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n  return f(*args, **kwds)\n/home/david/miniconda3/envs/biome/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n  return f(*args, **kwds)\n")])])]),n("h2",{attrs:{id:"exploring-and-preparing-the-data"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#exploring-and-preparing-the-data"}},[t._v("#")]),t._v(" Exploring and preparing the data")]),t._v(" "),n("p",[t._v("For this tutorial we are going to use the "),n("a",{attrs:{href:"https://www.kaggle.com/Cornell-University/arxiv",target:"_blank",rel:"noopener noreferrer"}},[t._v("arXiv dataset"),n("OutboundLink")],1),t._v(" compiled by the Cornell University, which consists of metadata of scientific papers stored in "),n("a",{attrs:{href:"https://arxiv.org/",target:"_blank",rel:"noopener noreferrer"}},[t._v("arXiv"),n("OutboundLink")],1),t._v(".")]),t._v(" "),n("p",[t._v("We preprocessed the data in a separate "),n("a",{attrs:{href:"https://drive.google.com/file/d/1zUSz81x15RH5mL5GoN7i7xqiNGEqclU0/view?usp=sharing",target:"_blank",rel:"noopener noreferrer"}},[t._v("notebook"),n("OutboundLink")],1),t._v(" producing three csv files (train, validate and test datasets) that contain the title, the abstract and the category of the corresponding paper.")]),t._v(" "),n("p",[t._v("Our NLP task will be to classify the papers into the given categories based on the title and abstract. Below we download the preprocessed data and create our "),n("a",{attrs:{href:"https://www.recogn.ai/biome-text/api/biome/text/dataset.html#dataset",target:"_blank",rel:"noopener noreferrer"}},[t._v("Datasets"),n("OutboundLink")],1),t._v(" with it.")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Downloading the datasets")]),t._v("\n!curl "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("O https"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("//")]),t._v("biome"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("tutorials"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("data"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("s3"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("eu"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("west"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.")]),t._v("amazonaws"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("com"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("transformers_arxiv"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("classifier"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("arxiv"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("dataset"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("train"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("json\n!curl "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("O https"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("//")]),t._v("biome"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("tutorials"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("data"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("s3"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("eu"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("west"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.")]),t._v("amazonaws"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("com"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("transformers_arxiv"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("classifier"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("arxiv"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("dataset"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("validate"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("json\n!curl "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("O https"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("//")]),t._v("biome"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("tutorials"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("data"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("s3"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("eu"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("west"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.")]),t._v("amazonaws"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("com"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("transformers_arxiv"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("classifier"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("arxiv"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("dataset"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("test"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("json\n")])])]),n("div",{staticClass:"language- extra-class"},[n("pre",[n("code",[t._v("  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  9.9M  100  9.9M    0     0  7844k      0  0:00:01  0:00:01 --:--:-- 7838k\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 1270k  100 1270k    0     0  1043k      0  0:00:01  0:00:01 --:--:-- 1043k\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 1236k  100 1236k    0     0  1434k      0 --:--:-- --:--:-- --:--:-- 1432k\n")])])]),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Loading from local")]),t._v("\ntrain_ds "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Dataset"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_json"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"arxiv-dataset-train.json"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nvalid_ds "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Dataset"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_json"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"arxiv-dataset-validate.json"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntest_ds "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Dataset"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_json"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"arxiv-dataset-test.json"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("div",{staticClass:"language- extra-class"},[n("pre",[n("code",[t._v("Using custom data configuration default\n\n\nDownloading and preparing dataset json/default-eeca6b23a235c21b (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/david/.cache/huggingface/datasets/json/default-eeca6b23a235c21b/0.0.0/fb88b12bd94767cb0cc7eedcd82ea1f402d2162addc03a37e81d4f8dc7313ad9...\n\n\n\nHBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…\n\n\nDataset json downloaded and prepared to /home/david/.cache/huggingface/datasets/json/default-eeca6b23a235c21b/0.0.0/fb88b12bd94767cb0cc7eedcd82ea1f402d2162addc03a37e81d4f8dc7313ad9. Subsequent calls will reuse this data.\n\n\nUsing custom data configuration default\n\n\nDownloading and preparing dataset json/default-750351052310c435 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/david/.cache/huggingface/datasets/json/default-750351052310c435/0.0.0/fb88b12bd94767cb0cc7eedcd82ea1f402d2162addc03a37e81d4f8dc7313ad9...\n\n\n\nHBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…\n\n\nDataset json downloaded and prepared to /home/david/.cache/huggingface/datasets/json/default-750351052310c435/0.0.0/fb88b12bd94767cb0cc7eedcd82ea1f402d2162addc03a37e81d4f8dc7313ad9. Subsequent calls will reuse this data.\n\n\nUsing custom data configuration default\n\n\nDownloading and preparing dataset json/default-b364982e9c8b4412 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/david/.cache/huggingface/datasets/json/default-b364982e9c8b4412/0.0.0/fb88b12bd94767cb0cc7eedcd82ea1f402d2162addc03a37e81d4f8dc7313ad9...\n\n\n\nHBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…\n\n\nDataset json downloaded and prepared to /home/david/.cache/huggingface/datasets/json/default-b364982e9c8b4412/0.0.0/fb88b12bd94767cb0cc7eedcd82ea1f402d2162addc03a37e81d4f8dc7313ad9. Subsequent calls will reuse this data.\n")])])]),n("p",[t._v("Let's have a look at the first 10 examples of the train dataset.")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("train_ds"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("head"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("div",[n("style",{attrs:{scoped:""}},[t._v('\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\x3c!--beforebegin--\x3e<div class="language- extra-class">\x3c!--afterbegin--\x3e<pre><code>.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n</code></pre>\n\x3c!--beforeend--\x3e</div>\x3c!--afterend--\x3e')]),t._v(" "),n("table",{staticClass:"dataframe",attrs:{border:"1"}},[n("thead",[n("tr",{staticStyle:{"text-align":"right"}},[n("th"),t._v(" "),n("th",[t._v("title")]),t._v(" "),n("th",[t._v("categories")]),t._v(" "),n("th",[t._v("abstract")])])]),t._v(" "),n("tbody",[n("tr",[n("th",[t._v("0")]),t._v(" "),n("td",[t._v("Coherent structures in the wake of a SAE squar...")]),t._v(" "),n("td",[t._v("physics.flu-dyn")]),t._v(" "),n("td",[t._v("The wake of a SAE squareback vehicle model i...")])]),t._v(" "),n("tr",[n("th",[t._v("1")]),t._v(" "),n("td",[t._v("On the interaction of precipitates and tensile...")]),t._v(" "),n("td",[t._v("cond-mat.mtrl-sci")]),t._v(" "),n("td",[t._v("Although magnesium alloys deform extensively...")])]),t._v(" "),n("tr",[n("th",[t._v("2")]),t._v(" "),n("td",[t._v("Multi-domain Spectral Collocation Method for V...")]),t._v(" "),n("td",[t._v("math.NA")]),t._v(" "),n("td",[t._v("Spectral and spectral element methods using ...")])]),t._v(" "),n("tr",[n("th",[t._v("3")]),t._v(" "),n("td",[t._v("The min-max edge q-coloring problem")]),t._v(" "),n("td",[t._v("cs.DS")]),t._v(" "),n("td",[t._v("In this paper we introduce and study a new p...")])]),t._v(" "),n("tr",[n("th",[t._v("4")]),t._v(" "),n("td",[t._v("A Simple Model for a Dual Non-Abelian Monopole...")]),t._v(" "),n("td",[t._v("hep-th")]),t._v(" "),n("td",[t._v("We investigate the flux-tube joining two equ...")])]),t._v(" "),n("tr",[n("th",[t._v("5")]),t._v(" "),n("td",[t._v("A second moment bound for critical points of p...")]),t._v(" "),n("td",[t._v("math.PR")]),t._v(" "),n("td",[t._v("We consider the number of critical points of...")])]),t._v(" "),n("tr",[n("th",[t._v("6")]),t._v(" "),n("td",[t._v("$\\Sigma^0$ production in proton nucleus collis...")]),t._v(" "),n("td",[t._v("nucl-ex")]),t._v(" "),n("td",[t._v("The production of $\\Sigma^{0}$ baryons in th...")])]),t._v(" "),n("tr",[n("th",[t._v("7")]),t._v(" "),n("td",[t._v("Zero-temperature glass transition in two dimen...")]),t._v(" "),n("td",[t._v("cond-mat.stat-mech")]),t._v(" "),n("td",[t._v("The nature of the glass transition is theore...")])]),t._v(" "),n("tr",[n("th",[t._v("8")]),t._v(" "),n("td",[t._v("Measurement of the $B^0_s\\to\\mu^+\\mu^-$ branch...")]),t._v(" "),n("td",[t._v("hep-ex")]),t._v(" "),n("td",[t._v("A search for the rare decays $B^0_s\\to\\mu^+\\...")])]),t._v(" "),n("tr",[n("th",[t._v("9")]),t._v(" "),n("td",[t._v("STAR inner tracking upgrade - A performance study")]),t._v(" "),n("td",[t._v("nucl-ex")]),t._v(" "),n("td",[t._v("Anisotropic flow measurements have demonstra...")])])])])]),t._v(" "),n("p",[t._v("Our pipeline defined in the next section, or to be more precise the "),n("code",[t._v("TaskClassification")]),t._v(" task "),n("a",{attrs:{href:"https://www.recogn.ai/biome-text/documentation/basics.html#head",target:"_blank",rel:"noopener noreferrer"}},[t._v("head"),n("OutboundLink")],1),t._v(", will expect a "),n("em",[t._v("text")]),t._v(" and "),n("em",[t._v("label")]),t._v(" column to be present in our data.\nTherefore, we need to map our input to these two columns:")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Renaming the 'categories' column into 'label'")]),t._v("\ntrain_ds"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("rename_column_"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"categories"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"label"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nvalid_ds"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("rename_column_"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"categories"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"label"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntest_ds"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("rename_column_"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"categories"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"label"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Combining 'title' and 'abstract' into a 'text' column, and remove them afterwards")]),t._v("\ntrain_ds "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" train_ds"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" x"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"text"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" x"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"title"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('" "')]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" x"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"abstract"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" remove_columns"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"title"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"abstract"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nvalid_ds "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" valid_ds"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" x"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"text"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" x"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"title"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('" "')]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" x"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"abstract"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" remove_columns"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"title"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"abstract"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntest_ds "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" test_ds"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" x"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"text"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" x"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"title"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('" "')]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" x"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"abstract"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" remove_columns"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"title"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"abstract"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("div",{staticClass:"language- extra-class"},[n("pre",[n("code",[t._v("HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))\n\n\n\n\n\n\nHBox(children=(FloatProgress(value=0.0, max=1250.0), HTML(value='')))\n\n\n\n\n\n\nHBox(children=(FloatProgress(value=0.0, max=1250.0), HTML(value='')))\n")])])]),n("h2",{attrs:{id:"configuring-and-training-the-pipeline"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#configuring-and-training-the-pipeline"}},[t._v("#")]),t._v(" Configuring and training the pipeline")]),t._v(" "),n("p",[t._v("As we have seen in "),n("a",{attrs:{href:"https://www.recogn.ai/biome-text/documentation/tutorials/1-Training_a_text_classifier.html#explore-the-training-data",target:"_blank",rel:"noopener noreferrer"}},[t._v("previous tutorials"),n("OutboundLink")],1),t._v(", a "),n("em",[t._v("biome.text")]),t._v(" "),n("a",{attrs:{href:"https://www.recogn.ai/biome-text/documentation/basics.html#pipeline",target:"_blank",rel:"noopener noreferrer"}},[n("code",[t._v("Pipeline")]),n("OutboundLink")],1),t._v(" consists of tokenizing the input, extracting text features, applying a language encoding (optionally) and executing a task-specific head in the end. In "),n("em",[t._v("biome.text")]),t._v(" the pre-trained transformers by Hugging Face are treated as a text feature, just like the "),n("em",[t._v("word")]),t._v(" and "),n("em",[t._v("char")]),t._v(" feature.")]),t._v(" "),n("p",[t._v("In this section we will configure and train 3 different pipelines to showcase the usage of transformers in "),n("em",[t._v("biome.text")]),t._v(".")]),t._v(" "),n("h3",{attrs:{id:"fine-tuning-the-transformer"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#fine-tuning-the-transformer"}},[t._v("#")]),t._v(" Fine-tuning the transformer")]),t._v(" "),n("p",[t._v("In our first pipeline we follow the common approach to use pretrained transformers in classification tasks. It consists of fine-tuning the transformer weights and using a special token as pooler in the end. In our configuration the former step means setting the "),n("code",[t._v("trainable")]),t._v(" parameter in the "),n("em",[t._v("transformers")]),t._v(" features to "),n("code",[t._v("True")]),t._v(". The downside of fine-tuning is that most of the pre-trained transformers are relatively big and require dedicated hardware to be fine-tuned. For example, in this tutorial we will use "),n("code",[t._v("distilroberta-base")]),t._v(", a "),n("a",{attrs:{href:"https://github.com/huggingface/transformers/tree/master/examples/distillation",target:"_blank",rel:"noopener noreferrer"}},[t._v("distilled version"),n("OutboundLink")],1),t._v(" of RoBERTa with a total of ~80M parameters.")]),t._v(" "),n("p",[t._v("We also need to specify the maximum number of input tokens "),n("code",[t._v("max_length")]),t._v(" supported by the pretrained transformer. If you are sure that your input data does not exceed this limit, you can skip this parameter.")]),t._v(" "),n("p",[t._v("With BERT-like models, such as RoBERTa, a special [CLS] token is added as first token to each input. It is pretrained to effectively represent the entire input and can be used as pooler in the head component. Many BERT like models pass this token through a non-linear tanh activation layer that is part of the pretraining. If you want to use these pretrained weights you have to use the "),n("code",[t._v("bert_pooler")]),t._v(" together with the corresponding "),n("code",[t._v("pretrained_model")]),t._v(". We will fine-tune these weights as well (setting "),n("code",[t._v("require_grad")]),t._v(" to "),n("code",[t._v("True")]),t._v(") and add a little dropout.")]),t._v(" "),n("div",{staticClass:"custom-block tip"},[n("p",{staticClass:"custom-block-title"},[t._v("Tip")]),t._v(" "),n("p",[t._v("You can also use the [CLS] token directly without passing it through the non-linear layer by using the "),n("code",[t._v("cls_pooler")]),t._v(".")])]),t._v(" "),n("p",[t._v("The "),n("code",[t._v("TextClassification")]),t._v(" head automatically applies a linear layer with an output dimension corresponding to the number of labels in the end.")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("pipeline_dict_finetuning "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"name"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"arxiv_categories_classification"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"features"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"transformers"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"model_name"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"distilroberta-base"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"trainable"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# freeze the weights of the transformer")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"max_length"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("512")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"head"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"TextClassification"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"labels"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" train_ds"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("unique"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"label"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"pooler"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"bert_pooler"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"pretrained_model"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"distilroberta-base"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"requires_grad"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"dropout"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# If you do not want to use the pre-trained activation layer for the CLS token (see text) ")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v('# "pooler": {')]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v('#     "type": "cls_pooler",')]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# }")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("pl "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Pipeline"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_config"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pipeline_dict_finetuning"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("In our trainer configuration we will use canonical values for the "),n("code",[t._v("batch_size")]),t._v(" and "),n("code",[t._v("lr")]),t._v(" taken from the Hugging Face transformers library. We also will apply a linearly decaying learning rate scheduler with 50 warm-up steps, which is recommended when fine-tuning a pretrained model. For now we will stick to two epochs to allow for a rapid iteration.")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("trainer "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TrainerConfiguration"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    optimizer"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"adamw"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"lr"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5e")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    learning_rate_scheduler"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"linear_with_warmup"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"num_epochs"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"num_steps_per_epoch"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1250")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"warmup_steps"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("50")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    batch_size"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    num_epochs"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("pl"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    output"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"output/fine_tuning"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    training"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("train_ds"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    validation"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("valid_ds"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    trainer"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("trainer"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("After two epochs we achieve an accuracy of about 0.65, which is competetive looking at the corresponding "),n("a",{attrs:{href:"https://www.kaggle.com/Cornell-University/arxiv/notebooks",target:"_blank",rel:"noopener noreferrer"}},[t._v("Kaggle notebooks"),n("OutboundLink")],1),t._v(". Keep in mind that we did not optimize any of the training parameters.")]),t._v(" "),n("h3",{attrs:{id:"training-with-a-frozen-transformer"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#training-with-a-frozen-transformer"}},[t._v("#")]),t._v(" Training with a frozen transformer")]),t._v(" "),n("p",[t._v("In our second pipeline we keep the weights of the transformer frozen by setting "),n("code",[t._v("trainable: False")]),t._v(" and only train the pooler in the head component. In this setup the training will be significantly faster and does not necessarily require dedicated hardware.")]),t._v(" "),n("p",[t._v("As pooler we will use a bidirectional "),n("a",{attrs:{href:"https://en.wikipedia.org/wiki/Gated_recurrent_unit",target:"_blank",rel:"noopener noreferrer"}},[t._v("GRU"),n("OutboundLink")],1),t._v(" in the head.")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("pipeline_dict_frozen "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"name"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"arxiv_categories_classification"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"features"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"transformers"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"model_name"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"distilroberta-base"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"trainable"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"max_length"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("512")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"head"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"TextClassification"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"labels"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" train_ds"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("unique"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"label"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"pooler"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"gru"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"num_layers"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"hidden_size"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"bidirectional"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("pl "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Pipeline"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_config"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pipeline_dict_frozen"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("In our training configuration we will use the same "),n("code",[t._v("batch_size")]),t._v(" as in the previous configuration but increase the learning rate to Pytorch's default value for the AdamW optimizer, in order to work well with the GRU. We also remove the learning rate scheduler with its warmup steps, since we do not modify the pretrained transformer weights.")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("trainer "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TrainerConfiguration"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    optimizer"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"adamw"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"lr"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.002")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    batch_size"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    num_epochs"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("pl"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    output"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"output/frozen_transformer"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    training"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("train_ds"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    validation"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("valid_ds"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    trainer"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("trainer"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("div",{staticClass:"language- extra-class"},[n("pre",[n("code",[t._v("HBox(children=(FloatProgress(value=0.0, description='Loading instances into memory', max=10000.0, style=Progre…\n\n\n2020-11-25 13:05:41,770 - biome.text.dataset - INFO - Caching instances to /home/david/.cache/huggingface/datasets/json/default-eeca6b23a235c21b/0.0.0/fb88b12bd94767cb0cc7eedcd82ea1f402d2162addc03a37e81d4f8dc7313ad9/c7f20f7b187d1e70.instance_list)\n\n\n\n\n\n\nHBox(children=(FloatProgress(value=0.0, description='Loading instances into memory', max=1250.0, style=Progres…\n\n\n2020-11-25 13:05:49,968 - biome.text.dataset - INFO - Caching instances to /home/david/.cache/huggingface/datasets/json/default-750351052310c435/0.0.0/fb88b12bd94767cb0cc7eedcd82ea1f402d2162addc03a37e81d4f8dc7313ad9/a6f11b086d6ad975.instance_list)\n\n\n\n\n\n2020-11-25 13:05:50,600 - allennlp.common.params - INFO - random_seed = 13370\n2020-11-25 13:05:50,600 - allennlp.common.params - INFO - numpy_seed = 1337\n2020-11-25 13:05:50,600 - allennlp.common.params - INFO - pytorch_seed = 133\n2020-11-25 13:05:50,602 - allennlp.common.checks - INFO - Pytorch version: 1.6.0\n2020-11-25 13:05:50,675 - allennlp.common.params - INFO - type = gradient_descent\n2020-11-25 13:05:50,676 - allennlp.common.params - INFO - local_rank = 0\n2020-11-25 13:05:50,676 - allennlp.common.params - INFO - patience = 2\n2020-11-25 13:05:50,676 - allennlp.common.params - INFO - validation_metric = -loss\n2020-11-25 13:05:50,677 - allennlp.common.params - INFO - num_epochs = 2\n2020-11-25 13:05:50,677 - allennlp.common.params - INFO - cuda_device = None\n2020-11-25 13:05:50,678 - allennlp.common.params - INFO - grad_norm = None\n2020-11-25 13:05:50,678 - allennlp.common.params - INFO - grad_clipping = None\n2020-11-25 13:05:50,678 - allennlp.common.params - INFO - distributed = False\n2020-11-25 13:05:50,679 - allennlp.common.params - INFO - world_size = 1\n2020-11-25 13:05:50,679 - allennlp.common.params - INFO - num_gradient_accumulation_steps = 1\n2020-11-25 13:05:50,679 - allennlp.common.params - INFO - use_amp = False\n2020-11-25 13:05:50,680 - allennlp.common.params - INFO - no_grad = None\n2020-11-25 13:05:50,680 - allennlp.common.params - INFO - learning_rate_scheduler = None\n2020-11-25 13:05:50,681 - allennlp.common.params - INFO - momentum_scheduler = None\n2020-11-25 13:05:50,681 - allennlp.common.params - INFO - moving_average = None\n2020-11-25 13:05:50,681 - allennlp.common.params - INFO - batch_callbacks = None\n2020-11-25 13:05:50,682 - allennlp.common.params - INFO - end_callbacks = None\n2020-11-25 13:05:50,682 - allennlp.common.params - INFO - trainer_callbacks = None\n2020-11-25 13:05:50,683 - allennlp.common.params - INFO - optimizer.type = adamw\n2020-11-25 13:05:50,683 - allennlp.common.params - INFO - optimizer.parameter_groups = None\n2020-11-25 13:05:50,684 - allennlp.common.params - INFO - optimizer.lr = 0.002\n2020-11-25 13:05:50,684 - allennlp.common.params - INFO - optimizer.betas = (0.9, 0.999)\n2020-11-25 13:05:50,685 - allennlp.common.params - INFO - optimizer.eps = 1e-08\n2020-11-25 13:05:50,685 - allennlp.common.params - INFO - optimizer.weight_decay = 0.01\n2020-11-25 13:05:50,685 - allennlp.common.params - INFO - optimizer.amsgrad = False\n2020-11-25 13:05:50,688 - allennlp.training.optimizers - INFO - Number of trainable parameters: 702514\n2020-11-25 13:05:50,689 - allennlp.common.util - INFO - The following parameters are Frozen (without gradient):\n2020-11-25 13:05:50,689 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.embeddings.word_embeddings.weight\n2020-11-25 13:05:50,689 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.embeddings.position_embeddings.weight\n2020-11-25 13:05:50,690 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.embeddings.token_type_embeddings.weight\n2020-11-25 13:05:50,691 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.embeddings.LayerNorm.weight\n2020-11-25 13:05:50,691 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.embeddings.LayerNorm.bias\n2020-11-25 13:05:50,692 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.0.attention.self.query.weight\n2020-11-25 13:05:50,692 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.0.attention.self.query.bias\n2020-11-25 13:05:50,693 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.0.attention.self.key.weight\n2020-11-25 13:05:50,694 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.0.attention.self.key.bias\n2020-11-25 13:05:50,694 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.0.attention.self.value.weight\n2020-11-25 13:05:50,695 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.0.attention.self.value.bias\n2020-11-25 13:05:50,695 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.0.attention.output.dense.weight\n2020-11-25 13:05:50,695 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.0.attention.output.dense.bias\n2020-11-25 13:05:50,696 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.0.attention.output.LayerNorm.weight\n2020-11-25 13:05:50,697 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.0.attention.output.LayerNorm.bias\n2020-11-25 13:05:50,697 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.0.intermediate.dense.weight\n2020-11-25 13:05:50,697 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.0.intermediate.dense.bias\n2020-11-25 13:05:50,698 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.0.output.dense.weight\n2020-11-25 13:05:50,699 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.0.output.dense.bias\n2020-11-25 13:05:50,699 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.0.output.LayerNorm.weight\n2020-11-25 13:05:50,700 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.0.output.LayerNorm.bias\n2020-11-25 13:05:50,701 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.1.attention.self.query.weight\n2020-11-25 13:05:50,701 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.1.attention.self.query.bias\n2020-11-25 13:05:50,701 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.1.attention.self.key.weight\n2020-11-25 13:05:50,702 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.1.attention.self.key.bias\n2020-11-25 13:05:50,703 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.1.attention.self.value.weight\n2020-11-25 13:05:50,704 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.1.attention.self.value.bias\n2020-11-25 13:05:50,704 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.1.attention.output.dense.weight\n2020-11-25 13:05:50,704 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.1.attention.output.dense.bias\n2020-11-25 13:05:50,705 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.1.attention.output.LayerNorm.weight\n2020-11-25 13:05:50,705 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.1.attention.output.LayerNorm.bias\n2020-11-25 13:05:50,706 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.1.intermediate.dense.weight\n2020-11-25 13:05:50,706 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.1.intermediate.dense.bias\n2020-11-25 13:05:50,706 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.1.output.dense.weight\n2020-11-25 13:05:50,707 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.1.output.dense.bias\n2020-11-25 13:05:50,708 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.1.output.LayerNorm.weight\n2020-11-25 13:05:50,708 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.1.output.LayerNorm.bias\n2020-11-25 13:05:50,709 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.2.attention.self.query.weight\n2020-11-25 13:05:50,709 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.2.attention.self.query.bias\n2020-11-25 13:05:50,709 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.2.attention.self.key.weight\n2020-11-25 13:05:50,710 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.2.attention.self.key.bias\n2020-11-25 13:05:50,711 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.2.attention.self.value.weight\n2020-11-25 13:05:50,712 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.2.attention.self.value.bias\n2020-11-25 13:05:50,712 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.2.attention.output.dense.weight\n2020-11-25 13:05:50,712 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.2.attention.output.dense.bias\n2020-11-25 13:05:50,712 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.2.attention.output.LayerNorm.weight\n2020-11-25 13:05:50,713 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.2.attention.output.LayerNorm.bias\n2020-11-25 13:05:50,713 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.2.intermediate.dense.weight\n2020-11-25 13:05:50,714 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.2.intermediate.dense.bias\n2020-11-25 13:05:50,715 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.2.output.dense.weight\n2020-11-25 13:05:50,715 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.2.output.dense.bias\n2020-11-25 13:05:50,716 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.2.output.LayerNorm.weight\n2020-11-25 13:05:50,716 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.2.output.LayerNorm.bias\n2020-11-25 13:05:50,716 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.3.attention.self.query.weight\n2020-11-25 13:05:50,717 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.3.attention.self.query.bias\n2020-11-25 13:05:50,717 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.3.attention.self.key.weight\n2020-11-25 13:05:50,717 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.3.attention.self.key.bias\n2020-11-25 13:05:50,718 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.3.attention.self.value.weight\n2020-11-25 13:05:50,718 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.3.attention.self.value.bias\n2020-11-25 13:05:50,718 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.3.attention.output.dense.weight\n2020-11-25 13:05:50,719 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.3.attention.output.dense.bias\n2020-11-25 13:05:50,719 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.3.attention.output.LayerNorm.weight\n2020-11-25 13:05:50,719 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.3.attention.output.LayerNorm.bias\n2020-11-25 13:05:50,720 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.3.intermediate.dense.weight\n2020-11-25 13:05:50,720 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.3.intermediate.dense.bias\n2020-11-25 13:05:50,720 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.3.output.dense.weight\n2020-11-25 13:05:50,721 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.3.output.dense.bias\n2020-11-25 13:05:50,721 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.3.output.LayerNorm.weight\n2020-11-25 13:05:50,722 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.3.output.LayerNorm.bias\n2020-11-25 13:05:50,722 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.4.attention.self.query.weight\n2020-11-25 13:05:50,722 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.4.attention.self.query.bias\n2020-11-25 13:05:50,723 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.4.attention.self.key.weight\n2020-11-25 13:05:50,723 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.4.attention.self.key.bias\n2020-11-25 13:05:50,723 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.4.attention.self.value.weight\n2020-11-25 13:05:50,723 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.4.attention.self.value.bias\n2020-11-25 13:05:50,724 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.4.attention.output.dense.weight\n2020-11-25 13:05:50,724 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.4.attention.output.dense.bias\n2020-11-25 13:05:50,724 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.4.attention.output.LayerNorm.weight\n2020-11-25 13:05:50,725 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.4.attention.output.LayerNorm.bias\n2020-11-25 13:05:50,725 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.4.intermediate.dense.weight\n2020-11-25 13:05:50,727 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.4.intermediate.dense.bias\n2020-11-25 13:05:50,727 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.4.output.dense.weight\n2020-11-25 13:05:50,728 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.4.output.dense.bias\n2020-11-25 13:05:50,728 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.4.output.LayerNorm.weight\n2020-11-25 13:05:50,728 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.4.output.LayerNorm.bias\n2020-11-25 13:05:50,729 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.5.attention.self.query.weight\n2020-11-25 13:05:50,729 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.5.attention.self.query.bias\n2020-11-25 13:05:50,729 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.5.attention.self.key.weight\n2020-11-25 13:05:50,730 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.5.attention.self.key.bias\n2020-11-25 13:05:50,730 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.5.attention.self.value.weight\n2020-11-25 13:05:50,730 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.5.attention.self.value.bias\n2020-11-25 13:05:50,734 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.5.attention.output.dense.weight\n2020-11-25 13:05:50,734 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.5.attention.output.dense.bias\n2020-11-25 13:05:50,735 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.5.attention.output.LayerNorm.weight\n2020-11-25 13:05:50,735 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.5.attention.output.LayerNorm.bias\n2020-11-25 13:05:50,735 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.5.intermediate.dense.weight\n2020-11-25 13:05:50,736 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.5.intermediate.dense.bias\n2020-11-25 13:05:50,736 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.5.output.dense.weight\n2020-11-25 13:05:50,737 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.5.output.dense.bias\n2020-11-25 13:05:50,737 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.5.output.LayerNorm.weight\n2020-11-25 13:05:50,737 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.encoder.layer.5.output.LayerNorm.bias\n2020-11-25 13:05:50,738 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.pooler.dense.weight\n2020-11-25 13:05:50,739 - allennlp.common.util - INFO - _head.backbone.embedder.token_embedder_transformers.transformer_model.pooler.dense.bias\n2020-11-25 13:05:50,739 - allennlp.common.util - INFO - The following parameters are Tunable (with gradient):\n2020-11-25 13:05:50,740 - allennlp.common.util - INFO - _head.pooler._module.weight_ih_l0\n2020-11-25 13:05:50,741 - allennlp.common.util - INFO - _head.pooler._module.weight_hh_l0\n2020-11-25 13:05:50,741 - allennlp.common.util - INFO - _head.pooler._module.bias_ih_l0\n2020-11-25 13:05:50,741 - allennlp.common.util - INFO - _head.pooler._module.bias_hh_l0\n2020-11-25 13:05:50,741 - allennlp.common.util - INFO - _head.pooler._module.weight_ih_l0_reverse\n2020-11-25 13:05:50,742 - allennlp.common.util - INFO - _head.pooler._module.weight_hh_l0_reverse\n2020-11-25 13:05:50,742 - allennlp.common.util - INFO - _head.pooler._module.bias_ih_l0_reverse\n2020-11-25 13:05:50,743 - allennlp.common.util - INFO - _head.pooler._module.bias_hh_l0_reverse\n2020-11-25 13:05:50,743 - allennlp.common.util - INFO - _head._classification_layer.weight\n2020-11-25 13:05:50,743 - allennlp.common.util - INFO - _head._classification_layer.bias\n2020-11-25 13:05:50,744 - allennlp.common.params - INFO - checkpointer.type = default\n2020-11-25 13:05:50,744 - allennlp.common.params - INFO - checkpointer.keep_serialized_model_every_num_seconds = None\n2020-11-25 13:05:50,745 - allennlp.common.params - INFO - checkpointer.num_serialized_models_to_keep = 1\n2020-11-25 13:05:50,745 - allennlp.common.params - INFO - checkpointer.model_save_interval = None\n2020-11-25 13:05:50,745 - allennlp.common.params - INFO - tensorboard_writer.summary_interval = 100\n2020-11-25 13:05:50,747 - allennlp.common.params - INFO - tensorboard_writer.histogram_interval = None\n2020-11-25 13:05:50,748 - allennlp.common.params - INFO - tensorboard_writer.batch_size_interval = None\n2020-11-25 13:05:50,748 - allennlp.common.params - INFO - tensorboard_writer.should_log_parameter_statistics = True\n2020-11-25 13:05:50,749 - allennlp.common.params - INFO - tensorboard_writer.should_log_learning_rate = True\n2020-11-25 13:05:50,749 - allennlp.common.params - INFO - tensorboard_writer.get_batch_num_total = None\n[34m[1mwandb[0m: Currently logged in as: [33mdcfidalgo[0m (use `wandb login --relogin` to force relogin)\n[34m[1mwandb[0m: wandb version 0.10.11 is available!  To upgrade, please run:\n[34m[1mwandb[0m:  $ pip install wandb --upgrade\n")])])]),n("p",[t._v("Tracking run with wandb version 0.10.8"),n("br"),t._v("\nSyncing run "),n("strong",{staticStyle:{color:"#cdcd00"}},[t._v("comfy-jazz-218")]),t._v(" to "),n("a",{attrs:{href:"https://wandb.ai",target:"_blank"}},[t._v("Weights & Biases")]),t._v(" "),n("a",{attrs:{href:"https://docs.wandb.com/integrations/jupyter.html",target:"_blank"}},[t._v("(Documentation)")]),t._v("."),n("br"),t._v("\nProject page: "),n("a",{attrs:{href:"https://wandb.ai/dcfidalgo/biome",target:"_blank"}},[t._v("https://wandb.ai/dcfidalgo/biome")]),n("br"),t._v("\nRun page: "),n("a",{attrs:{href:"https://wandb.ai/dcfidalgo/biome/runs/3guw1my1",target:"_blank"}},[t._v("https://wandb.ai/dcfidalgo/biome/runs/3guw1my1")]),n("br"),t._v("\nRun data is saved locally in "),n("code",[t._v("wandb/run-20201125_130551-3guw1my1")]),n("br"),n("br")]),t._v(" "),n("div",{staticClass:"language- extra-class"},[n("pre",[n("code",[t._v('2020-11-25 13:05:52,016 - allennlp.training.trainer - INFO - Beginning training.\n2020-11-25 13:05:52,017 - allennlp.training.trainer - INFO - Epoch 0/1\n2020-11-25 13:05:52,018 - allennlp.training.trainer - INFO - Worker 0 memory usage: 3.1G\n2020-11-25 13:05:52,020 - allennlp.training.trainer - INFO - Training\n\n\n\nHBox(children=(FloatProgress(value=0.0, max=1250.0), HTML(value=\'\')))\n\n\n2020-11-25 13:05:57,995 - allennlp.training.util - WARNING - Metrics with names beginning with "_" will not be logged to the tqdm progress bar.\n\n\n\n\n---------------------------------------------------------------------------\n\nKeyboardInterrupt                         Traceback (most recent call last)\n\n<ipython-input-10-d00873962548> in <module>()\n      3     training=train_ds,\n      4     validation=valid_ds,\n----\x3e 5     trainer=trainer,\n      6 )\n\n\n~/recognai/biome/biome-text/src/biome/text/pipeline.py in train(self, output, training, trainer, validation, test, extend_vocab, loggers, lazy, restore, quiet)\n    347                     )\n    348 \n--\x3e 349             self._model.file_path, metrics = pipeline_trainer.train()\n    350             train_results = TrainingResults(self.model_path, metrics)\n    351 \n\n\n~/recognai/biome/biome-text/src/biome/text/_helpers.py in train(self)\n    206 \n    207         try:\n--\x3e 208             metrics = self._trainer.train()\n    209         except KeyboardInterrupt:\n    210             # if we have completed an epoch, try to create a model archive.\n\n\n~/miniconda3/envs/biome/lib/python3.7/site-packages/allennlp/training/trainer.py in train(self)\n    964         """\n    965         try:\n--\x3e 966             return self._try_train()\n    967         finally:\n    968             # make sure pending events are flushed to disk and files are closed properly\n\n\n~/miniconda3/envs/biome/lib/python3.7/site-packages/allennlp/training/trainer.py in _try_train(self)\n    999         for epoch in range(epoch_counter, self._num_epochs):\n   1000             epoch_start_time = time.time()\n-> 1001             train_metrics = self._train_epoch(epoch)\n   1002 \n   1003             if self._master and self._checkpointer is not None:\n\n\n~/miniconda3/envs/biome/lib/python3.7/site-packages/allennlp/training/trainer.py in _train_epoch(self, epoch)\n    714             for batch in batch_group:\n    715                 with amp.autocast(self._use_amp):\n--\x3e 716                     batch_outputs = self.batch_outputs(batch, for_training=True)\n    717                     batch_group_outputs.append(batch_outputs)\n    718                     loss = batch_outputs["loss"]\n\n\n~/miniconda3/envs/biome/lib/python3.7/site-packages/allennlp/training/trainer.py in batch_outputs(self, batch, for_training)\n    602         """\n    603         batch = nn_util.move_to_device(batch, self.cuda_device)\n--\x3e 604         output_dict = self._pytorch_model(**batch)\n    605 \n    606         if for_training:\n\n\n~/miniconda3/envs/biome/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n    720             result = self._slow_forward(*input, **kwargs)\n    721         else:\n--\x3e 722             result = self.forward(*input, **kwargs)\n    723         for hook in itertools.chain(\n    724                 _global_forward_hooks.values(),\n\n\n~/recognai/biome/biome-text/src/biome/text/_model.py in forward(self, *args, **kwargs)\n    144     def forward(self, *args, **kwargs) -> Dict[str, torch.Tensor]:\n    145         """The main forward method. Wraps the head forward method and converts the head output into a dictionary"""\n--\x3e 146         head_output: TaskOutput = self._head.forward(*args, **kwargs)\n    147         # we don\'t want to break AllenNLP API: TaskOutput -> as_dict()\n    148         return head_output.as_dict()\n\n\n~/recognai/biome/biome-text/src/biome/text/modules/heads/classification/text_classification.py in forward(self, text, label)\n     69 \n     70         mask = get_text_field_mask(text)\n---\x3e 71         embedded_text = self.backbone.forward(text, mask)\n     72         embedded_text = self.pooler(embedded_text, mask=mask)\n     73 \n\n\n~/recognai/biome/biome-text/src/biome/text/backbone.py in forward(self, text, mask, num_wrapping_dims)\n     70             Encoded representation of the input\n     71         """\n---\x3e 72         embeddings = self.embedder(text, num_wrapping_dims=num_wrapping_dims)\n     73         return self.encoder(embeddings, mask=mask)\n     74 \n\n\n~/miniconda3/envs/biome/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n    720             result = self._slow_forward(*input, **kwargs)\n    721         else:\n--\x3e 722             result = self.forward(*input, **kwargs)\n    723         for hook in itertools.chain(\n    724                 _global_forward_hooks.values(),\n\n\n~/miniconda3/envs/biome/lib/python3.7/site-packages/allennlp/modules/text_field_embedders/basic_text_field_embedder.py in forward(self, text_field_input, num_wrapping_dims, **kwargs)\n    101                 # If there are multiple tensor arguments, we have to require matching names from the\n    102                 # TokenIndexer.  I don\'t think there\'s an easy way around that.\n--\x3e 103                 token_vectors = embedder(**tensors, **forward_params_values)\n    104             if token_vectors is not None:\n    105                 # To handle some very rare use cases, we allow the return value of the embedder to\n\n\n~/miniconda3/envs/biome/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n    720             result = self._slow_forward(*input, **kwargs)\n    721         else:\n--\x3e 722             result = self.forward(*input, **kwargs)\n    723         for hook in itertools.chain(\n    724                 _global_forward_hooks.values(),\n\n\n~/miniconda3/envs/biome/lib/python3.7/site-packages/allennlp/modules/token_embedders/pretrained_transformer_embedder.py in forward(self, token_ids, mask, type_ids, segment_concat_mask)\n    183             parameters["token_type_ids"] = type_ids\n    184 \n--\x3e 185         transformer_output = self.transformer_model(**parameters)\n    186         if self._scalar_mix is not None:\n    187             # As far as I can tell, the hidden states will always be the last element\n\n\n~/miniconda3/envs/biome/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n    720             result = self._slow_forward(*input, **kwargs)\n    721         else:\n--\x3e 722             result = self.forward(*input, **kwargs)\n    723         for hook in itertools.chain(\n    724                 _global_forward_hooks.values(),\n\n\n~/miniconda3/envs/biome/lib/python3.7/site-packages/transformers/modeling_roberta.py in forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\n    683             output_attentions=output_attentions,\n    684             output_hidden_states=output_hidden_states,\n--\x3e 685             return_dict=return_dict,\n    686         )\n    687         sequence_output = encoder_outputs[0]\n\n\n~/miniconda3/envs/biome/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n    720             result = self._slow_forward(*input, **kwargs)\n    721         else:\n--\x3e 722             result = self.forward(*input, **kwargs)\n    723         for hook in itertools.chain(\n    724                 _global_forward_hooks.values(),\n\n\n~/miniconda3/envs/biome/lib/python3.7/site-packages/transformers/modeling_roberta.py in forward(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)\n    422                     encoder_hidden_states,\n    423                     encoder_attention_mask,\n--\x3e 424                     output_attentions,\n    425                 )\n    426             hidden_states = layer_outputs[0]\n\n\n~/miniconda3/envs/biome/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n    720             result = self._slow_forward(*input, **kwargs)\n    721         else:\n--\x3e 722             result = self.forward(*input, **kwargs)\n    723         for hook in itertools.chain(\n    724                 _global_forward_hooks.values(),\n\n\n~/miniconda3/envs/biome/lib/python3.7/site-packages/transformers/modeling_roberta.py in forward(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\n    341             attention_mask,\n    342             head_mask,\n--\x3e 343             output_attentions=output_attentions,\n    344         )\n    345         attention_output = self_attention_outputs[0]\n\n\n~/miniconda3/envs/biome/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n    720             result = self._slow_forward(*input, **kwargs)\n    721         else:\n--\x3e 722             result = self.forward(*input, **kwargs)\n    723         for hook in itertools.chain(\n    724                 _global_forward_hooks.values(),\n\n\n~/miniconda3/envs/biome/lib/python3.7/site-packages/transformers/modeling_roberta.py in forward(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\n    275             encoder_hidden_states,\n    276             encoder_attention_mask,\n--\x3e 277             output_attentions,\n    278         )\n    279         attention_output = self.output(self_outputs[0], hidden_states)\n\n\n~/miniconda3/envs/biome/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n    720             result = self._slow_forward(*input, **kwargs)\n    721         else:\n--\x3e 722             result = self.forward(*input, **kwargs)\n    723         for hook in itertools.chain(\n    724                 _global_forward_hooks.values(),\n\n\n~/miniconda3/envs/biome/lib/python3.7/site-packages/transformers/modeling_roberta.py in forward(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions)\n    203         # This is actually dropping out entire tokens to attend to, which might\n    204         # seem a bit unusual, but is taken from the original Transformer paper.\n--\x3e 205         attention_probs = self.dropout(attention_probs)\n    206 \n    207         # Mask heads if we want to\n\n\n~/miniconda3/envs/biome/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)\n    720             result = self._slow_forward(*input, **kwargs)\n    721         else:\n--\x3e 722             result = self.forward(*input, **kwargs)\n    723         for hook in itertools.chain(\n    724                 _global_forward_hooks.values(),\n\n\n~/miniconda3/envs/biome/lib/python3.7/site-packages/torch/nn/modules/dropout.py in forward(self, input)\n     56 \n     57     def forward(self, input: Tensor) -> Tensor:\n---\x3e 58         return F.dropout(input, self.p, self.training, self.inplace)\n     59 \n     60 \n\n\n~/miniconda3/envs/biome/lib/python3.7/site-packages/torch/nn/functional.py in dropout(input, p, training, inplace)\n    971     return (_VF.dropout_(input, p, training)\n    972             if inplace\n--\x3e 973             else _VF.dropout(input, p, training))\n    974 \n    975 \n\n\nKeyboardInterrupt: \n')])])]),n("p",[t._v("The training is about 4 times faster compared with fine-tuning the transformer, and after two epochs we reach a respectable accuracy of about 0.60. Keep in mind that we did not optimize any of the training parameters.")]),t._v(" "),n("h3",{attrs:{id:"combining-text-features"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#combining-text-features"}},[t._v("#")]),t._v(" Combining text features")]),t._v(" "),n("p",[t._v("As mentioned earlier, the pretrained transformers are treated as a text feature in "),n("em",[t._v("biome.text")]),t._v(". We can easily combine them with other features, such as the "),n("em",[t._v("char")]),t._v(" feature for example, which encodes word tokens based on their characters.")]),t._v(" "),n("p",[t._v("Keep in mind that the "),n("em",[t._v("char")]),t._v(" feature provides a feature vector per word (spaCy) token, while the "),n("em",[t._v("transformers")]),t._v(" feature provides a contextualized feature vector per word piece. Therefore, we simply sum up the word piece vectors of the transformers feature, to end up with concatenated feature vectors per word token.")]),t._v(" "),n("div",{staticClass:"custom-block warning"},[n("p",{staticClass:"custom-block-title"},[t._v("Note")]),t._v(" "),n("p",[t._v("This also means that special transformer tokens, such as BERT's [CLS] token, are ignored when combining text features.")])]),t._v(" "),n("p",[t._v("As in the second configuration, we will pool the feature vectors with a "),n("em",[t._v("GRU")]),t._v(" in the "),n("em",[t._v("head")]),t._v(" component.")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("pipeline_dict_combining "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"name"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"arxiv_categories_classification"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"tokenizer"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"features"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"char"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"embedding_dim"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("32")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"lowercase_characters"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"encoder"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"gru"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"num_layers"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"hidden_size"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("32")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"bidirectional"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"dropout"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"transformers"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"model_name"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"distilroberta-base"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"trainable"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"max_length"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("512")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"head"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"TextClassification"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"labels"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" train_ds"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("unique"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"label"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"pooler"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"gru"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"num_layers"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"hidden_size"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"bidirectional"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("pl "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Pipeline"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_config"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pipeline_dict_combining"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("For the "),n("em",[t._v("char")]),t._v(" feature we have to manually create the vocab before the training.")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("pl"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("create_vocabulary"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("VocabularyConfiguration"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sources"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("train_ds"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" valid_ds"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("We will use the same training configuration as in the frozen transformer section.")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("trainer "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TrainerConfiguration"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    optimizer"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"adamw"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"lr"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.001")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    batch_size"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    num_epochs"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("pl"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    output"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"output/combined_features"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    training"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("train_ds"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    validation"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("valid_ds"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    trainer"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("trainer"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("With an accuracy of 0.55, combining features in this case seems to be counterproductive. The main reason is the exclusion of the special transformers tokens and the usage of feature vectors per word instead of word-pieces. Even when fine-tuning the transformer, those differences seem to significantly affect the performance as shown in our "),n("a",{attrs:{href:"https://wandb.ai/ignacioct/biome/reports/Exploring-Ways-to-use-Pretrained-Transformers-in-biome-text--VmlldzoyNzk2MTM",target:"_blank",rel:"noopener noreferrer"}},[t._v("WandB report"),n("OutboundLink")],1),t._v(".")]),t._v(" "),n("h3",{attrs:{id:"compare-performances-with-tensorboard-optional"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#compare-performances-with-tensorboard-optional"}},[t._v("#")]),t._v(" Compare performances with TensorBoard (optional)")]),t._v(" "),n("p",[t._v("In the output folder of the trainig we automatically log the results with "),n("a",{attrs:{href:"https://www.tensorflow.org/tensorboard/",target:"_blank",rel:"noopener noreferrer"}},[t._v("TensorBoard"),n("OutboundLink")],1),t._v(". This helps us to conveniently compare the three training runs from above. Alternatively, if you installed and logged in to WandB, the runs should have been logged automatically to the "),n("em",[t._v("biome")]),t._v(" project of your account.")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v("load_ext tensorboard\n"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v("tensorboard "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("logdir"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("output\n")])])]),n("h2",{attrs:{id:"optimizing-the-trainer-configuration"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#optimizing-the-trainer-configuration"}},[t._v("#")]),t._v(" Optimizing the trainer configuration")]),t._v(" "),n("p",[t._v("As described in the "),n("a",{attrs:{href:"https://www.recogn.ai/biome-text/documentation/tutorials/3-Hyperparameter_optimization_with_Ray_Tune.html#imports",target:"_blank",rel:"noopener noreferrer"}},[t._v("HPO tutorial"),n("OutboundLink")],1),t._v(", "),n("em",[t._v("biome.text")]),t._v(" relies on the "),n("a",{attrs:{href:"https://docs.ray.io/en/latest/tune.html#tune-index",target:"_blank",rel:"noopener noreferrer"}},[t._v("Ray Tune library"),n("OutboundLink")],1),t._v(" to perform hyperparameter optimization.\nWe recommend to go through that tutorial first, as we will be skipping most of the implementation details here.")]),t._v(" "),n("h3",{attrs:{id:"frozen-transformer"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#frozen-transformer"}},[t._v("#")]),t._v(" Frozen transformer")]),t._v(" "),n("p",[t._v("In this section we will first try to improve the performance of the frozen-transformer configuration by conducting a random search for three training parameters:")]),t._v(" "),n("ul",[n("li",[t._v("learning rate")]),t._v(" "),n("li",[t._v("weight decay")]),t._v(" "),n("li",[t._v("batch size")])]),t._v(" "),n("p",[t._v("We also set "),n("code",[t._v("num_serialized_models_to_keep")]),t._v(" to 0 to reduce the disk storage footprint.")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("trainer_dict "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"optimizer"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"adamw"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"lr"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" tune"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loguniform"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5e")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5e")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"weight_decay"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" tune"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loguniform"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"batch_size"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" tune"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("choice"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("16")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"num_epochs"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"num_serialized_models_to_keep"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),n("p",[t._v("Having defined the search space for our hyperparameters, we create a "),n("code",[t._v("TuneExperiment")]),t._v(" where we specify the number of samples to be dranw from our search space, the "),n("code",[t._v("local_dir")]),t._v(" for our HPO output and the computing resources we want Ray Tune to have access to.")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("tune_exp "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TuneExperiment"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    pipeline_config"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("pipeline_dict_frozen"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n    trainer_config"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("trainer_dict"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    train_dataset"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("train_ds"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    valid_dataset"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("valid_ds"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    name"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"frozen_transformer_sweep"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# parameters for tune.run")]),t._v("\n    num_samples"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("50")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    local_dir"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"tune_runs"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    resources_per_trial"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"gpu"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"cpu"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("With our TuneExperiment object at hand, we simply have to pass it on to the "),n("a",{attrs:{href:"https://docs.ray.io/en/master/tune/api_docs/execution.html#tune-run",target:"_blank",rel:"noopener noreferrer"}},[n("code",[t._v("tune.run")]),n("OutboundLink")],1),t._v(" function to start our random search.")]),t._v(" "),n("p",[t._v("To speed things up we will use the "),n("a",{attrs:{href:"https://blog.ml.cmu.edu/2018/12/12/massively-parallel-hyperparameter-optimization/",target:"_blank",rel:"noopener noreferrer"}},[t._v("ASHA"),n("OutboundLink")],1),t._v(" trial scheduler that terminates low performing trials early. In our case we take the "),n("em",[t._v("validation_accuracy")]),t._v(" as a meassure of the models performance.")]),t._v(" "),n("p",[t._v("In Google Colab with a GPU backend this random search should not take more than about 1.5 hours and we recommend following the progress via WandB. Alternatively, you could follow the progress via "),n("a",{attrs:{href:"https://www.tensorflow.org/tensorboard/",target:"_blank",rel:"noopener noreferrer"}},[t._v("TensorBoard"),n("OutboundLink")],1),t._v(" by launching a TensorBoard instance before starting the random search, and pointing it to the "),n("em",[t._v("local_dir")]),t._v(" output:")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v("tensorboard "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("logdir"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("tune_runs\n")])])]),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("analysis "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tune"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("run"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    tune_exp"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    scheduler"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("tune"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("schedulers"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ASHAScheduler"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n    metric"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"validation_accuracy"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    mode"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"max"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    progress_reporter"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("tune"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("JupyterNotebookReporter"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("overwrite"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("With the best configuration of our random search we should achieve an accuracy of about 0.64.")]),t._v(" "),n("h3",{attrs:{id:"fine-tuning-the-transformer-2"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#fine-tuning-the-transformer-2"}},[t._v("#")]),t._v(" Fine-tuning the transformer")]),t._v(" "),n("p",[t._v("We will also try to optimize the training parameters for a fine-tuning of the transformer. Since this is computationally much more expensive, we will take only a subset of our training data for the random search.")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("train_1000 "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" train_ds"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shuffle"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("seed"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("43")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("select"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1000")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("The training parameters we are going to tune are the following:")]),t._v(" "),n("ul",[n("li",[t._v("learning rate")]),t._v(" "),n("li",[t._v("weight decay")]),t._v(" "),n("li",[t._v("warmup steps")])]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("trainer_dict "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"optimizer"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"adamw"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"lr"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" tune"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loguniform"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"weight_decay"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" tune"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loguniform"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"learning_rate_scheduler"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"linear_with_warmup"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"num_epochs"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"num_steps_per_epoch"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("125")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"warmup_steps"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" tune"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("choice"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("101")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"batch_size"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"num_epochs"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"num_serialized_models_to_keep"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),n("p",[t._v("After having defined the search space, we create a "),n("code",[t._v("TuneExperiment")]),t._v(" providing this time the subset of the training data.")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("tune_exp "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TuneExperiment"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    pipeline_config"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("pipeline_dict_finetuning"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n    trainer_config"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("trainer_dict"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    train_dataset"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("train_ds"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    valid_dataset"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("valid_ds"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    name"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"finetuning_sweep3"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# parameters for tune.run")]),t._v("\n    num_samples"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("50")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    local_dir"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"tune_runs"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    resources_per_trial"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"gpu"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"cpu"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("Again, we will use the "),n("a",{attrs:{href:"https://blog.ml.cmu.edu/2018/12/12/massively-parallel-hyperparameter-optimization/",target:"_blank",rel:"noopener noreferrer"}},[t._v("ASHA"),n("OutboundLink")],1),t._v(" trial scheduler and maximize the "),n("em",[t._v("validation_accuracy")]),t._v(".")]),t._v(" "),n("p",[t._v("In Google Colab with a GPU backend, this random search should not take longer than 1.5 hours.")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("analysis "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tune"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("run"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    tune_exp"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    scheduler"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("tune"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("schedulers"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ASHAScheduler"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    metric"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"validation_accuracy"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    mode"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"max"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    progress_reporter"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("tune"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("JupyterNotebookReporter"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("overwrite"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("We now take the configuration that yielded the best "),n("em",[t._v("validation accuracy")]),t._v(" and train the pipeline on the full training set. In our random search the best configuration was following:")]),t._v(" "),n("ul",[n("li",[t._v("learning rate: 0.0000453")]),t._v(" "),n("li",[t._v("warmup steps: 45")]),t._v(" "),n("li",[t._v("weight decay: 0.003197")])]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("pl "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Pipeline"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_config"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pipeline_dict_finetuning"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("trainer "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" TrainerConfiguration"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    optimizer"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"adamw"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"lr"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.0000453")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"weight_decay"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.003197")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    learning_rate_scheduler"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"linear_with_warmup"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"num_epochs"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"num_steps_per_epoch"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1250")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"warmup_steps"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("45")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    batch_size"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    num_epochs"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("pl"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    output"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"output/transformer_final_model/"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    training"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("train_ds"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    validation"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("valid_ds"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    trainer"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("trainer"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("With the optimized training parameters we achieve an accuracy of about 0.67.")]),t._v(" "),n("h3",{attrs:{id:"evaluating-with-a-test-set"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#evaluating-with-a-test-set"}},[t._v("#")]),t._v(" Evaluating with a test set")]),t._v(" "),n("p",[t._v("TO BE DONE")]),t._v(" "),n("h2",{attrs:{id:"making-predictions"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#making-predictions"}},[t._v("#")]),t._v(" Making predictions")]),t._v(" "),n("p",[t._v("Let's quickly recap what we have learnt so far:")]),t._v(" "),n("ul",[n("li",[t._v("Freezing the pretrained transformer and optimizing a GRU pooler in the head can be valid option if computing resources are limited;")]),t._v(" "),n("li",[t._v('However, fine-tuning the transformer at word-piece level and using the CLS token as "pooler" works best;')]),t._v(" "),n("li",[t._v("A quick HPO of the training parameters improved the accuracies by ~0.03.")])]),t._v(" "),n("p",[t._v("With our best model at hand we will finally make a simple prediction. First we have to load the pipeline from our training output:")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("pl_trained "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Pipeline"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_pretrained"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"output/transformer_final_model/model.tar.gz"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),n("p",[t._v("Then we can simply call the "),n("code",[t._v("predict")]),t._v(" method that outputs a dictionary with a "),n("code",[t._v("labels")]),t._v(" and "),n("code",[t._v("probabilities")]),t._v(" key containing a list of labels and their corresponding probabilities, ordered from most to less likely.")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[t._v("prediction_dict "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pl_trained"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("text"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"This is a title of a super intelligent Natural Language Processing system"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nprediction_dict\n")])])]),n("p",[t._v('The most likely category predicted is the "'),n("em",[t._v("cs.CL")]),t._v('" category, which seems fitting according to this '),n("a",{attrs:{href:"https://arxiv.org/category_taxonomy",target:"_blank",rel:"noopener noreferrer"}},[t._v("list of arxiv categories and their meanings"),n("OutboundLink")],1),t._v(".")])])}),[],!1,null,null,null);e.default=s.exports}}]);
