(window.webpackJsonp=window.webpackJsonp||[]).push([[14],{423:function(e,t,a){"use strict";a.r(t);var n=a(26),s=Object(n.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h1",{attrs:{id:"biome-text-backbone"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#biome-text-backbone"}},[e._v("#")]),e._v(" biome.text.backbone "),a("Badge",{attrs:{text:"Module"}})],1),e._v(" "),a("div"),e._v(" "),a("div"),e._v(" "),a("pre",{staticClass:"title"},[a("h2",{attrs:{id:"modelbackbone"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#modelbackbone"}},[e._v("#")]),e._v(" ModelBackbone "),a("Badge",{attrs:{text:"Class"}})],1),e._v("\n")]),e._v(" "),a("pre",{staticClass:"language-python"},[a("code",[e._v("\n"),a("span",{staticClass:"token keyword"},[e._v("class")]),e._v(" "),a("span",{staticClass:"ident"},[e._v("ModelBackbone")]),e._v(" ("),e._v("\n    "),a("span",[e._v("vocab: allennlp.data.vocabulary.Vocabulary")]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("featurizer: "),a("a",{attrs:{title:"biome.text.featurizer.InputFeaturizer",href:"featurizer.html#biome.text.featurizer.InputFeaturizer"}},[e._v("InputFeaturizer")])]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("embedder: allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder")]),a("span",[e._v(",")]),e._v("\n    "),a("span",[e._v("encoder: Union["),a("a",{attrs:{title:"biome.text.modules.configuration.allennlp_configuration.Seq2SeqEncoderConfiguration",href:"modules/configuration/allennlp_configuration.html#biome.text.modules.configuration.allennlp_configuration.Seq2SeqEncoderConfiguration"}},[e._v("Seq2SeqEncoderConfiguration")]),e._v(", NoneType] = None")]),a("span",[e._v(",")]),e._v("\n"),a("span",[e._v(")")]),e._v("\n")]),e._v("\n")]),e._v(" "),a("p",[e._v("The backbone of the model.")]),e._v(" "),a("p",[e._v("It is composed of a tokenizer, featurizer and an encoder.\nThis component of the model can be pretrained and used with different task heads.")]),e._v(" "),a("h2",{attrs:{id:"attributes"}},[e._v("Attributes")]),e._v(" "),a("dl",[a("dt",[a("strong",[a("code",[e._v("vocab")])])]),e._v(" "),a("dd",[e._v("The vocabulary of the pipeline")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("featurizer")])])]),e._v(" "),a("dd",[e._v("Defines the input features of the tokens and indexes")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("embedder")])])]),e._v(" "),a("dd",[e._v("The embedding layer")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("encoder")])])]),e._v(" "),a("dd",[e._v("Outputs an encoded sequence of the tokens")])]),e._v(" "),a("p",[e._v("Initializes internal Module state, shared by both nn.Module and ScriptModule.")]),e._v(" "),a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"ancestors"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#ancestors"}},[e._v("#")]),e._v(" Ancestors")]),e._v("\n")]),e._v(" "),a("ul",{staticClass:"hlist"},[a("li",[e._v("torch.nn.modules.module.Module")])]),e._v(" "),a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"instance-variables"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#instance-variables"}},[e._v("#")]),e._v(" Instance variables")]),e._v("\n")]),e._v(" "),a("dl",[a("dt",{attrs:{id:"biome.text.backbone.ModelBackbone.tokenizer"}},[a("code",{staticClass:"name"},[e._v("var "),a("span",{staticClass:"ident"},[e._v("tokenizer")]),e._v(" : "),a("a",{attrs:{title:"biome.text.tokenizer.Tokenizer",href:"tokenizer.html#biome.text.tokenizer.Tokenizer"}},[e._v("Tokenizer")])])]),e._v(" "),a("dd")]),e._v(" "),a("dl",[a("pre",{staticClass:"title"},[a("h3",{attrs:{id:"forward"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#forward"}},[e._v("#")]),e._v(" forward "),a("Badge",{attrs:{text:"Method"}})],1),e._v("\n")]),e._v(" "),a("dt",[a("div",{staticClass:"language-python extra-class"},[a("pre",{staticClass:"language-python"},[a("code",[e._v("\n"),a("span",{staticClass:"token keyword"},[e._v("def")]),e._v(" "),a("span",{staticClass:"ident"},[e._v("forward")]),e._v(" ("),e._v("\n  self,\n  text: Dict[str, Dict[str, torch.Tensor]],\n  mask: torch.Tensor,\n  num_wrapping_dims: int = 0,\n)  -> torch.Tensor\n")]),e._v("\n")])])]),e._v(" "),a("dd",[a("p",[e._v("Applies the embedding and encoding layer")]),e._v(" "),a("h2",{attrs:{id:"parameters"}},[e._v("Parameters")]),e._v(" "),a("dl",[a("dt",[a("strong",[a("code",[e._v("text")])])]),e._v(" "),a("dd",[e._v("Output of the "),a("code",[e._v("batch.as_tensor_dict()")]),e._v(" method, basically the indices of the indexed tokens")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("mask")])])]),e._v(" "),a("dd",[e._v("A mask indicating which one of the tokens are padding tokens")]),e._v(" "),a("dt",[a("strong",[a("code",[e._v("num_wrapping_dims")])])]),e._v(" "),a("dd",[e._v("0 if "),a("code",[e._v("text")]),e._v(" is the output of a "),a("code",[e._v("TextField")]),e._v(", 1 if it is the output of a "),a("code",[e._v("ListField")])])]),e._v(" "),a("h2",{attrs:{id:"returns"}},[e._v("Returns")]),e._v(" "),a("dl",[a("dt",[a("code",[e._v("tensor")])]),e._v(" "),a("dd",[e._v("Encoded representation of the input")])])])])])}),[],!1,null,null,null);t.default=s.exports}}]);
